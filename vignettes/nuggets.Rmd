---
title: "nuggets: Get Started"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{nuggets: Get Started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, include=FALSE}
library(nuggets)
library(dplyr)

options(tibble.width = Inf)
```


# Introduction

Package `nuggets` searches for patterns that can be expressed as formulae in
the form of elementary conjunctions, referred to in this text as *conditions*.
Conditions are constructed from *predicates*, which correspond to data
columns. The interpretation of conditions depends on the choice of underlying
logic:

- *Crisp (Boolean) logic*: each predicate takes values `TRUE` (1) or `FALSE`
  (0). The truth value of a condition is computed according to the rules of
  classical Boolean algebra.
  
- *Fuzzy logic*: each predicate is assigned a *truth degree* from the interval
  $[0, 1]$. The truth degree of a conjunction is then computed using a chosen
  *triangular norm (t-norm)*. The package supports three common t-norms, which
  are defined for predicates' truth degrees $a, b \in [0, 1]$ as follows:
  - *Gödel* (minimum) t-norm: $\min(a, b)$ ;
  - *Goguen* (product) t-norm: $a \cdot b$ ;
  - *Łukasiewicz* t-norm: $\max(0, a + b - 1)$ 
  
Before applying `nuggets`, data columns intended as predicates must be prepared
either by *dichotomization* (conversion into *dummy variables*) or by
transformation into *fuzzy sets*. The package provides functions for both
transformations. See the section [Data Preparation](#data-preparation) below
for more details.

`nuggets` implements functions to search for pre-defined types of patterns, for
example:

- `dig_associations()` for association rules,
- `dig_baseline_contrasts()`, `dig_complement_contrasts()`, and
  `dig_paired_baseline_contrasts()` for various contrast patterns on numeric
  variables,
- `dig_correlations()` for conditional correlations. 

See [Pre-defined Patterns](#pre-defined-patterns) below for further details.

Discovered rules and patterns can be post-processed, visualized, and explored
interactively. Section
[Post-processing and Visualization](#post-processing-and-visualization)
describes these features.

Finally, the package allows users to provide custom evaluation functions for
conditions and to search for *user-defined* types of patterns:

- `dig()` is a general function for searching arbitrary pattern types.
- `dig_grid()` is a wrapper around `dig()` for patterns defined by conditions
  and a pair of columns evaluated by a user-defined function.  

See [Custom Patterns](#custom-patterns) for more information.




# Data Preparation

For patterns based on crisp conditions, the data columns that serve as
predicates in conditions must be transformed either to logical (`TRUE`/`FALSE`)
columns, or to fuzzy sets with values from the interval $[0, 1]$. The first
option is simpler and faster, and it is the recommended option for most
applications. The second option is more flexible and allows to model
uncertainty in data, but it is more computationally demanding.



## Preparations of Crisp (Boolean) Predicates

For patterns based on crisp conditions, the data columns that would serve as
predicates in conditions have to be transformed to logical (`TRUE`/`FALSE`)
columns. That can be done in two ways:

- numeric columns can be transformed to factors with a selected number of
  levels, and then
- factors can be transformed to dummy logical columns.

Both operations can be done with the help of the `partition()` function. The
`partition()` function requires the dataset as its first argument and a
*tidyselect* selection expression to select the columns to be transformed.

Factors and logical columns are automatically transformed to dummy logical columns
by the `partition()` function. For numeric columns, the `partition()` function
requires the `.method` argument to specify the method of partitioning:

- `.method = "dummy"` transforms numeric columns to factors and then to dummy
  logical columns. That effectively creates a separate logical column for each
  distinct value of the numeric column.
- `.method = "crisp"` transforms numeric columns to crisp predicates by
  dividing the range of values into intervals and coding the values into dummy
  logical columns according to the intervals.
- there exist other methods of partitioning of numeric columns. These
  methods create fuzzy predicates and are described in the next section.

For example, consider the built-in `mtcars` dataset. This dataset contains
information about various car models. For the sake of illustration,
let us transform the `cyl` column into factor first:

```{r}
mtcars$cyl <- factor(mtcars$cyl,
                     levels= c(4, 6, 8),
                     labels = c("four", "six", "eight"))
head(mtcars)
```

Factors are transformed to dummy logical columns by the `partition()` function
automatically:

```{r}
partition(mtcars, cyl)
```

The `vs`, `am`, and `gear` columns are numeric but actually represent
categories. To transform them to dummy logical columns in the same way as
factors, we can use the `partition()` function with the `.method` argument set to
`"dummy"`:

```{r}
partition(mtcars, vs:gear, .method = "dummy")
```

The `mpg` column is numeric and therefore cannot be transformed directly into
dummy logical columns. A better approach is to use the `"crisp"` method of
partitioning.

The `"crisp"` method divides the range of values of the selected columns into
intervals specified by the `.breaks` argument and then encodes the values into
dummy logical columns corresponding to the intervals. The `.breaks` argument
is a numeric vector that specifies the interval boundaries.

For example, the `mpg` values can be divided into four intervals: (-Inf, 15],
(15, 20], (20, 30], and (30, Inf). The `.breaks` argument is then the vector
`c(-Inf, 15, 20, 30, Inf)`, which defines the boundaries of these intervals.

```{r}
partition(mtcars, mpg, .method = "crisp", .breaks = c(-Inf, 15, 20, 30, Inf))
```

If we want the breaks to be evenly spaced across the range of values, we can
set `.breaks` to a single integer. This value specifies the number of intervals
to create. For example, the following command divides the `disp` values into
three intervals of equal width:

```{r}
partition(mtcars, disp, .method = "crisp", .breaks = 3)
```

Each call to `partition()` returns a tibble with the selected columns
transformed to dummy logical columns, while the other columns remain
unchanged.

The transformation of the whole `mtcars` dataset to crisp predicates can be done
as follows:

```{r}
crispMtcars <- mtcars |>
    partition(cyl, vs:gear, .method = "dummy") |>
    partition(mpg, .method = "crisp", .breaks = c(-Inf, 15, 20, 30, Inf)) |>
    partition(disp:carb, .method = "crisp", .breaks = 3) 

head(crispMtcars, n = 3)
```

Now all columns are logical and can be used as predicates in crisp conditions.



## Preparations of Fuzzy Predicates

In many real-world datasets, numeric attributes do not lend themselves to
clear-cut, crisp boundaries. For example, deciding whether a car has "low
mileage" or "high mileage" is often subjective. A vehicle with 19 miles per
gallon may be considered "low" in one context but "medium" in another. Crisp
intervals force a strict separation between categories, which can be too rigid
and may lose information about gradual changes in the data.

To address this, **fuzzy predicates** are used. A fuzzy predicate expresses
the degree to which a condition is satisfied. Instead of being strictly
`TRUE` or `FALSE`, each predicate is represented by a number in the interval
$[0,1]$. A truth degree of 0 means the predicate is entirely false, 1 means it
is fully true, and values in between indicate partial membership. This allows
us to model smooth transitions between categories and capture more nuanced
patterns.

For example, a fuzzy predicate could represent "medium horsepower" in the
`mtcars` dataset. A car with 120 hp may belong to this category to a degree
of 0.8, while a car with 150 hp may belong to it only to a degree of 0.2.
Such representations are more faithful to human reasoning and often yield
patterns that are both more robust and more interpretable.

The transformation of numeric columns to fuzzy predicates can be done with the
`partition()` function. As with crisp partitioning, factors are transformed to
dummy logical columns. Numeric columns, however, are transformed into *fuzzy
truth values*. The `partition()` function provides two fuzzy partitioning methods:

- `.method = "triangle"` creates fuzzy sets with triangular membership functions.  
- `.method = "raisedcos"` creates fuzzy sets with raised cosine membership functions.

These membership functions specify how strongly a value belongs to a fuzzy set.
The choice of function depends on the desired smoothness of the transition
between sets.

```{r}
#| fig.alt: >
#|   Comparison of triangular and raised cosine membership functions
#| fig.cap: >
#|   Comparison of triangular and raised cosine membership functions
#| fig.width: 7
#| fig.height: 4
#| out.width: "80%"
#| fig.align: "center"
#| echo: false
#| message: false
#| warning: false
library(dplyr)
library(ggplot2)
library(tidyr)


# Triangular membership
triangular <- function(x, a, b, c) {
  pmax(pmin((x - a) / (b - a), (c - x) / (c - b)), 0)
}

# Raised cosine "triangle-like" membership
raisedcos <- function(x, a, b, c) {
  mu <- numeric(length(x))
  
  # Rising part (a -> b)
  rise <- (x >= a) & (x <= b)
  mu[rise] <- 0.5 * (1 - cos(pi * (x[rise] - a) / (b - a)))
  
  # Falling part (b -> c)
  fall <- (x >= b) & (x <= c)
  mu[fall] <- 0.5 * (1 + cos(pi * (x[fall] - b) / (c - b)))
  
  mu
}

# Input values
x_vals <- seq(0, 100, length.out = 1000)

# Define fuzzy sets: peak at 50, base from 20 to 80
df <- tibble(
  x = x_vals,
  triangle = triangular(x_vals, 20, 50, 80),
  raisedcos  = raisedcos(x_vals, 20, 50, 80)
) |> 
  pivot_longer(-x, names_to = "method", values_to = "membership")

# Plot
ggplot(df, aes(x = x, y = membership, color = method)) +
  geom_line(size = 1.2) +
  labs(x = "", y = "membership degree") +
  theme_gray(base_size = 16) +
  theme(legend.position = "top")
```

More advanced fuzzy partitioning of numeric columns can be achieved with the
[lfl](https://cran.r-project.org/package=lfl) package, which provides
tools for defining fuzzy sets of many types, including linguistic terms such as
"very small" or "extremely big". See the
[`lfl` documentation](https://github.com/beerda/lfl/blob/master/vignettes/main.pdf)
for more information.

In the following example, the `mpg` and `hp` columns from the `mtcars` dataset
are transformed to fuzzy sets with triangular membership functions. The
`.breaks` argument specifies the fuzzy set definitions. For
`.method = "triangle"`, each consecutive triplet of values in `.breaks`
defines one triangular fuzzy set: the first and last values are the borders of
the triangle, and the middle value is its peak.

For instance, `.breaks = c(-Inf, 15, 20, 30, Inf)` creates three triangular 
fuzzy sets defined with triplets (-Inf,15,20), (15,20,30), and (20,30,Inf):

```{r}
partition(mtcars, mpg, .method = "triangle", .breaks = c(-Inf, 15, 20, 30, Inf))
```

If a regular partitioning of the range of values is desired, `.breaks` can be set to a
single integer, which specifies the number of fuzzy sets to create. For example,
the following command divides the `disp` values into three overlapping
triangular fuzzy sets:

```{r}
partition(mtcars, disp, .method = "triangle", .breaks = 3)
```

The fuzzy transformation of the whole `mtcars` dataset is shown below:

```{r, message=FALSE}
fuzzyMtcars <- mtcars |>
    partition(cyl, vs:gear, .method = "dummy") |>
    partition(mpg, .method = "triangle", .breaks = c(-Inf, 15, 20, 30, Inf)) |>
    partition(disp:carb, .method = "triangle", .breaks = 3) 

head(fuzzyMtcars, n = 3)
```

Note that the `cyl`, `vs`, `am`, and `gear` columns are still represented by
dummy logical columns, while the `mpg`, `disp`, and other columns are now
represented by fuzzy sets. This combination allows both crisp and fuzzy
predicates to be used together in pattern discovery, offering more flexibility 
and interpretability.



# Pre-defined Patterns

`nuggets` provides a set of functions for searching for some best-known pattern types.
These functions allow to process Boolean data, fuzzy data, or both. The result of
these functions is always a tibble with patterns stored as rows. For more advance
usage, which allows to search for custom patterns or to compute user-defined measures
and statistics, see the section **Custom Patterns**.


### Search for Association Rules

Association rules are rules of the form $A \Rightarrow B$, where $A$ is either
Boolean or fuzzy condition in the form of conjunction, and $B$ is a Boolean or
fuzzy predicate.

Before continuing with the search for rules, it is advisable to create the so-called
*vector of disjoints*. The vector of disjoints is a character vector with the same
length as the number of columns in the analyzed dataset. It specifies predicates, which
are mutually exclusive and should not be combined together in a single pattern's condition:
columns with equal values in the disjoint vector will not appear in a single condition.
Providing the vector of disjoints to the algorithm will speed-up the search as it makes
no sense, e.g., to combine `Plant=Qn1` and `Plant=Qn2` in a condition
`Plant=Qn1 & Plant=Qn2` as such formula is never true for any data row.

The vector of disjoints can be easily created from the column names of the dataset, e.g.,
by obtaining the first part of column names before the equal sign, which is neatly
provided by the `var_names()` function as follows:

```{r}
#disj <- var_names(colnames(fuzzyCO2))
#print(disj)
```

The function `dig_associations` takes the analyzed dataset as its first parameter and
a pair of `tidyselect` expressions to select the column names to appear
in the left-hand (antecedent) and right-hand (consequent) side of the rule. The following
command searches for associations rules, such that:

- any column except those starting with "Treatment" is in the antecedent;
- any column starting with "Treatment" is in the consequent;
- the minimum support is 0.02 (support is the proportion of rows that satisfy the
  antecedent AND consequent));
- the minimum confidence is 0.8 (confidence is the proportion of rows satisfying the
  consequent GIVEN the antecedent is true).

```{r}
#result <- dig_associations(fuzzyCO2,
                           #antecedent = !starts_with("Treatment"),
                           #consequent = starts_with("Treatment"),
                           #disjoint = disj,
                           #min_support = 0.02,
                           #min_confidence = 0.8)
```

The result is a tibble with found rules. We may arrange it by support in descending order:

```{r}
#result <- arrange(result, desc(support))
#print(result)
```


## Conditional Correlations

TBD (`dig_correlations`)

## Contrast Patterns

TBD (`dig_contrasts`)


# Post-processing and Visualization

TBD

# Custom Patterns

The `nuggets` package allows to execute a user-defined callback function on each generated
frequent condition. That way a custom type of patterns may be searched. The following example
replicates the search for associations rules with the custom callback function. For that, a dataset
has to be dichotomized and the disjoint vector created as in the **Data Preparation** section
above:

```{r}
#head(fuzzyCO2)
#print(disj)
```

As we want to search for associations rules with some minimum support and confidence, we define
the variables to hold that thresholds. We also need to define a callback function that will be
called for each found frequent condition. Its purpose is to generate the rules with the
obtained condition as an antecedent:

```{r}
min_support <- 0.02
min_confidence <- 0.8

f <- function(condition, support, foci_supports) {
    conf <- foci_supports / support
    sel <- !is.na(conf) & conf >= min_confidence & !is.na(foci_supports) & foci_supports >= min_support
    conf <- conf[sel]
    supp <- foci_supports[sel]
    
    lapply(seq_along(conf), function(i) { 
      list(antecedent = format_condition(names(condition)),
           consequent = format_condition(names(conf)[[i]]),
           support = supp[[i]],
           confidence = conf[[i]])
    })
}
```

The callback function `f()` defines three arguments: `condition`, `support` and `foci_supports`.
The names of the arguments are not random. Based on the argument names of the callback function,
the searching algorithm provides information to the function. Here `condition` is a vector of indices
representing the conjunction of predicates in a condition. By the predicate we mean the column in the
source dataset. The `support` argument gets the relative frequency of the condition in the dataset.
`foci_supports` is a vector of supports of special predicates, which we call "foci" (plural of "focus"),
within the rows satisfying the condition. For associations rules, foci are potential rule consequents.

Now we can run the digging for rules:

```{r}
#result <- dig(fuzzyCO2,
              #f = f,
              #condition = !starts_with("Treatment"),
              #focus = starts_with("Treatment"),
              #disjoint = disj,
              #min_length = 1,
              #min_support = min_support)
```

As we return a list of lists in the callback function, we have to flatten the first level 
of lists in the result and binding it into a data frame:

```{r}
#result <- result |>
  #unlist(recursive = FALSE) |>
  #lapply(as_tibble) |>
  #do.call(rbind, args = _) |>
  #arrange(desc(support))
#
#print(result)
```

[{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://beerda.github.io/nuggets/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Data Preparation","text":"applying nuggets pattern discovery, data columns intended predicates must prepared either dichotomization (conversion dummy variables) transformation fuzzy sets. vignette provides comprehensive guide data preparation functions techniques available nuggets package. package provides two main approaches data preparation: Crisp (Boolean) predicates: Transform data columns logical (TRUE/FALSE) columns. approach simpler faster, recommended applications. Fuzzy predicates: Transform numeric columns membership degrees interval \\([0, 1]\\). approach flexible allows modeling uncertainty data, computationally demanding. primary function data preparation partition(), handles crisp fuzzy transformations. Additional utility functions help identify remove uninformative columns detect tautologies data.","code":""},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"data-preparation-with-partition","dir":"Articles","previous_headings":"","what":"Data Preparation with partition()","title":"Data Preparation","text":"patterns based crisp conditions, data columns serve predicates conditions must transformed either logical (TRUE/FALSE) columns, fuzzy sets values interval \\([0, 1]\\). first option simpler faster, recommended option applications. second option flexible allows model uncertainty data, computationally demanding.","code":""},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"preparation-of-crisp-boolean-predicates","dir":"Articles","previous_headings":"Data Preparation with partition()","what":"Preparation of Crisp (Boolean) Predicates","title":"Data Preparation","text":"patterns based crisp conditions, data columns serve predicates conditions transformed logical (TRUE/FALSE) columns. can done two ways: numeric columns can transformed factors selected number levels, factors can transformed dummy logical columns. operations can done help partition() function. partition() function requires dataset first argument tidyselect selection expression select columns transformed. Factors logical columns automatically transformed dummy logical columns partition() function. numeric columns, partition() function requires .method argument specify method partitioning: .method = \"dummy\" transforms numeric columns factors dummy logical columns. effectively creates separate logical column distinct value numeric column. .method = \"crisp\" transforms numeric columns crisp predicates dividing range values intervals coding values dummy logical columns according intervals. exist methods partitioning numeric columns. methods create fuzzy predicates described next section. example, consider built-mtcars dataset. dataset contains information various car models. sake illustration, let us transform cyl column factor first: Factors transformed dummy logical columns partition() function automatically: vs, , gear columns numeric actually represent categories. transform dummy logical columns way factors, can use partition() function .method argument set \"dummy\": mpg column numeric therefore transformed directly dummy logical columns. better approach use \"crisp\" method partitioning. \"crisp\" method divides range values selected columns intervals specified .breaks argument encodes values dummy logical columns corresponding intervals. .breaks argument numeric vector specifies interval boundaries. example, mpg values can divided four intervals: (-Inf, 15], (15, 20], (20, 30], (30, Inf). .breaks argument vector c(-Inf, 15, 20, 30, Inf), defines boundaries intervals. Note: advisable put -Inf Inf first last elements .breaks vector ensure values covered intervals. want breaks evenly spaced across range values, can set .breaks single integer. value specifies number intervals create. example, following command divides disp values three intervals equal width: call partition() returns tibble selected columns transformed dummy logical columns, columns remain unchanged. transformation whole mtcars dataset crisp predicates can done follows: Now columns logical can used predicates crisp conditions.","code":"# Create a copy to avoid modifying the original dataset mtcars_example <- mtcars mtcars_example$cyl <- factor(mtcars_example$cyl,                      levels= c(4, 6, 8),                      labels = c(\"four\", \"six\", \"eight\")) head(mtcars_example) #>                    mpg   cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   six  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   six  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8  four  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   six  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7 eight  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   six  225 105 2.76 3.460 20.22  1  0    3    1 partition(mtcars_example, cyl) #> # A tibble: 32 × 13 #>      mpg  disp    hp  drat    wt  qsec    vs    am  gear  carb `cyl=four` #>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <lgl>      #>  1  21    160    110  3.9   2.62  16.5     0     1     4     4 FALSE      #>  2  21    160    110  3.9   2.88  17.0     0     1     4     4 FALSE      #>  3  22.8  108     93  3.85  2.32  18.6     1     1     4     1 TRUE       #>  4  21.4  258    110  3.08  3.22  19.4     1     0     3     1 FALSE      #>  5  18.7  360    175  3.15  3.44  17.0     0     0     3     2 FALSE      #>  6  18.1  225    105  2.76  3.46  20.2     1     0     3     1 FALSE      #>  7  14.3  360    245  3.21  3.57  15.8     0     0     3     4 FALSE      #>  8  24.4  147.    62  3.69  3.19  20       1     0     4     2 TRUE       #>  9  22.8  141.    95  3.92  3.15  22.9     1     0     4     2 TRUE       #> 10  19.2  168.   123  3.92  3.44  18.3     1     0     4     4 FALSE      #>    `cyl=six` `cyl=eight` #>    <lgl>     <lgl>       #>  1 TRUE      FALSE       #>  2 TRUE      FALSE       #>  3 FALSE     FALSE       #>  4 TRUE      FALSE       #>  5 FALSE     TRUE        #>  6 TRUE      FALSE       #>  7 FALSE     TRUE        #>  8 FALSE     FALSE       #>  9 FALSE     FALSE       #> 10 TRUE      FALSE       #> # ℹ 22 more rows partition(mtcars_example, vs:gear, .method = \"dummy\") #> # A tibble: 32 × 15 #>      mpg cyl    disp    hp  drat    wt  qsec  carb `vs=0` `vs=1` `am=0` `am=1` #>    <dbl> <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <lgl>  <lgl>  <lgl>  <lgl>  #>  1  21   six    160    110  3.9   2.62  16.5     4 TRUE   FALSE  FALSE  TRUE   #>  2  21   six    160    110  3.9   2.88  17.0     4 TRUE   FALSE  FALSE  TRUE   #>  3  22.8 four   108     93  3.85  2.32  18.6     1 FALSE  TRUE   FALSE  TRUE   #>  4  21.4 six    258    110  3.08  3.22  19.4     1 FALSE  TRUE   TRUE   FALSE  #>  5  18.7 eight  360    175  3.15  3.44  17.0     2 TRUE   FALSE  TRUE   FALSE  #>  6  18.1 six    225    105  2.76  3.46  20.2     1 FALSE  TRUE   TRUE   FALSE  #>  7  14.3 eight  360    245  3.21  3.57  15.8     4 TRUE   FALSE  TRUE   FALSE  #>  8  24.4 four   147.    62  3.69  3.19  20       2 FALSE  TRUE   TRUE   FALSE  #>  9  22.8 four   141.    95  3.92  3.15  22.9     2 FALSE  TRUE   TRUE   FALSE  #> 10  19.2 six    168.   123  3.92  3.44  18.3     4 FALSE  TRUE   TRUE   FALSE  #>    `gear=3` `gear=4` `gear=5` #>    <lgl>    <lgl>    <lgl>    #>  1 FALSE    TRUE     FALSE    #>  2 FALSE    TRUE     FALSE    #>  3 FALSE    TRUE     FALSE    #>  4 TRUE     FALSE    FALSE    #>  5 TRUE     FALSE    FALSE    #>  6 TRUE     FALSE    FALSE    #>  7 TRUE     FALSE    FALSE    #>  8 FALSE    TRUE     FALSE    #>  9 FALSE    TRUE     FALSE    #> 10 FALSE    TRUE     FALSE    #> # ℹ 22 more rows partition(mtcars_example, mpg, .method = \"crisp\", .breaks = c(-Inf, 15, 20, 30, Inf)) #> # A tibble: 32 × 14 #>    cyl    disp    hp  drat    wt  qsec    vs    am  gear  carb `mpg=(-Inf;15]` #>    <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <lgl>           #>  1 six    160    110  3.9   2.62  16.5     0     1     4     4 FALSE           #>  2 six    160    110  3.9   2.88  17.0     0     1     4     4 FALSE           #>  3 four   108     93  3.85  2.32  18.6     1     1     4     1 FALSE           #>  4 six    258    110  3.08  3.22  19.4     1     0     3     1 FALSE           #>  5 eight  360    175  3.15  3.44  17.0     0     0     3     2 FALSE           #>  6 six    225    105  2.76  3.46  20.2     1     0     3     1 FALSE           #>  7 eight  360    245  3.21  3.57  15.8     0     0     3     4 TRUE            #>  8 four   147.    62  3.69  3.19  20       1     0     4     2 FALSE           #>  9 four   141.    95  3.92  3.15  22.9     1     0     4     2 FALSE           #> 10 six    168.   123  3.92  3.44  18.3     1     0     4     4 FALSE           #>    `mpg=(15;20]` `mpg=(20;30]` `mpg=(30;Inf]` #>    <lgl>         <lgl>         <lgl>          #>  1 FALSE         TRUE          FALSE          #>  2 FALSE         TRUE          FALSE          #>  3 FALSE         TRUE          FALSE          #>  4 FALSE         TRUE          FALSE          #>  5 TRUE          FALSE         FALSE          #>  6 TRUE          FALSE         FALSE          #>  7 FALSE         FALSE         FALSE          #>  8 FALSE         TRUE          FALSE          #>  9 FALSE         TRUE          FALSE          #> 10 TRUE          FALSE         FALSE          #> # ℹ 22 more rows partition(mtcars_example, disp, .method = \"crisp\", .breaks = 3) #> # A tibble: 32 × 13 #>      mpg cyl      hp  drat    wt  qsec    vs    am  gear  carb `disp=(-Inf;205]` #>    <dbl> <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <lgl>             #>  1  21   six     110  3.9   2.62  16.5     0     1     4     4 TRUE              #>  2  21   six     110  3.9   2.88  17.0     0     1     4     4 TRUE              #>  3  22.8 four     93  3.85  2.32  18.6     1     1     4     1 TRUE              #>  4  21.4 six     110  3.08  3.22  19.4     1     0     3     1 FALSE             #>  5  18.7 eight   175  3.15  3.44  17.0     0     0     3     2 FALSE             #>  6  18.1 six     105  2.76  3.46  20.2     1     0     3     1 FALSE             #>  7  14.3 eight   245  3.21  3.57  15.8     0     0     3     4 FALSE             #>  8  24.4 four     62  3.69  3.19  20       1     0     4     2 TRUE              #>  9  22.8 four     95  3.92  3.15  22.9     1     0     4     2 TRUE              #> 10  19.2 six     123  3.92  3.44  18.3     1     0     4     4 TRUE              #>    `disp=(205;338]` `disp=(338;Inf]` #>    <lgl>            <lgl>            #>  1 FALSE            FALSE            #>  2 FALSE            FALSE            #>  3 FALSE            FALSE            #>  4 TRUE             FALSE            #>  5 FALSE            TRUE             #>  6 TRUE             FALSE            #>  7 FALSE            TRUE             #>  8 FALSE            FALSE            #>  9 FALSE            FALSE            #> 10 FALSE            FALSE            #> # ℹ 22 more rows crisp_mtcars <- mtcars_example |>     partition(cyl, vs:gear, .method = \"dummy\") |>     partition(mpg, .method = \"crisp\", .breaks = c(-Inf, 15, 20, 30, Inf)) |>     partition(disp:carb, .method = \"crisp\", .breaks = 3)   head(crisp_mtcars, n = 3) #> # A tibble: 3 × 32 #>   `cyl=four` `cyl=six` `cyl=eight` `vs=0` `vs=1` `am=0` `am=1` `gear=3` `gear=4` #>   <lgl>      <lgl>     <lgl>       <lgl>  <lgl>  <lgl>  <lgl>  <lgl>    <lgl>    #> 1 FALSE      TRUE      FALSE       TRUE   FALSE  FALSE  TRUE   FALSE    TRUE     #> 2 FALSE      TRUE      FALSE       TRUE   FALSE  FALSE  TRUE   FALSE    TRUE     #> 3 TRUE       FALSE     FALSE       FALSE  TRUE   FALSE  TRUE   FALSE    TRUE     #>   `gear=5` `mpg=(-Inf;15]` `mpg=(15;20]` `mpg=(20;30]` `mpg=(30;Inf]` #>   <lgl>    <lgl>           <lgl>         <lgl>         <lgl>          #> 1 FALSE    FALSE           FALSE         TRUE          FALSE          #> 2 FALSE    FALSE           FALSE         TRUE          FALSE          #> 3 FALSE    FALSE           FALSE         TRUE          FALSE          #>   `disp=(-Inf;205]` `disp=(205;338]` `disp=(338;Inf]` `hp=(-Inf;146]` #>   <lgl>             <lgl>            <lgl>            <lgl>           #> 1 TRUE              FALSE            FALSE            TRUE            #> 2 TRUE              FALSE            FALSE            TRUE            #> 3 TRUE              FALSE            FALSE            TRUE            #>   `hp=(146;241]` `hp=(241;Inf]` `drat=(-Inf;3.48]` `drat=(3.48;4.21]` #>   <lgl>          <lgl>          <lgl>              <lgl>              #> 1 FALSE          FALSE          FALSE              TRUE               #> 2 FALSE          FALSE          FALSE              TRUE               #> 3 FALSE          FALSE          FALSE              TRUE               #>   `drat=(4.21;Inf]` `wt=(-Inf;2.82]` `wt=(2.82;4.12]` `wt=(4.12;Inf]` #>   <lgl>             <lgl>            <lgl>            <lgl>           #> 1 FALSE             TRUE             FALSE            FALSE           #> 2 FALSE             FALSE            TRUE             FALSE           #> 3 FALSE             TRUE             FALSE            FALSE           #>   `qsec=(-Inf;17.3]` `qsec=(17.3;20.1]` `qsec=(20.1;Inf]` `carb=(-Inf;3.33]` #>   <lgl>              <lgl>              <lgl>             <lgl>              #> 1 TRUE               FALSE              FALSE             FALSE              #> 2 TRUE               FALSE              FALSE             FALSE              #> 3 FALSE              TRUE               FALSE             TRUE               #>   `carb=(3.33;5.67]` `carb=(5.67;Inf]` #>   <lgl>              <lgl>             #> 1 TRUE               FALSE             #> 2 TRUE               FALSE             #> 3 FALSE              FALSE"},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"data-driven-breakpoint-selection-with--style","dir":"Articles","previous_headings":"Data Preparation with partition() > Preparation of Crisp (Boolean) Predicates","what":"Data-Driven Breakpoint Selection with .style","title":"Data Preparation","text":".breaks specified single integer (number intervals), partition() function can use various data-driven methods determine optimal breakpoints, rather simply dividing range equal-width intervals. controlled .style argument, leverages methods classInt package. .style argument supports following methods: \"equal\" (default) – equal-width intervals across column range \"quantile\" – equal-frequency intervals (quantile-based) \"kmeans\" – intervals found 1D k-means clustering \"sd\" – intervals based standard deviations mean \"hclust\" – hierarchical clustering intervals \"bclust\" – model-based clustering intervals \"fisher\" / \"jenks\" – Fisher–Jenks optimal partitioning \"dpih\" – kernel-based density partitioning \"headtails\" – head/tails natural breaks \"maximum\" – maximization-based partitioning \"box\" – breaks boxplot hinges methods particularly useful data distribution skewed natural clusters. example, quantile-based partitioning ensures interval contains approximately number observations, can valuable imbalanced datasets. examples using CO2 dataset: .style_params argument allows pass additional parameters underlying algorithm. named list arguments accepted respective method classInt::classIntervals(). example, using k-means clustering, can specify algorithm: using quantile-based intervals, can control quantile type: data-driven methods can produce meaningful intervals better reflect structure data, leading interpretable patterns subsequent analysis.","code":"# Equal-width intervals (default) partition(CO2, conc, .method = \"crisp\", .breaks = 4, .style = \"equal\") #> # A tibble: 84 × 8 #>    Plant Type   Treatment  uptake `conc=(-Inf;321]` `conc=(321;548]` #>    <ord> <fct>  <fct>       <dbl> <lgl>             <lgl>            #>  1 Qn1   Quebec nonchilled   16   TRUE              FALSE            #>  2 Qn1   Quebec nonchilled   30.4 TRUE              FALSE            #>  3 Qn1   Quebec nonchilled   34.8 TRUE              FALSE            #>  4 Qn1   Quebec nonchilled   37.2 FALSE             TRUE             #>  5 Qn1   Quebec nonchilled   35.3 FALSE             TRUE             #>  6 Qn1   Quebec nonchilled   39.2 FALSE             FALSE            #>  7 Qn1   Quebec nonchilled   39.7 FALSE             FALSE            #>  8 Qn2   Quebec nonchilled   13.6 TRUE              FALSE            #>  9 Qn2   Quebec nonchilled   27.3 TRUE              FALSE            #> 10 Qn2   Quebec nonchilled   37.1 TRUE              FALSE            #>    `conc=(548;774]` `conc=(774;Inf]` #>    <lgl>            <lgl>            #>  1 FALSE            FALSE            #>  2 FALSE            FALSE            #>  3 FALSE            FALSE            #>  4 FALSE            FALSE            #>  5 FALSE            FALSE            #>  6 TRUE             FALSE            #>  7 FALSE            TRUE             #>  8 FALSE            FALSE            #>  9 FALSE            FALSE            #> 10 FALSE            FALSE            #> # ℹ 74 more rows # Quantile-based intervals (equal frequency in each interval) partition(CO2, conc, .method = \"crisp\", .breaks = 4, .style = \"quantile\") #> # A tibble: 84 × 8 #>    Plant Type   Treatment  uptake `conc=(-Inf;175]` `conc=(175;350]` #>    <ord> <fct>  <fct>       <dbl> <lgl>             <lgl>            #>  1 Qn1   Quebec nonchilled   16   TRUE              FALSE            #>  2 Qn1   Quebec nonchilled   30.4 TRUE              FALSE            #>  3 Qn1   Quebec nonchilled   34.8 FALSE             TRUE             #>  4 Qn1   Quebec nonchilled   37.2 FALSE             TRUE             #>  5 Qn1   Quebec nonchilled   35.3 FALSE             FALSE            #>  6 Qn1   Quebec nonchilled   39.2 FALSE             FALSE            #>  7 Qn1   Quebec nonchilled   39.7 FALSE             FALSE            #>  8 Qn2   Quebec nonchilled   13.6 TRUE              FALSE            #>  9 Qn2   Quebec nonchilled   27.3 TRUE              FALSE            #> 10 Qn2   Quebec nonchilled   37.1 FALSE             TRUE             #>    `conc=(350;675]` `conc=(675;Inf]` #>    <lgl>            <lgl>            #>  1 FALSE            FALSE            #>  2 FALSE            FALSE            #>  3 FALSE            FALSE            #>  4 FALSE            FALSE            #>  5 TRUE             FALSE            #>  6 TRUE             FALSE            #>  7 FALSE            TRUE             #>  8 FALSE            FALSE            #>  9 FALSE            FALSE            #> 10 FALSE            FALSE            #> # ℹ 74 more rows # K-means clustering to find natural breakpoints partition(CO2, conc, .method = \"crisp\", .breaks = 4, .style = \"kmeans\") #> # A tibble: 84 × 8 #>    Plant Type   Treatment  uptake `conc=(-Inf;212]` `conc=(212;425]` #>    <ord> <fct>  <fct>       <dbl> <lgl>             <lgl>            #>  1 Qn1   Quebec nonchilled   16   TRUE              FALSE            #>  2 Qn1   Quebec nonchilled   30.4 TRUE              FALSE            #>  3 Qn1   Quebec nonchilled   34.8 FALSE             TRUE             #>  4 Qn1   Quebec nonchilled   37.2 FALSE             TRUE             #>  5 Qn1   Quebec nonchilled   35.3 FALSE             FALSE            #>  6 Qn1   Quebec nonchilled   39.2 FALSE             FALSE            #>  7 Qn1   Quebec nonchilled   39.7 FALSE             FALSE            #>  8 Qn2   Quebec nonchilled   13.6 TRUE              FALSE            #>  9 Qn2   Quebec nonchilled   27.3 TRUE              FALSE            #> 10 Qn2   Quebec nonchilled   37.1 FALSE             TRUE             #>    `conc=(425;838]` `conc=(838;Inf]` #>    <lgl>            <lgl>            #>  1 FALSE            FALSE            #>  2 FALSE            FALSE            #>  3 FALSE            FALSE            #>  4 FALSE            FALSE            #>  5 TRUE             FALSE            #>  6 TRUE             FALSE            #>  7 FALSE            TRUE             #>  8 FALSE            FALSE            #>  9 FALSE            FALSE            #> 10 FALSE            FALSE            #> # ℹ 74 more rows # Standard deviation-based intervals partition(CO2, conc, .method = \"crisp\", .breaks = 4, .style = \"sd\") #> # A tibble: 84 × 8 #>    Plant Type   Treatment  uptake `conc=(-Inf;139]` `conc=(139;435]` #>    <ord> <fct>  <fct>       <dbl> <lgl>             <lgl>            #>  1 Qn1   Quebec nonchilled   16   TRUE              FALSE            #>  2 Qn1   Quebec nonchilled   30.4 FALSE             TRUE             #>  3 Qn1   Quebec nonchilled   34.8 FALSE             TRUE             #>  4 Qn1   Quebec nonchilled   37.2 FALSE             TRUE             #>  5 Qn1   Quebec nonchilled   35.3 FALSE             FALSE            #>  6 Qn1   Quebec nonchilled   39.2 FALSE             FALSE            #>  7 Qn1   Quebec nonchilled   39.7 FALSE             FALSE            #>  8 Qn2   Quebec nonchilled   13.6 TRUE              FALSE            #>  9 Qn2   Quebec nonchilled   27.3 FALSE             TRUE             #> 10 Qn2   Quebec nonchilled   37.1 FALSE             TRUE             #>    `conc=(435;731]` `conc=(731;Inf]` #>    <lgl>            <lgl>            #>  1 FALSE            FALSE            #>  2 FALSE            FALSE            #>  3 FALSE            FALSE            #>  4 FALSE            FALSE            #>  5 TRUE             FALSE            #>  6 TRUE             FALSE            #>  7 FALSE            TRUE             #>  8 FALSE            FALSE            #>  9 FALSE            FALSE            #> 10 FALSE            FALSE            #> # ℹ 74 more rows # Use Lloyd's algorithm for k-means partition(CO2, conc, .method = \"crisp\", .breaks = 4,            .style = \"kmeans\",            .style_params = list(algorithm = \"Lloyd\")) #> # A tibble: 84 × 8 #>    Plant Type   Treatment  uptake `conc=(-Inf;300]` `conc=(300;588]` #>    <ord> <fct>  <fct>       <dbl> <lgl>             <lgl>            #>  1 Qn1   Quebec nonchilled   16   TRUE              FALSE            #>  2 Qn1   Quebec nonchilled   30.4 TRUE              FALSE            #>  3 Qn1   Quebec nonchilled   34.8 TRUE              FALSE            #>  4 Qn1   Quebec nonchilled   37.2 FALSE             TRUE             #>  5 Qn1   Quebec nonchilled   35.3 FALSE             TRUE             #>  6 Qn1   Quebec nonchilled   39.2 FALSE             FALSE            #>  7 Qn1   Quebec nonchilled   39.7 FALSE             FALSE            #>  8 Qn2   Quebec nonchilled   13.6 TRUE              FALSE            #>  9 Qn2   Quebec nonchilled   27.3 TRUE              FALSE            #> 10 Qn2   Quebec nonchilled   37.1 TRUE              FALSE            #>    `conc=(588;838]` `conc=(838;Inf]` #>    <lgl>            <lgl>            #>  1 FALSE            FALSE            #>  2 FALSE            FALSE            #>  3 FALSE            FALSE            #>  4 FALSE            FALSE            #>  5 FALSE            FALSE            #>  6 TRUE             FALSE            #>  7 FALSE            TRUE             #>  8 FALSE            FALSE            #>  9 FALSE            FALSE            #> 10 FALSE            FALSE            #> # ℹ 74 more rows # Use different quantile types (see ?quantile for details) partition(CO2, conc, .method = \"crisp\", .breaks = 4,            .style = \"quantile\",            .style_params = list(type = 7)) #> # A tibble: 84 × 8 #>    Plant Type   Treatment  uptake `conc=(-Inf;175]` `conc=(175;350]` #>    <ord> <fct>  <fct>       <dbl> <lgl>             <lgl>            #>  1 Qn1   Quebec nonchilled   16   TRUE              FALSE            #>  2 Qn1   Quebec nonchilled   30.4 TRUE              FALSE            #>  3 Qn1   Quebec nonchilled   34.8 FALSE             TRUE             #>  4 Qn1   Quebec nonchilled   37.2 FALSE             TRUE             #>  5 Qn1   Quebec nonchilled   35.3 FALSE             FALSE            #>  6 Qn1   Quebec nonchilled   39.2 FALSE             FALSE            #>  7 Qn1   Quebec nonchilled   39.7 FALSE             FALSE            #>  8 Qn2   Quebec nonchilled   13.6 TRUE              FALSE            #>  9 Qn2   Quebec nonchilled   27.3 TRUE              FALSE            #> 10 Qn2   Quebec nonchilled   37.1 FALSE             TRUE             #>    `conc=(350;675]` `conc=(675;Inf]` #>    <lgl>            <lgl>            #>  1 FALSE            FALSE            #>  2 FALSE            FALSE            #>  3 FALSE            FALSE            #>  4 FALSE            FALSE            #>  5 TRUE             FALSE            #>  6 TRUE             FALSE            #>  7 FALSE            TRUE             #>  8 FALSE            FALSE            #>  9 FALSE            FALSE            #> 10 FALSE            FALSE            #> # ℹ 74 more rows"},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"preparation-of-triangular-and-raised-cosine-fuzzy-predicates","dir":"Articles","previous_headings":"Data Preparation with partition()","what":"Preparation of Triangular and Raised-Cosine Fuzzy Predicates","title":"Data Preparation","text":"many real-world datasets, numeric attributes lend clear-cut, crisp boundaries. example, deciding whether car “low mileage” “high mileage” often subjective. vehicle 19 miles per gallon may considered “low” one context “medium” another. Crisp intervals force strict separation categories, can rigid may lose information gradual changes data. address , fuzzy predicates used. fuzzy predicate expresses degree condition satisfied. Instead strictly TRUE FALSE (although allowed ), predicate represented number interval \\([0,1]\\). truth degree 0 means predicate entirely false, 1 means fully true, values indicate partial membership. allows us model smooth transitions categories capture nuanced patterns. example, fuzzy predicate represent “medium horsepower” mtcars dataset. car 120 hp may belong category degree 0.8, car 150 hp may belong degree 0.2. representations faithful human reasoning often yield patterns robust interpretable. transformation numeric columns fuzzy predicates can done partition() function. crisp partitioning, factors transformed dummy logical columns. Numeric columns, however, transformed fuzzy truth values. partition() function provides two fuzzy partitioning methods: .method = \"triangle\" creates fuzzy sets triangular trapezoidal membership functions; .method = \"raisedcos\" creates fuzzy sets raised cosine trapezoidal raised-cosine membership functions. membership functions specify strongly value belongs fuzzy set. choice function depends desired smoothness transition sets. advanced fuzzy partitioning numeric columns can achieved lfl package, provides tools defining fuzzy sets many types, including linguistic terms “small” “extremely big”. See lfl documentation information. triangular raised cosine shapes fully defined three points: left border, peak, right border. .breaks argument partition() function specifies points. See following figure illustration triangular raised cosine membership functions .breaks = c(-10, 0, 10): Comparison triangular raised cosine membership functions .breaks = c(-10, 0, 10) consecutive triplet values .breaks defines one fuzzy set. create e.g. three fuzzy sets, five break points needed. instance, .breaks = c(-10, -5, 0, 5, 10) defines three fuzzy sets peaks -5, 0, 5. See following figure illustration fuzzy sets: Fuzzy sets triangular membership functions partition(x, .method = \"triangle\", .breaks = c(-10, -5, 0, 5, 10)) often useful extend fuzzy sets edges infinity. ensures values covered fuzzy sets. achieve , -Inf Inf can added first last elements .breaks vector: Fuzzy sets triangular membership functions partition(x, .method = \"triangle\", .breaks = c(-Inf, -5, 0, 5, Inf)) regular partitioning range values desired, .breaks can set single integer, specifies number fuzzy sets create. example, .breaks = 4 creates partitioning four fuzzy sets: Fuzzy sets triangular membership functions partition(x, .method = \"triangle\", .breaks = 4) valid raised cosine fuzzy sets. instance, following figure shows five raised cosine fuzzy sets defined .breaks = c(-Inf, -10, -5, 0, 5, 10, Inf): Fuzzy sets raised cosine membership functions partition(x, .method = \"raisedcos\", .breaks = c(-Inf, -10, -5, 0, 5, 10, Inf)) fuzzy transformation whole mtcars dataset can done follows: Note cyl, vs, , gear columns still represented dummy logical columns, mpg, disp, columns now represented fuzzy sets. combination allows crisp fuzzy predicates used together pattern discovery, offering flexibility interpretability.","code":"# Start with a fresh copy of mtcars fuzzy_mtcars <- mtcars |>     mutate(cyl = factor(cyl, levels = c(4, 6, 8), labels = c(\"four\", \"six\", \"eight\"))) |>     partition(cyl, vs:gear, .method = \"dummy\") |>     partition(mpg, .method = \"triangle\", .breaks = c(-Inf, 15, 20, 30, Inf)) |>     partition(disp:carb, .method = \"triangle\", .breaks = 3)   head(fuzzy_mtcars, n = 3) #> # A tibble: 3 × 31 #>   `cyl=four` `cyl=six` `cyl=eight` `vs=0` `vs=1` `am=0` `am=1` `gear=3` `gear=4` #>   <lgl>      <lgl>     <lgl>       <lgl>  <lgl>  <lgl>  <lgl>  <lgl>    <lgl>    #> 1 FALSE      TRUE      FALSE       TRUE   FALSE  FALSE  TRUE   FALSE    TRUE     #> 2 FALSE      TRUE      FALSE       TRUE   FALSE  FALSE  TRUE   FALSE    TRUE     #> 3 TRUE       FALSE     FALSE       FALSE  TRUE   FALSE  TRUE   FALSE    TRUE     #>   `gear=5` `mpg=(-Inf;15;20)` `mpg=(15;20;30)` `mpg=(20;30;Inf)` #>   <lgl>                 <dbl>            <dbl>             <dbl> #> 1 FALSE                     0             0.9               0.1  #> 2 FALSE                     0             0.9               0.1  #> 3 FALSE                     0             0.72              0.28 #>   `disp=(-Inf;71.1;272)` `disp=(71.1;272;472)` `disp=(272;472;Inf)` #>                    <dbl>                 <dbl>                <dbl> #> 1                  0.557                 0.443                    0 #> 2                  0.557                 0.443                    0 #> 3                  0.816                 0.184                    0 #>   `hp=(-Inf;52;194)` `hp=(52;194;335)` `hp=(194;335;Inf)` #>                <dbl>             <dbl>              <dbl> #> 1              0.592             0.408                  0 #> 2              0.592             0.408                  0 #> 3              0.711             0.289                  0 #>   `drat=(-Inf;2.76;3.84)` `drat=(2.76;3.84;4.93)` `drat=(3.84;4.93;Inf)` #>                     <dbl>                   <dbl>                  <dbl> #> 1                       0                   0.945                0.0550  #> 2                       0                   0.945                0.0550  #> 3                       0                   0.991                0.00917 #>   `wt=(-Inf;1.51;3.47)` `wt=(1.51;3.47;5.42)` `wt=(3.47;5.42;Inf)` #>                   <dbl>                 <dbl>                <dbl> #> 1                 0.434                 0.566                    0 #> 2                 0.304                 0.696                    0 #> 3                 0.587                 0.413                    0 #>   `qsec=(-Inf;14.5;18.7)` `qsec=(14.5;18.7;22.9)` `qsec=(18.7;22.9;Inf)` #>                     <dbl>                   <dbl>                  <dbl> #> 1                  0.533                    0.467                      0 #> 2                  0.4                      0.6                        0 #> 3                  0.0214                   0.979                      0 #>   `carb=(-Inf;1;4.5)` `carb=(1;4.5;8)` `carb=(4.5;8;Inf)` #>                 <dbl>            <dbl>              <dbl> #> 1               0.143            0.857                  0 #> 2               0.143            0.857                  0 #> 3               1                0                      0"},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"preparation-of-trapezoidal-fuzzy-predicates","dir":"Articles","previous_headings":"Data Preparation with partition()","what":"Preparation of Trapezoidal Fuzzy Predicates","title":"Data Preparation","text":"triangular raised cosine membership functions often sufficient capture gradual transitions numeric data. However, situations useful fuzzy sets stay fully true (membership = 1) wider interval decreasing . generalization corresponds trapezoidal fuzzy set, can seen triangle raised cosine “flat top”. partition(), trapezoids can defined \"triangle\" \"raisedcos\" methods controlling many consecutive break points constitute one fuzzy set far window shifts along breaks. can accomplished .span .inc arguments: .span - specifies width flat top terms number break intervals merged. .inc - shift window along .breaks forming next fuzzy set. default, .span = 1 .inc = 1, means fuzzy set triangular raised cosine. Setting .span value greater 1 creates trapezoidal fuzzy sets. .span = 2, fuzzy set defined four consecutive break points - flat top spans two break intervals. following figure result setting .span = 2 .breaks = c(-10, -5, 5, 10): Fuzzy sets triangular membership functions partition(x, .method = \"triangle\", .span = 2, .breaks = c(-10, -5, 5, 10)) Additional fuzzy sets created shifting window along break points. shift controlled .inc argument. default, .inc = 1, means window shifts one break point. Consider following example shows effect setting .inc = 1 addition .span = 2 .breaks = c(-15, -10, -5, 0, 5, 10, 15): Fuzzy sets triangular membership functions partition(x, .method = \"triangle\", .inc = 1, .span = 2, .breaks = c(-15, -10, -5, 0, 5, 10, 15)) Setting .inc value greater 1 modifies shift window along break points. example, .inc = 3, window shifts three break points, effectively skips two fuzzy sets created fuzzy set: Fuzzy sets triangular membership functions partition(x, .method = \"triangle\", .inc = 3, .span = 2, .breaks = c(-15, -10, -5, 0, 5, 10, 15))","code":""},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"identifying-and-removing-uninformative-columns","dir":"Articles","previous_headings":"","what":"Identifying and Removing Uninformative Columns","title":"Data Preparation","text":"preparing data pattern discovery, important identify potentially remove columns provide little useful information. nuggets package provides two functions purpose: is_almost_constant() remove_almost_constant().","code":""},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"testing-for-almost-constant-columns","dir":"Articles","previous_headings":"Identifying and Removing Uninformative Columns","what":"Testing for Almost Constant Columns","title":"Data Preparation","text":"is_almost_constant() function checks whether vector contains (almost) value majority elements. useful detecting low-variability degenerate variables. function returns TRUE proportion frequent value vector greater equal specified threshold (default 1.0, meaning completely constant). function also handles NA values appropriately:","code":"# Completely constant vector is_almost_constant(c(1, 1, 1, 1, 1)) #> [1] TRUE  # Variable vector is_almost_constant(c(1, 2, 3, 4, 5)) #> [1] FALSE  # Almost constant (80% are the same value) is_almost_constant(c(1, 1, 1, 1, 2), threshold = 0.8) #> [1] TRUE  # Not almost constant with threshold 0.8 is_almost_constant(c(1, 1, 1, 2, 2), threshold = 0.8) #> [1] FALSE # With NA values - by default NA is treated as a regular value is_almost_constant(c(NA, NA, NA, 1, 2), threshold = 0.5) #> [1] TRUE  # With NA removed before computing proportions is_almost_constant(c(NA, NA, NA, 1, 2), threshold = 0.5, na_rm = TRUE) #> [1] TRUE"},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"removing-almost-constant-columns","dir":"Articles","previous_headings":"Identifying and Removing Uninformative Columns","what":"Removing Almost Constant Columns","title":"Data Preparation","text":"remove_almost_constant() function extends is_almost_constant() work entire data frames. tests selected columns removes almost constant according specified threshold. can also restrict check subset columns using tidyselect syntax: function particularly useful applying partition() dataset. generated predicates may (almost) constant thus uninformative pattern discovery. Removing can significantly speed subsequent mining process. example:","code":"# Create a data frame with some constant and variable columns d <- data.frame(   a1 = 1:10,              # variable   a2 = c(1:9, NA),        # variable   b1 = \"b\",               # constant   b2 = NA,                # constant (all NA)   c1 = rep(c(TRUE, FALSE), 5),  # variable   c2 = rep(c(TRUE, NA), 5),     # 50% TRUE, 50% NA   d  = c(rep(TRUE, 4), rep(FALSE, 4), NA, NA)  # 40% TRUE, 40% FALSE, 20% NA )  # Remove columns that are completely constant remove_almost_constant(d, .threshold = 1.0, .na_rm = FALSE) #> # A tibble: 10 × 5 #>       a1    a2 c1    c2    d     #>    <int> <int> <lgl> <lgl> <lgl> #>  1     1     1 TRUE  TRUE  TRUE  #>  2     2     2 FALSE NA    TRUE  #>  3     3     3 TRUE  TRUE  TRUE  #>  4     4     4 FALSE NA    TRUE  #>  5     5     5 TRUE  TRUE  FALSE #>  6     6     6 FALSE NA    FALSE #>  7     7     7 TRUE  TRUE  FALSE #>  8     8     8 FALSE NA    FALSE #>  9     9     9 TRUE  TRUE  NA    #> 10    10    NA FALSE NA    NA  # Remove columns where the majority value occurs in >= 50% of rows remove_almost_constant(d, .threshold = 0.5, .na_rm = FALSE) #> # A tibble: 10 × 3 #>       a1    a2 d     #>    <int> <int> <lgl> #>  1     1     1 TRUE  #>  2     2     2 TRUE  #>  3     3     3 TRUE  #>  4     4     4 TRUE  #>  5     5     5 FALSE #>  6     6     6 FALSE #>  7     7     7 FALSE #>  8     8     8 FALSE #>  9     9     9 NA    #> 10    10    NA NA  # Same as above, but removing NA before computing proportions remove_almost_constant(d, .threshold = 0.5, .na_rm = TRUE) #> # A tibble: 10 × 2 #>       a1    a2 #>    <int> <int> #>  1     1     1 #>  2     2     2 #>  3     3     3 #>  4     4     4 #>  5     5     5 #>  6     6     6 #>  7     7     7 #>  8     8     8 #>  9     9     9 #> 10    10    NA # Only check columns a1 through b2 remove_almost_constant(d, a1:b2, .threshold = 0.5, .na_rm = TRUE) #> # A tibble: 10 × 5 #>       a1    a2 c1    c2    d     #>    <int> <int> <lgl> <lgl> <lgl> #>  1     1     1 TRUE  TRUE  TRUE  #>  2     2     2 FALSE NA    TRUE  #>  3     3     3 TRUE  TRUE  TRUE  #>  4     4     4 FALSE NA    TRUE  #>  5     5     5 TRUE  TRUE  FALSE #>  6     6     6 FALSE NA    FALSE #>  7     7     7 TRUE  TRUE  FALSE #>  8     8     8 FALSE NA    FALSE #>  9     9     9 TRUE  TRUE  NA    #> 10    10    NA FALSE NA    NA # Prepare mtcars data with partition - use fresh copy prepared_data <- mtcars |>     mutate(cyl = factor(cyl, levels = c(4, 6, 8), labels = c(\"four\", \"six\", \"eight\"))) |>     partition(cyl, vs:gear, .method = \"dummy\") |>     partition(mpg:carb, .method = \"crisp\", .breaks = 3)  # Check for and remove any almost constant columns prepared_data <- remove_almost_constant(prepared_data,                                         .threshold = 0.95,                                         .verbose = TRUE) #> Removing (almost) constant columns:"},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"finding-tautologies-in-data","dir":"Articles","previous_headings":"","what":"Finding Tautologies in Data","title":"Data Preparation","text":"preparing data partition() methods, can useful identify tautologies—rules always almost always true dataset. dig_tautologies() function helps find patterns, can used filter redundant conditions subsequent pattern discovery.","code":""},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"what-are-tautologies","dir":"Articles","previous_headings":"Finding Tautologies in Data","what":"What are Tautologies?","title":"Data Preparation","text":"tautology context rule form {a1 & a2 & ... & } => {c} antecedent (left side) almost always implies consequent (right side). rules hold high confidence specific dataset. example, dataset vehicles, might discover: - engine_type=electric => fuel_type=electricity (confidence ≈ 1.0) - manual_transmission=TRUE => automatic_transmission=FALSE (confidence = 1.0) tautological rules, true, may provide interesting insights analysis. Identifying allows exclude similar conditions complex pattern searches.","code":""},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"using-dig_tautologies","dir":"Articles","previous_headings":"Finding Tautologies in Data","what":"Using dig_tautologies()","title":"Data Preparation","text":"dig_tautologies() function works similarly dig_associations(), specifically optimized finding rules high confidence. searches iteratively, using tautologies found earlier iterations prune search space later iterations. Basic usage: function returns tibble format dig_associations(), containing rules quality measures (support, confidence, etc.).","code":"# Prepare fuzzy data - use fresh copy of mtcars fuzzy_mtcars <- mtcars |>     mutate(cyl = factor(cyl, levels = c(4, 6, 8), labels = c(\"four\", \"six\", \"eight\"))) |>     partition(cyl, vs:gear, .method = \"dummy\") |>     partition(mpg:carb, .method = \"triangle\", .breaks = 3)  # Create disjoint vector disj <- var_names(colnames(fuzzy_mtcars))  # Find tautologies with very high confidence tautologies <- dig_tautologies(     fuzzy_mtcars,     antecedent = everything(),     consequent = everything(),     disjoint = disj,     min_confidence = 0.95,     min_support = 0.1,     max_length = 3,     t_norm = \"goguen\" )  print(tautologies) #> # A tibble: 0 × 0"},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"parameters","dir":"Articles","previous_headings":"Finding Tautologies in Data","what":"Parameters","title":"Data Preparation","text":"Key parameters dig_tautologies() include: antecedent consequent: Tidyselect expressions defining columns can appear side rule. disjoint: vector specifying mutually exclusive predicates (predicates appear together condition). min_confidence: minimum confidence threshold. tautologies, typically set high (e.g., 0.9 0.95). min_support: minimum support threshold. ensures tautology based sufficient number observations. max_length: maximum number predicates antecedent. t_norm: t-norm use fuzzy conjunction (\"goedel\", \"goguen\", \"lukas\").","code":""},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"using-tautologies-to-filter-searches","dir":"Articles","previous_headings":"Finding Tautologies in Data","what":"Using Tautologies to Filter Searches","title":"Data Preparation","text":"’ve identified tautologies, can use excluded argument dig() related functions avoid generating similar conditions: approach can significantly reduce computation time help focus interesting patterns.","code":"# Convert tautologies to excluded format excluded_conditions <- parse_condition(tautologies$antecedent)  # Use in subsequent pattern search results <- dig_associations(     fuzzy_mtcars,     antecedent = !starts_with(\"am\"),     consequent = starts_with(\"am\"),     disjoint = disj,     excluded = excluded_conditions,  # Exclude tautological patterns     min_support = 0.1,     min_confidence = 0.8 )"},{"path":"https://beerda.github.io/nuggets/articles/data-preparation.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Data Preparation","text":"vignette covered essential data preparation techniques nuggets package: partition(): primary function transforming data crisp fuzzy predicates, support various partitioning methods including: Crisp (Boolean) partitioning configurable intervals Triangular raised-cosine fuzzy sets Trapezoidal fuzzy sets using .span .inc parameters is_almost_constant() remove_almost_constant(): Utility functions identifying removing uninformative columns low variability. dig_tautologies(): function finding tautological rules data, can used filter subsequent pattern searches. tools, can effectively prepare data pattern discovery using various dig_*() functions provided nuggets package. information pattern discovery , see main “Getting Started” vignette function documentation.","code":""},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"nuggets: Get Started","text":"Package nuggets searches patterns can expressed formulae form elementary conjunctions, referred text conditions. Conditions constructed predicates, correspond data columns. interpretation conditions depends choice underlying logic: Crisp (Boolean) logic: predicate takes values TRUE (1) FALSE (0). truth value condition computed according rules classical Boolean algebra. Fuzzy logic: predicate assigned truth degree interval \\([0, 1]\\). truth degree conjunction computed using chosen triangular norm (t-norm). package supports three common t-norms, defined predicates’ truth degrees \\(, b \\[0, 1]\\) follows: Gödel (minimum) t-norm: \\(\\min(, b)\\) ; Goguen (product) t-norm: \\(\\cdot b\\) ; Łukasiewicz t-norm: \\(\\max(0, + b - 1)\\) applying nuggets, data columns intended predicates must prepared either dichotomization (conversion dummy variables) transformation fuzzy sets. package provides functions transformations. See Data Preparation vignette comprehensive guide, section Data Preparation quick overview. nuggets implements functions search pre-defined types patterns, example: dig_associations() association rules, dig_baseline_contrasts(), dig_complement_contrasts(), dig_paired_baseline_contrasts() various contrast patterns numeric variables, dig_correlations() conditional correlations. See Pre-defined Patterns details. Discovered rules patterns can post-processed, visualized, explored interactively. Section Post-processing Visualization describes features. Finally, package allows users provide custom evaluation functions conditions search user-defined types patterns: dig() general function searching arbitrary pattern types. dig_grid() wrapper around dig() patterns defined conditions pair columns evaluated user-defined function. See Custom Patterns information.","code":""},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"Data Preparation","title":"nuggets: Get Started","text":"applying nuggets, data columns intended predicates must prepared either dichotomization (conversion dummy variables) transformation fuzzy sets. package provides partition() function transformations. detailed guide data preparation, including information available functions advanced techniques, please see Data Preparation vignette.","code":""},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"crisp-boolean-predicates-example","dir":"Articles","previous_headings":"Data Preparation","what":"Crisp (Boolean) Predicates Example","title":"nuggets: Get Started","text":"crisp patterns, numeric columns transformed logical (TRUE/FALSE) columns. ’s quick example using built-mtcars dataset: Now columns logical can used predicates crisp conditions.","code":"# Transform the whole dataset to crisp predicates # First, convert cyl to a factor for illustration crisp_mtcars <- mtcars |>     mutate(cyl = factor(cyl, levels = c(4, 6, 8), labels = c(\"four\", \"six\", \"eight\"))) |>     partition(cyl, vs:gear, .method = \"dummy\") |>     partition(mpg, .method = \"crisp\", .breaks = c(-Inf, 15, 20, 30, Inf)) |>     partition(disp:carb, .method = \"crisp\", .breaks = 3)   head(crisp_mtcars, n = 3) #> # A tibble: 3 × 32 #>   `cyl=four` `cyl=six` `cyl=eight` `vs=0` `vs=1` `am=0` `am=1` `gear=3` `gear=4` #>   <lgl>      <lgl>     <lgl>       <lgl>  <lgl>  <lgl>  <lgl>  <lgl>    <lgl>    #> 1 FALSE      TRUE      FALSE       TRUE   FALSE  FALSE  TRUE   FALSE    TRUE     #> 2 FALSE      TRUE      FALSE       TRUE   FALSE  FALSE  TRUE   FALSE    TRUE     #> 3 TRUE       FALSE     FALSE       FALSE  TRUE   FALSE  TRUE   FALSE    TRUE     #>   `gear=5` `mpg=(-Inf;15]` `mpg=(15;20]` `mpg=(20;30]` `mpg=(30;Inf]` #>   <lgl>    <lgl>           <lgl>         <lgl>         <lgl>          #> 1 FALSE    FALSE           FALSE         TRUE          FALSE          #> 2 FALSE    FALSE           FALSE         TRUE          FALSE          #> 3 FALSE    FALSE           FALSE         TRUE          FALSE          #>   `disp=(-Inf;205]` `disp=(205;338]` `disp=(338;Inf]` `hp=(-Inf;146]` #>   <lgl>             <lgl>            <lgl>            <lgl>           #> 1 TRUE              FALSE            FALSE            TRUE            #> 2 TRUE              FALSE            FALSE            TRUE            #> 3 TRUE              FALSE            FALSE            TRUE            #>   `hp=(146;241]` `hp=(241;Inf]` `drat=(-Inf;3.48]` `drat=(3.48;4.21]` #>   <lgl>          <lgl>          <lgl>              <lgl>              #> 1 FALSE          FALSE          FALSE              TRUE               #> 2 FALSE          FALSE          FALSE              TRUE               #> 3 FALSE          FALSE          FALSE              TRUE               #>   `drat=(4.21;Inf]` `wt=(-Inf;2.82]` `wt=(2.82;4.12]` `wt=(4.12;Inf]` #>   <lgl>             <lgl>            <lgl>            <lgl>           #> 1 FALSE             TRUE             FALSE            FALSE           #> 2 FALSE             FALSE            TRUE             FALSE           #> 3 FALSE             TRUE             FALSE            FALSE           #>   `qsec=(-Inf;17.3]` `qsec=(17.3;20.1]` `qsec=(20.1;Inf]` `carb=(-Inf;3.33]` #>   <lgl>              <lgl>              <lgl>             <lgl>              #> 1 TRUE               FALSE              FALSE             FALSE              #> 2 TRUE               FALSE              FALSE             FALSE              #> 3 FALSE              TRUE               FALSE             TRUE               #>   `carb=(3.33;5.67]` `carb=(5.67;Inf]` #>   <lgl>              <lgl>             #> 1 TRUE               FALSE             #> 2 TRUE               FALSE             #> 3 FALSE              FALSE"},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"fuzzy-predicates-example","dir":"Articles","previous_headings":"Data Preparation","what":"Fuzzy Predicates Example","title":"nuggets: Get Started","text":"Fuzzy predicates express degree condition satisfied, values interval \\([0,1]\\). allows modeling smooth transitions categories: Note cyl, vs, , gear columns still represented dummy logical columns, numeric columns now represented fuzzy sets. combination allows crisp fuzzy predicates used together pattern discovery.","code":"# Start with fresh mtcars and transform to fuzzy predicates fuzzy_mtcars <- mtcars |>     mutate(cyl = factor(cyl, levels = c(4, 6, 8), labels = c(\"four\", \"six\", \"eight\"))) |>     partition(cyl, vs:gear, .method = \"dummy\") |>     partition(mpg, .method = \"triangle\", .breaks = c(-Inf, 15, 20, 30, Inf)) |>     partition(disp:carb, .method = \"triangle\", .breaks = 3)   head(fuzzy_mtcars, n = 3) #> # A tibble: 3 × 31 #>   `cyl=four` `cyl=six` `cyl=eight` `vs=0` `vs=1` `am=0` `am=1` `gear=3` `gear=4` #>   <lgl>      <lgl>     <lgl>       <lgl>  <lgl>  <lgl>  <lgl>  <lgl>    <lgl>    #> 1 FALSE      TRUE      FALSE       TRUE   FALSE  FALSE  TRUE   FALSE    TRUE     #> 2 FALSE      TRUE      FALSE       TRUE   FALSE  FALSE  TRUE   FALSE    TRUE     #> 3 TRUE       FALSE     FALSE       FALSE  TRUE   FALSE  TRUE   FALSE    TRUE     #>   `gear=5` `mpg=(-Inf;15;20)` `mpg=(15;20;30)` `mpg=(20;30;Inf)` #>   <lgl>                 <dbl>            <dbl>             <dbl> #> 1 FALSE                     0             0.9               0.1  #> 2 FALSE                     0             0.9               0.1  #> 3 FALSE                     0             0.72              0.28 #>   `disp=(-Inf;71.1;272)` `disp=(71.1;272;472)` `disp=(272;472;Inf)` #>                    <dbl>                 <dbl>                <dbl> #> 1                  0.557                 0.443                    0 #> 2                  0.557                 0.443                    0 #> 3                  0.816                 0.184                    0 #>   `hp=(-Inf;52;194)` `hp=(52;194;335)` `hp=(194;335;Inf)` #>                <dbl>             <dbl>              <dbl> #> 1              0.592             0.408                  0 #> 2              0.592             0.408                  0 #> 3              0.711             0.289                  0 #>   `drat=(-Inf;2.76;3.84)` `drat=(2.76;3.84;4.93)` `drat=(3.84;4.93;Inf)` #>                     <dbl>                   <dbl>                  <dbl> #> 1                       0                   0.945                0.0550  #> 2                       0                   0.945                0.0550  #> 3                       0                   0.991                0.00917 #>   `wt=(-Inf;1.51;3.47)` `wt=(1.51;3.47;5.42)` `wt=(3.47;5.42;Inf)` #>                   <dbl>                 <dbl>                <dbl> #> 1                 0.434                 0.566                    0 #> 2                 0.304                 0.696                    0 #> 3                 0.587                 0.413                    0 #>   `qsec=(-Inf;14.5;18.7)` `qsec=(14.5;18.7;22.9)` `qsec=(18.7;22.9;Inf)` #>                     <dbl>                   <dbl>                  <dbl> #> 1                  0.533                    0.467                      0 #> 2                  0.4                      0.6                        0 #> 3                  0.0214                   0.979                      0 #>   `carb=(-Inf;1;4.5)` `carb=(1;4.5;8)` `carb=(4.5;8;Inf)` #>                 <dbl>            <dbl>              <dbl> #> 1               0.143            0.857                  0 #> 2               0.143            0.857                  0 #> 3               1                0                      0"},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"advanced-data-preparation-capabilities","dir":"Articles","previous_headings":"Data Preparation","what":"Advanced Data Preparation Capabilities","title":"nuggets: Get Started","text":"nuggets package provides powerful flexible data preparation tools. Data Preparation vignette covers capabilities depth, including: Equal-width intervals uniform discretization Data-driven methods (quantile, k-means, hierarchical clustering, etc.) optimal breakpoints respect data structure Custom breakpoints domain-specific intervals Triangular membership functions basic fuzzy sets Raised-cosine membership functions smoother transitions Trapezoidal shapes using .span .inc parameters overlapping fuzzy sets is_almost_constant() remove_almost_constant() identify filter uninformative columns dig_tautologies() find always-true rules can used prune search spaces Custom labels predicates make discovered patterns interpretable example, can use quantile-based partitioning ensure balanced predicates, use raised-cosine fuzzy sets custom labels create meaningful linguistic terms like “very_low”, “low”, “medium”, “high”, “very_high”. preparation choices significantly impact interpretability usefulness patterns discovered subsequent analyses.","code":""},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"pre-defined-patterns","dir":"Articles","previous_headings":"","what":"Pre-defined Patterns","title":"nuggets: Get Started","text":"package nuggets provides set functions discovering best-known pattern types. functions can process Boolean data, fuzzy data, . function returns tibble, every row represents one detected pattern. Note: section assumes data already preprocessed — .e., transformed binarized fuzzified form. See previous section Data Preparation details prepare dataset (example, crisp_mtcars fuzzy_mtcars). advanced workflows — defining custom pattern types computing user-defined measures — see section Custom Patterns.","code":""},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"search-for-association-rules","dir":"Articles","previous_headings":"Pre-defined Patterns","what":"Search for Association Rules","title":"nuggets: Get Started","text":"Association rules identify conditions (antecedents) specific feature (consequent) present often. \\[ \\Rightarrow C \\] condition satisfied, feature C tends present. example,university_edu & middle_age & IT_industry => high_income can read :People middle age university education working industry likely high income. practice, antecedent set predicates, consequent C usually single predicate. set predicates \\(\\), let \\(\\text{supp}()\\) denote support — relative frequency (logical data) mean truth degree (fuzzy data) rows satisfying predicates \\(\\). Using notation: Length — number predicates antecedent. Coverage — \\(\\text{supp}()\\). Consequent support — \\(\\text{supp}(\\{c\\})\\). Support — \\(\\text{supp}(\\cup \\{c\\})\\). Confidence — \\(\\text{supp}(\\cup \\{c\\}) / \\text{supp}()\\). Optional additional measures (\"lift\", \"conviction\", \"added_value\") can computed using measures argument. searching rules, recommended create vector disjoints, specifies predicates must appear together condition. vector length number dataset columns. example, columns representing gear=3 gear=4 mutually exclusive, shared group label disj prevents meaningless conditions like gear=3 & gear=4. can conveniently generate vector var_names(): dig_associations() function searches association rules. main arguments : x: data matrix data frame (logical numeric); antecedent, consequent: tidyselect expressions selecting columns side rule; disjoint: vector defining mutually exclusive predicates; rule filtering thresholds min_support, min_confidence, min_coverage, limits like min_length, max_length; optional parameters measures, t_norm, contingency_table. following example, search fuzzy association rules dataset fuzzy_mtcars, : - column except starting \"\" may appear antecedent; - columns starting \"\" may appear consequent; - minimum support 0.02; - minimum confidence 0.8; - additional quality measures \"lift\" \"conviction\" computed. result tibble containing discovered rules quality metrics. can arrange , example, decreasing support: example illustrates typical workflow mining association rules nuggets. structure arguments apply analyzing either fuzzy Boolean datasets.","code":"disj <- var_names(colnames(fuzzy_mtcars)) print(disj) #>  [1] \"cyl\"  \"cyl\"  \"cyl\"  \"vs\"   \"vs\"   \"am\"   \"am\"   \"gear\" \"gear\" \"gear\" #> [11] \"mpg\"  \"mpg\"  \"mpg\"  \"disp\" \"disp\" \"disp\" \"hp\"   \"hp\"   \"hp\"   \"drat\" #> [21] \"drat\" \"drat\" \"wt\"   \"wt\"   \"wt\"   \"qsec\" \"qsec\" \"qsec\" \"carb\" \"carb\" #> [31] \"carb\" result <- dig_associations(fuzzy_mtcars,                            antecedent = !starts_with(\"am\"),                            consequent = starts_with(\"am\"),                            disjoint = disj,                            min_support = 0.02,                            min_confidence = 0.8,                            measures = c(\"lift\", \"conviction\"),                            contingency_table = TRUE) result <- arrange(result, desc(support)) print(result) #> # A tibble: 526 × 14 #>    antecedent                     consequent support confidence coverage #>    <chr>                          <chr>        <dbl>      <dbl>    <dbl> #>  1 {gear=3}                       {am=0}       0.469      1        0.469 #>  2 {gear=3,vs=0}                  {am=0}       0.375      1        0.375 #>  3 {cyl=eight,gear=3,vs=0}        {am=0}       0.375      1        0.375 #>  4 {cyl=eight,vs=0}               {am=0}       0.375      0.857    0.438 #>  5 {cyl=eight,gear=3}             {am=0}       0.375      1        0.375 #>  6 {cyl=eight}                    {am=0}       0.375      0.857    0.438 #>  7 {mpg=(-Inf;15;20)}             {am=0}       0.327      0.847    0.387 #>  8 {drat=(-Inf;2.76;3.84)}        {am=0}       0.311      0.948    0.328 #>  9 {gear=3,mpg=(-Inf;15;20)}      {am=0}       0.309      1        0.309 #> 10 {drat=(-Inf;2.76;3.84),gear=3} {am=0}       0.307      1        0.307 #>    conseq_support count antecedent_length    pp    pn    np    nn  lift #>             <dbl> <dbl>             <int> <dbl> <dbl> <dbl> <dbl> <dbl> #>  1          0.594 15                    1 15    0      4     13    1.68 #>  2          0.594 12                    2 12    0      7     13    1.68 #>  3          0.594 12                    3 12    0      7     13    1.68 #>  4          0.594 12                    2 12    2      7     11    1.44 #>  5          0.594 12                    2 12    0      7     13    1.68 #>  6          0.594 12                    1 12    2      7     11    1.44 #>  7          0.594 10.5                  1 10.5  1.90   8.52  11.1  1.43 #>  8          0.594  9.96                 1  9.96 0.546  9.04  12.5  1.60 #>  9          0.594  9.88                 2  9.88 0      9.12  13.0  1.68 #> 10          0.594  9.82                 2  9.82 0      9.18  13    1.68 #>    conviction #>         <dbl> #>  1     Inf    #>  2     Inf    #>  3     Inf    #>  4       2.84 #>  5     Inf    #>  6       2.84 #>  7       2.65 #>  8       7.82 #>  9     Inf    #> 10     Inf    #> # ℹ 516 more rows"},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"conditional-correlations","dir":"Articles","previous_headings":"Pre-defined Patterns","what":"Conditional Correlations","title":"nuggets: Get Started","text":"TBD (dig_correlations)","code":""},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"contrast-patterns","dir":"Articles","previous_headings":"Pre-defined Patterns","what":"Contrast Patterns","title":"nuggets: Get Started","text":"TBD (dig_contrasts)","code":""},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"post-processing-and-visualization","dir":"Articles","previous_headings":"","what":"Post-processing and Visualization","title":"nuggets: Get Started","text":"TBD","code":""},{"path":"https://beerda.github.io/nuggets/articles/nuggets.html","id":"custom-patterns","dir":"Articles","previous_headings":"","what":"Custom Patterns","title":"nuggets: Get Started","text":"nuggets package allows execute user-defined callback function generated frequent condition. way custom type patterns may searched. following example replicates search associations rules custom callback function. , dataset dichotomized disjoint vector created Data Preparation section : want search associations rules minimum support confidence, define variables hold thresholds. also need define callback function called found frequent condition. purpose generate rules obtained condition antecedent: callback function f() defines three arguments: condition, support foci_supports. names arguments random. Based argument names callback function, searching algorithm provides information function. condition vector indices representing conjunction predicates condition. predicate mean column source dataset. support argument gets relative frequency condition dataset. foci_supports vector supports special predicates, call “foci” (plural “focus”), within rows satisfying condition. associations rules, foci potential rule consequents. Now can run digging rules: return list lists callback function, flatten first level lists result binding data frame:","code":"#head(fuzzyCO2) #print(disj) min_support <- 0.02 min_confidence <- 0.8  f <- function(condition, support, foci_supports) {     conf <- foci_supports / support     sel <- !is.na(conf) & conf >= min_confidence & !is.na(foci_supports) & foci_supports >= min_support     conf <- conf[sel]     supp <- foci_supports[sel]          lapply(seq_along(conf), function(i) {        list(antecedent = format_condition(names(condition)),            consequent = format_condition(names(conf)[[i]]),            support = supp[[i]],            confidence = conf[[i]])     }) } #result <- dig(fuzzyCO2,               #f = f,               #condition = !starts_with(\"Treatment\"),               #focus = starts_with(\"Treatment\"),               #disjoint = disj,               #min_length = 1,               #min_support = min_support) #result <- result |>   #unlist(recursive = FALSE) |>   #lapply(as_tibble) |>   #do.call(rbind, args = _) |>   #arrange(desc(support)) # #print(result)"},{"path":"https://beerda.github.io/nuggets/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michal Burda. Author, maintainer.","code":""},{"path":"https://beerda.github.io/nuggets/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Burda M (2024). “nuggets: Data Pattern Extraction Framework R.” Torra, Vicenc, Narukawa, Yasuo, Kikuchi, Hiroaki (eds.), Modeling Decisions Artificial Intelligence, 115–126. ISBN 978-3-031-68208-7, doi:10.1007/978-3-031-68208-7_10.","code":"@InProceedings{,   title = {nuggets: Data Pattern Extraction Framework in R},   author = {Michal Burda},   editor = {{Torra} and {Vicenc} and {Narukawa} and {Yasuo} and {Kikuchi} and {Hiroaki}},   booktitle = {Modeling Decisions for Artificial Intelligence},   year = {2024},   publisher = {Springer Nature Switzerland},   address = {Cham},   pages = {115--126},   isbn = {978-3-031-68208-7},   doi = {10.1007/978-3-031-68208-7_10}, }"},{"path":[]},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"project-overview","dir":"","previous_headings":"","what":"Project Overview","title":"Copilot Instructions for nuggets","text":"nuggets R package providing framework systematic exploration association rules, contrast patterns, emerging patterns, subgroup discovery, conditional correlations. package supports crisp (Boolean) fuzzy data, performance-critical code implemented C++17.","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"repository-structure","dir":"","previous_headings":"","what":"Repository Structure","title":"Copilot Instructions for nuggets","text":"R/ - R source code roxygen2 documentation src/ - C++ source code using Rcpp, RcppThread, Boost headers (C++20 features C++17 compatibility) tests/testthat/ - Unit tests using testthat framework man/ - Generated documentation (auto-generated, edit manually) vignettes/ - Package vignettes inst/ - Installed files .github/workflows/ - CI/CD workflows R CMD check, test coverage, pkgdown","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"prerequisites","dir":"","previous_headings":"Development Setup","what":"Prerequisites","title":"Copilot Instructions for nuggets","text":"R >= 4.1.0 C++17 newer compatible compiler Required R packages: devtools, testthat, roxygen2","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"building-and-testing","dir":"","previous_headings":"Development Setup","what":"Building and Testing","title":"Copilot Instructions for nuggets","text":"","code":"# Install dependencies devtools::install_deps(dependencies = TRUE)  # Build documentation devtools::document()  # Run tests devtools::test()  # Run R CMD check devtools::check()  # Build package devtools::build()"},{"path":[]},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"r-code","dir":"","previous_headings":"Coding Standards","what":"R Code","title":"Copilot Instructions for nuggets","text":"@title description @param parameters @return return values @seealso references related functions @examples working examples @export exported functions Functions: snake_case (e.g., dig_associations(), is_condition()) Internal functions: prefix . (e.g., .extract_cols()) Variables: snake_case Internal functions related Shiny UI: camelCase, file name prefix ui- (e.g. ui-exploreApp.R function exploreApp()) Use cli package error messages, prefer functions defined R/testers.R error_context parameter, used, must always last argument Argument names initialized caller_arg() (e.g., arg_x = caller_arg(x)) call = caller_env() See .extract_cols() example Argument names string constants (e.g., arg_x = \"x\") call = current_env() See var_grid() example See ERROR_CONTEXT_HOWTO.md comprehensive documentation error handling patterns Use tidyverse style conventions Indent 4 spaces Use <- assignment Use pipe operator |> (base R) %>% (magrittr) consistently, prefer base R pipe operator","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"c-code","dir":"","previous_headings":"Coding Standards","what":"C++ Code","title":"Copilot Instructions for nuggets","text":"C++17 features used code (via // [[Rcpp::plugins(cpp17)]] src/common.h) DESCRIPTION specifies C++17 SystemRequirements maintain broader platform compatibility Use Rcpp R/C++ interface Use RcppThread parallel processing Use Boost headers (BH package) Headers src/*.h Implementation src/*.cpp Subdirectories logical modules (e.g., src/dig/, src/antichain/) Use Rcpp attributes exported functions: // [[Rcpp::export]] Add roxygen2 documentation exported functions Internal functions clear comments C++ unit tests use testthat framework Test files: src/test-*.cpp Tests run part R package tests Debug flags src/common.h Must disabled release (see RELEASE_HOWTO.md)","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"testing","dir":"","previous_headings":"Coding Standards","what":"Testing","title":"Copilot Instructions for nuggets","text":"Use testthat framework (edition 3) Test files tests/testthat/ test-*.R naming Group related tests test_that() blocks Aim high test coverage CI runs test coverage checks automatically Use devtools::test_coverage() locally Descriptive test names explaining tested Example: test_that(\"numeric matrix\", { ... })","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"r-package-dependencies","dir":"","previous_headings":"Dependencies","what":"R Package Dependencies","title":"Copilot Instructions for nuggets","text":"Runtime: classInt, cli, DT, fastmatch, generics, ggplot2, grid, htmltools, lifecycle, methods, purrr, Rcpp, rlang, shiny, shinyjs, shinyWidgets, stats, stringr, tibble, tidyr, tidyselect, utils Build/Link: BH, cli, Rcpp, RcppThread, testthat Suggests: arules, dplyr, testthat, xml2, withr, knitr, rmarkdown","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"adding-dependencies","dir":"","previous_headings":"Dependencies","what":"Adding Dependencies","title":"Copilot Instructions for nuggets","text":"Add appropriate field DESCRIPTION (Imports, Suggests, LinkingTo) Use usethis::use_package() helper functions Document dependency needed","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"release-process","dir":"","previous_headings":"","what":"Release Process","title":"Copilot Instructions for nuggets","text":"See RELEASE_HOWTO.md detailed release checklist including: 1. Update version date DESCRIPTION 2. Update NEWS.md 3. Disable debug src/common.h 4. Run spell check, rhub checks, Windows/Mac checks 5. Use devtools::release()","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"deprecation-process","dir":"","previous_headings":"","what":"Deprecation Process","title":"Copilot Instructions for nuggets","text":"package uses lifecycle function deprecation: 1. New deprecation: deprecate_soft() - soft warning 2. Next version: replace deprecate_warn() - warning 3. Final version: replace deprecate_stop() - error, remove function body 4. Remove completely next major version","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"cicd-workflows","dir":"","previous_headings":"","what":"CI/CD Workflows","title":"Copilot Instructions for nuggets","text":"R-CMD-check: Runs push/PR main/master/devel, tests multiple OS R versions test-coverage: Generates coverage reports via codecov pkgdown: Builds deploys documentation site rhub: Additional platform testing","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"interactive-features","dir":"","previous_headings":"","what":"Interactive Features","title":"Copilot Instructions for nuggets","text":"package includes Shiny applications interactive exploration: - explore() - Main interactive exploration function - Located R/explore-*.R files - Use shiny, shinyjs, shinyWidgets, DT UI components","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Copilot Instructions for nuggets","text":"Use roxygen2 documentation Run devtools::document() regenerate man/*.Rd files README.md generated README.Rmd - edit .Rmd file Vignettes vignettes/*.Rmd Package website built pkgdown","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"adding-a-new-function","dir":"","previous_headings":"Common Tasks","what":"Adding a New Function","title":"Copilot Instructions for nuggets","text":"Create function appropriate R/*.R file Put appropriate copyright header file, present source files Add roxygen2 documentation Export public function: @export Add tests tests/testthat/test-*.R Run devtools::document() update NAMESPACE man/ Run devtools::test() devtools::check()","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"adding-c-code","dir":"","previous_headings":"Common Tasks","what":"Adding C++ Code","title":"Copilot Instructions for nuggets","text":"Add source src/.cpp src/subdirectory/.cpp Update src/*.h headers needed Put appropriate copyright header file, present source files Use // [[Rcpp::export]] functions exposed R Run Rcpp::compileAttributes() update RcppExports.cpp/R Add tests src/test-*.cpp tests/testthat/ Test compilation devtools::load_all()","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"fixing-a-bug","dir":"","previous_headings":"Common Tasks","what":"Fixing a Bug","title":"Copilot Instructions for nuggets","text":"Add failing test reproduces bug Fix code Verify test passes Run full test suite: devtools::test() Run R CMD check: devtools::check()","code":""},{"path":"https://beerda.github.io/nuggets/copilot-instructions.html","id":"best-practices","dir":"","previous_headings":"","what":"Best Practices","title":"Copilot Instructions for nuggets","text":"Never edit generated files: NAMESPACE, man/*.Rd, RcppExports.cpp/R Always run tests: Use devtools::test() frequently Check package: Run devtools::check() committing significant changes Document code: Add roxygen2 comments immediately Follow error context patterns: See ERROR_CONTEXT_HOWTO.md Use existing patterns: Study similar functions codebase implementing new features Performance matters: Consider R C++ implementations performance-critical code Support crisp fuzzy: functions work logical numeric [0,1] data","code":""},{"path":"https://beerda.github.io/nuggets/index.html","id":"nuggets","dir":"","previous_headings":"","what":"Extensible Framework for Data Pattern Exploration","title":"Extensible Framework for Data Pattern Exploration","text":"nuggets package R statistical computing environment providing framework systematic exploration association rules (Agrawal (1994)), contrast patterns (Chen (2022)), emerging patterns (Dong (1999)), subgroup discovery (Atzmueller (2015)), conditional correlations (Hájek (1978)). User-defined functions may also supplied guide custom pattern searches. Supports crisp (Boolean) fuzzy data. Generates candidate conditions expressed elementary conjunctions, evaluates dataset, inspects induced sub-data statistical, logical, structural properties associations, correlations, contrasts. Includes methods visualization logical structures supports interactive exploration integrated Shiny applications.","code":""},{"path":"https://beerda.github.io/nuggets/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key Features","title":"Extensible Framework for Data Pattern Exploration","text":"Support categorical numeric data. Provides Boolean fuzzy logic approach. Data preparation functions easy pre-processing phase. Functions examining associations, conditional correlations, contrasts among data variables. Visualization pattern post-processing tools. Integrated Shiny applications interactive exploration discovered patterns.","code":""},{"path":"https://beerda.github.io/nuggets/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Extensible Framework for Data Pattern Exploration","text":"install stable version nuggets CRAN, type following command within R session: can also install development version nuggets GitHub : start using package, load R session :","code":"install.packages(\"nuggets\") install.packages(\"devtools\") devtools::install_github(\"beerda/nuggets\") library(nuggets)"},{"path":"https://beerda.github.io/nuggets/index.html","id":"minimal-example","dir":"","previous_headings":"","what":"Minimal Example","title":"Extensible Framework for Data Pattern Exploration","text":"following example demonstrates use nuggets find association rules built-mtcars dataset:","code":"# Preprocess: dichotomize and fuzzify numeric variables cars <- mtcars |>     partition(cyl, vs:gear, .method = \"dummy\") |>     partition(carb, .method = \"crisp\", .breaks = c(0, 3, 10)) |>     partition(mpg, disp:qsec, .method = \"triangle\", .breaks = 3)  # Search for associations among conditions rules <- dig_associations(cars,                           antecedent = everything(),                           consequent = everything(),                           max_length = 4,                           min_support = 0.1,                           measures = c(\"lift\", \"conviction\"))  # Explore the found rules interactively explore(rules, cars)"},{"path":"https://beerda.github.io/nuggets/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Extensible Framework for Data Pattern Exploration","text":"Contributions, suggestions, bug reports welcome. Please submit issues GitHub.","code":""},{"path":"https://beerda.github.io/nuggets/reference/association_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an association matrix from a nugget of flavour associations. — association_matrix","title":"Create an association matrix from a nugget of flavour associations. — association_matrix","text":"association matrix matrix rows correspond antecedents, columns correspond consequents, values taken specified column nugget. Missing values filled zeros.","code":""},{"path":"https://beerda.github.io/nuggets/reference/association_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an association matrix from a nugget of flavour associations. — association_matrix","text":"","code":"association_matrix(   x,   value,   error_context = list(arg_x = \"x\", arg_value = \"value\", call = current_env()) )"},{"path":"https://beerda.github.io/nuggets/reference/association_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an association matrix from a nugget of flavour associations. — association_matrix","text":"x nugget flavour associations. value tidyselect expression (see tidyselect syntax) specifying column use filling matrix values. error_context list details used error messages. must contain: - arg_x: name x argument; - arg_value: name value argument; - call: environment evaluate error messages. Defaults current environment.","code":""},{"path":"https://beerda.github.io/nuggets/reference/association_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an association matrix from a nugget of flavour associations. — association_matrix","text":"numeric matrix row names corresponding antecedents column names corresponding consequents. Values taken column specified value. Missing values filled zeros.","code":""},{"path":"https://beerda.github.io/nuggets/reference/association_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an association matrix from a nugget of flavour associations. — association_matrix","text":"pair antecedent consequent must unique nugget. multiple rows pair, error raised.","code":""},{"path":"https://beerda.github.io/nuggets/reference/association_matrix.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create an association matrix from a nugget of flavour associations. — association_matrix","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/bound_range.html","id":null,"dir":"Reference","previous_headings":"","what":"Bound a range of numeric values — bound_range","title":"Bound a range of numeric values — bound_range","text":"function computes range numeric values vector adjusts bounds \"nice\" rounded numbers. Specifically, rounds lower bound downwards (similar floor()) upper bound upwards (similar ceiling()) specified number digits. can useful preparing data ranges axis labels, plotting, reporting. function returns numeric vector length two, containing adjusted lower upper bounds.","code":""},{"path":"https://beerda.github.io/nuggets/reference/bound_range.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bound a range of numeric values — bound_range","text":"","code":"bound_range(x, digits = 0, na_rm = FALSE)"},{"path":"https://beerda.github.io/nuggets/reference/bound_range.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bound a range of numeric values — bound_range","text":"x numeric vector bounded. digits integer scalar specifying number digits round bounds . positive value determines number decimal places used. negative value rounds nearest 10, 100, etc. digits NULL, rounding performed exact range returned. na_rm logical flag indicating whether NA values removed computing range. TRUE, range computed non-NA values . FALSE x contains NA values, function returns c(NA, NA).","code":""},{"path":"https://beerda.github.io/nuggets/reference/bound_range.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bound a range of numeric values — bound_range","text":"numeric vector length two rounded lower upper bounds range x. lower bound always rounded , upper bound always rounded . x NULL length zero, function returns NULL.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/bound_range.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Bound a range of numeric values — bound_range","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/bound_range.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bound a range of numeric values — bound_range","text":"","code":"bound_range(c(1.9, 2, 3.1), digits = 0)      # returns c(1, 4) #> [1] 1 4 bound_range(c(190, 200, 301), digits = -2)   # returns c(100, 400) #> [1] 100 400"},{"path":"https://beerda.github.io/nuggets/reference/dig.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for patterns of a custom type — dig","title":"Search for patterns of a custom type — dig","text":"general function searching patterns custom type. function allows selection columns x used condition predicates. enumerates possible conditions form elementary conjunctions selected predicates, condition executes user-defined callback function f. callback expected perform analysis return object (often list) representing pattern patterns related condition. results calls returned list.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for patterns of a custom type — dig","text":"","code":"dig(   x,   f,   condition = everything(),   focus = NULL,   disjoint = var_names(colnames(x)),   excluded = NULL,   min_length = 0,   max_length = Inf,   min_support = 0,   min_focus_support = 0,   min_conditional_focus_support = 0,   max_support = 1,   filter_empty_foci = FALSE,   t_norm = \"goguen\",   max_results = Inf,   verbose = FALSE,   threads = 1L,   error_context = list(arg_x = \"x\", arg_f = \"f\", arg_condition = \"condition\", arg_focus =     \"focus\", arg_disjoint = \"disjoint\", arg_excluded = \"excluded\", arg_min_length =     \"min_length\", arg_max_length = \"max_length\", arg_min_support = \"min_support\",     arg_min_focus_support = \"min_focus_support\", arg_min_conditional_focus_support =     \"min_conditional_focus_support\", arg_max_support = \"max_support\",     arg_filter_empty_foci = \"filter_empty_foci\", arg_t_norm = \"t_norm\", arg_max_results =     \"max_results\", arg_verbose = \"verbose\",       arg_threads = \"threads\", call =     current_env()) )"},{"path":"https://beerda.github.io/nuggets/reference/dig.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for patterns of a custom type — dig","text":"x matrix data frame. matrix, must numeric (double) logical. data frame, columns must numeric (double) logical. f callback function executed generated condition. may declare subset arguments listed . algorithm detects arguments present provides values f. design allows user control amount information received computational cost, arguments expensive compute others. function f expected return object (typically list) representing pattern patterns related condition. results calls f collected returned list. Possible arguments : condition, sum, support, indices, weights, pp, pn, np, nn, foci_supports (deprecated), thoroughly described \"Details\" section. condition tidyselect expression (see tidyselect syntax) specifying columns x use condition predicates focus tidyselect expression (see tidyselect syntax) specifying columns x use focus predicates disjoint atomic vector (length = number columns x) defining groups predicates. Columns group appear together condition. data partition(), use var_names() column names construct disjoint. excluded NULL list character vectors, representing implication formula. vector, last element form antecedent last element consequent. formulae treated tautologies used filter generated conditions. condition contains antecedent consequent formula, passed callback function f. Likewise, condition contains antecedent, corresponding focus (consequent) passed f. min_length Minimum number predicates condition required trigger callback f. Must \\(\\ge 0\\). set 0, empty condition also triggers callback. max_length Maximum number predicates allowed condition. Conditions longer max_length generated. Inf, limit total number available predicates. Must \\(\\ge 0\\) \\(\\ge min_length\\). setting strongly influences number generated conditions speed search. min_support Minimum support condition required trigger f. Support relative frequency condition x. logical data, proportion rows condition predicates TRUE. numeric (double) data, support mean (rows) products predicate values. Must \\([0,1]\\). condition’s support falls min_support, recursive generation extensions stopped. Thus, min_support directly affects search speed number callback calls. min_focus_support Minimum support focus required passed f. logical data, proportion rows condition focus TRUE. numeric (double) data, support computed mean (rows) t-norm predicate values (t-norm selected t_norm). Must \\([0,1]\\). Foci support threshold excluded. Together filter_empty_foci, parameter influences search speed number triggered calls f. min_conditional_focus_support Minimum conditional support focus within condition. Defined relative frequency rows focus TRUE among condition TRUE. \\(sum\\) (see support Details) number rows (sum truth degrees fuzzy data) satisfying condition, \\(pp\\) (see pp[] Details) sum truth degrees condition focus hold, conditional support \\(pp / sum\\). Must \\([0,1]\\). Foci threshold passed f. Together filter_empty_foci, parameter influences search speed number callback calls. max_support Maximum support condition trigger f. Conditions support threshold skipped, recursive generation supersets continues. Must \\([0,1]\\). filter_empty_foci Logical; controls whether f triggered conditions remaining foci filtering min_focus_support min_conditional_focus_support. TRUE, f called least one focus remains. FALSE, f called regardless. t_norm T-norm used conjunction weights: \"goedel\" (minimum), \"goguen\" (product), \"lukas\" (Łukasiewicz). max_results Maximum number results (objects returned callback f) store return output list. limit reached, generation conditions stops. Use positive integer enable early stopping; set Inf remove cap. verbose Logical; TRUE, print progress messages. threads Number threads parallel computation. error_context list details used constructing error messages. mainly useful dig() called another function errors refer caller’s argument names rather dig(). list must contain: arg_x – name argument x character string arg_f – name argument f character string arg_condition – name argument condition arg_focus – name argument focus arg_disjoint – name argument disjoint arg_excluded – name argument excluded arg_min_length – name argument min_length arg_max_length – name argument max_length arg_min_support – name argument min_support arg_min_focus_support – name argument min_focus_support arg_min_conditional_focus_support – name argument min_conditional_focus_support arg_max_support – name argument max_support arg_filter_empty_foci – name argument filter_empty_foci arg_t_norm – name argument t_norm arg_threads – name argument threads call – environment evaluate error messages","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for patterns of a custom type — dig","text":"list results returned callback function f.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Search for patterns of a custom type — dig","text":"callback function f may accept number arguments (see f argument description). algorithm automatically provides condition-related information f based arguments present. addition conditions, function can evaluate focus predicates (foci). Foci specified separately tested within generated condition. Extra information passed f. Restrictions may imposed generated conditions, : minimum maximum condition length (min_length, max_length); minimum condition support (min_support); minimum focus support (min_focus_support), .e. support rows condition focus hold. Let \\(P\\) set condition predicates selected condition \\(E\\) set focus predicates selected focus. function generates possible conditions elementary conjunctions distinct predicates \\(P\\). conditions filtered using disjoint, excluded, min_length, max_length, min_support, max_support. remaining condition, foci \\(E\\) tested filtered using min_focus_support min_conditional_focus_support. least one focus remains (filter_empty_foci = FALSE), callback f executed details condition foci. Results calls collected returned list. Let \\(C\\) condition (\\(C \\subseteq P\\)), \\(F\\) set filtered foci (\\(F \\subseteq E\\)), \\(R\\) set rows x, \\(\\mu_C(r)\\) truth degree condition \\(C\\) row \\(r\\). parameters passed f defined : condition: named integer vector column indices representing predicates \\(C\\). Names correspond column names. sum: numeric scalar value number rows satisfying \\(C\\) logical data, sum truth degrees fuzzy data, \\(sum = \\sum_{r \\R} \\mu_C(r)\\). support: numeric scalar value relative frequency rows satisfying \\(C\\), \\(supp = sum / |R|\\). pp, pn, np, nn: numeric vector entries contingency table \\(C\\) \\(F\\), satisfying Ruspini condition \\(pp + pn + np + nn = |R|\\). \\(\\)-th elements vectors correspond \\(\\)-th focus \\(F_i\\) \\(F\\) defined : pp[]: rows satisfying \\(C\\) \\(F_i\\), \\(pp_i = \\sum_{r \\R} \\mu_{C \\land F_i}(r)\\). pn[]: rows satisfying \\(C\\) \\(F_i\\), \\(pn_i = \\sum_{r \\R} \\mu_C(r) - pp_i\\). np[]: rows satisfying \\(F_i\\) \\(C\\), \\(np_i = \\sum_{r \\R} \\mu_{F_i}(r) - pp_i\\). nn[]: rows satisfying neither \\(C\\) \\(F_i\\), \\(nn_i = |R| - (pp_i + pn_i + np_i)\\).","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/dig.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Search for patterns of a custom type — dig","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search for patterns of a custom type — dig","text":"","code":"library(tibble)  # Prepare iris data d <- partition(iris, .breaks = 2)  # Simple callback: return formatted condition names dig(x = d,     f = function(condition) format_condition(names(condition)),     min_support = 0.5) #> [[1]] #> [1] \"{}\" #>  #> [[2]] #> [1] \"{Sepal.Width=(-Inf;3.2]}\" #>  #> [[3]] #> [1] \"{Petal.Length=(3.95;Inf],Sepal.Width=(-Inf;3.2]}\" #>  #> [[4]] #> [1] \"{Sepal.Length=(-Inf;6.1]}\" #>  #> [[5]] #> [1] \"{Petal.Length=(3.95;Inf]}\" #>  #> [[6]] #> [1] \"{Petal.Width=(-Inf;1.3]}\" #>  #> attr(,\"class\") #> [1] \"nugget\" \"list\"   #> attr(,\"call_function\") #> [1] \"dig\" #> attr(,\"call_data\") #> attr(,\"call_data\")$nrow #> [1] 150 #>  #> attr(,\"call_data\")$ncol #> [1] 11 #>  #> attr(,\"call_data\")$colnames #>  [1] \"Sepal.Length=(-Inf;6.1]\"  \"Sepal.Length=(6.1;Inf]\"   #>  [3] \"Sepal.Width=(-Inf;3.2]\"   \"Sepal.Width=(3.2;Inf]\"    #>  [5] \"Petal.Length=(-Inf;3.95]\" \"Petal.Length=(3.95;Inf]\"  #>  [7] \"Petal.Width=(-Inf;1.3]\"   \"Petal.Width=(1.3;Inf]\"    #>  [9] \"Species=setosa\"           \"Species=versicolor\"       #> [11] \"Species=virginica\"        #>  #> attr(,\"call_args\") #> attr(,\"call_args\")$x #> [1] \"d\" #>  #> attr(,\"call_args\")$condition #>  [1] \"Sepal.Length=(-Inf;6.1]\"  \"Sepal.Length=(6.1;Inf]\"   #>  [3] \"Sepal.Width=(-Inf;3.2]\"   \"Sepal.Width=(3.2;Inf]\"    #>  [5] \"Petal.Length=(-Inf;3.95]\" \"Petal.Length=(3.95;Inf]\"  #>  [7] \"Petal.Width=(-Inf;1.3]\"   \"Petal.Width=(1.3;Inf]\"    #>  [9] \"Species=setosa\"           \"Species=versicolor\"       #> [11] \"Species=virginica\"        #>  #> attr(,\"call_args\")$focus #> character(0) #>  #> attr(,\"call_args\")$disjoint #>  [1] \"Sepal.Length\" \"Sepal.Length\" \"Sepal.Width\"  \"Sepal.Width\"  \"Petal.Length\" #>  [6] \"Petal.Length\" \"Petal.Width\"  \"Petal.Width\"  \"Species\"      \"Species\"      #> [11] \"Species\"      #>  #> attr(,\"call_args\")$excluded #> NULL #>  #> attr(,\"call_args\")$min_length #> [1] 0 #>  #> attr(,\"call_args\")$max_length #> [1] Inf #>  #> attr(,\"call_args\")$min_support #> [1] 0.5 #>  #> attr(,\"call_args\")$min_focus_support #> [1] 0 #>  #> attr(,\"call_args\")$min_conditional_focus_support #> [1] 0 #>  #> attr(,\"call_args\")$max_support #> [1] 1 #>  #> attr(,\"call_args\")$filter_empty_foci #> [1] FALSE #>  #> attr(,\"call_args\")$t_norm #> [1] \"goguen\" #>  #> attr(,\"call_args\")$max_results #> [1] -1 #>  #> attr(,\"call_args\")$verbose #> [1] FALSE #>  #> attr(,\"call_args\")$threads #> [1] 1 #>   # Callback returning condition and support res <- dig(x = d,            f = function(condition, support) {                list(condition = format_condition(names(condition)),                     support = support)            },            min_support = 0.5) do.call(rbind, lapply(res, as_tibble)) #> # A tibble: 6 × 2 #>   condition                                        support #>   <chr>                                              <dbl> #> 1 {}                                                 1     #> 2 {Sepal.Width=(-Inf;3.2]}                           0.713 #> 3 {Petal.Length=(3.95;Inf],Sepal.Width=(-Inf;3.2]}   0.527 #> 4 {Sepal.Length=(-Inf;6.1]}                          0.633 #> 5 {Petal.Length=(3.95;Inf]}                          0.593 #> 6 {Petal.Width=(-Inf;1.3]}                           0.520  # Within each condition, evaluate also supports of columns starting with # \"Species\" res <- dig(x = d,            f = function(condition, support, pp) {                c(list(condition = format_condition(names(condition))),                  list(condition_support = support),                  as.list(pp / nrow(d)))            },            condition = !starts_with(\"Species\"),            focus = starts_with(\"Species\"),            min_support = 0.5,            min_focus_support = 0) do.call(rbind, lapply(res, as_tibble)) #> # A tibble: 6 × 5 #>   condition              condition_support `Species=setosa` `Species=versicolor` #>   <chr>                              <dbl>            <dbl>                <dbl> #> 1 {}                                 1                0.333                0.333 #> 2 {Sepal.Width=(-Inf;3.…             0.713            0.113                0.32  #> 3 {Petal.Length=(3.95;I…             0.527            0                    0.247 #> 4 {Sepal.Length=(-Inf;6…             0.633            0.333                0.227 #> 5 {Petal.Length=(3.95;I…             0.593            0                    0.26  #> 6 {Petal.Width=(-Inf;1.…             0.520            0.333                0.187 #> # ℹ 1 more variable: `Species=virginica` <dbl>  # Multiple patterns per condition based on foci res <- dig(x = d,            f = function(condition, support, pp) {                lapply(seq_along(pp), function(i) {                    list(condition = format_condition(names(condition)),                         condition_support = support,                         focus = names(pp)[i],                         focus_support = pp[[i]] / nrow(d))                })            },            condition = !starts_with(\"Species\"),            focus = starts_with(\"Species\"),            min_support = 0.5,            min_focus_support = 0)  # Flatten result and convert to tibble res <- unlist(res, recursive = FALSE) do.call(rbind, lapply(res, as_tibble)) #> # A tibble: 18 × 4 #>    condition                               condition_support focus focus_support #>    <chr>                                               <dbl> <chr>         <dbl> #>  1 {}                                                  1     Spec…        0.333  #>  2 {}                                                  1     Spec…        0.333  #>  3 {}                                                  1     Spec…        0.333  #>  4 {Sepal.Width=(-Inf;3.2]}                            0.713 Spec…        0.113  #>  5 {Sepal.Width=(-Inf;3.2]}                            0.713 Spec…        0.32   #>  6 {Sepal.Width=(-Inf;3.2]}                            0.713 Spec…        0.28   #>  7 {Petal.Length=(3.95;Inf],Sepal.Width=(…             0.527 Spec…        0      #>  8 {Petal.Length=(3.95;Inf],Sepal.Width=(…             0.527 Spec…        0.247  #>  9 {Petal.Length=(3.95;Inf],Sepal.Width=(…             0.527 Spec…        0.28   #> 10 {Sepal.Length=(-Inf;6.1]}                           0.633 Spec…        0.333  #> 11 {Sepal.Length=(-Inf;6.1]}                           0.633 Spec…        0.227  #> 12 {Sepal.Length=(-Inf;6.1]}                           0.633 Spec…        0.0733 #> 13 {Petal.Length=(3.95;Inf]}                           0.593 Spec…        0      #> 14 {Petal.Length=(3.95;Inf]}                           0.593 Spec…        0.26   #> 15 {Petal.Length=(3.95;Inf]}                           0.593 Spec…        0.333  #> 16 {Petal.Width=(-Inf;1.3]}                            0.520 Spec…        0.333  #> 17 {Petal.Width=(-Inf;1.3]}                            0.520 Spec…        0.187  #> 18 {Petal.Width=(-Inf;1.3]}                            0.520 Spec…        0"},{"path":"https://beerda.github.io/nuggets/reference/dig_associations.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for association rules — dig_associations","title":"Search for association rules — dig_associations","text":"Association rules identify conditions (antecedents) specific feature (consequent) present often. Scheme: => C condition satisfied, feature C present often. Example: university_edu & middle_age & IT_industry => high_income People middle age university education working industry likely high income. Antecedent usually set predicates, consequent C single predicate. following explanations need mathematical function \\(supp()\\), defined set \\(\\) predicates relative frequency rows satisfying predicates \\(\\). logical data, \\(supp()\\) equals relative frequency rows, predicates \\(i_1, i_2, \\ldots, i_n\\) \\(\\) TRUE. numerical (double) input, \\(supp()\\) computed mean (rows) truth degrees formula i_1 i_2 ... i_n, triangular norm selected t_norm argument. Association rules characterized following quality measures. Length rule number elements antecedent. Coverage rule equal \\(supp()\\). Consequent support rule equal \\(supp(\\{c\\})\\). Support rule equal \\(supp(\\cup \\{c\\})\\). Confidence rule fraction \\(supp() / supp(\\cup \\{c\\})\\).","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_associations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for association rules — dig_associations","text":"","code":"dig_associations(   x,   antecedent = everything(),   consequent = everything(),   disjoint = var_names(colnames(x)),   excluded = NULL,   min_length = 0L,   max_length = Inf,   min_coverage = 0,   min_support = 0,   min_confidence = 0,   contingency_table = FALSE,   measures = NULL,   t_norm = \"goguen\",   max_results = Inf,   verbose = FALSE,   threads = 1,   error_context = list(arg_x = \"x\", arg_antecedent = \"antecedent\", arg_consequent =     \"consequent\", arg_disjoint = \"disjoint\", arg_excluded = \"excluded\", arg_min_length =     \"min_length\", arg_max_length = \"max_length\", arg_min_coverage = \"min_coverage\",     arg_min_support = \"min_support\", arg_min_confidence = \"min_confidence\",     arg_contingency_table = \"contingency_table\", arg_measures = \"measures\", arg_t_norm =     \"t_norm\", arg_max_results = \"max_results\", arg_verbose = \"verbose\", arg_threads =     \"threads\", call = current_env()) )"},{"path":"https://beerda.github.io/nuggets/reference/dig_associations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for association rules — dig_associations","text":"x matrix data frame data search . matrix must numeric (double) logical. x data frame column must either numeric (double) logical. antecedent tidyselect expression (see tidyselect syntax) specifying columns use antecedent (left) part rules consequent tidyselect expression (see tidyselect syntax) specifying columns use consequent (right) part rules disjoint atomic vector size equal number columns x specifies groups predicates: elements disjoint vector equal, corresponding columns x present together single condition. x prepared partition(), using var_names() function x's column names convenient way create disjoint vector. excluded NULL list character vectors, character vector contains names columns must appear together single antecedent. min_length minimum length, .e., minimum number predicates antecedent, rule generated. Value must greater equal 0. 0, rules empty antecedent generated first place. max_length maximum length, .e., maximum number predicates antecedent, rule generated. equal Inf, maximum length limited number available predicates. min_coverage minimum coverage rule dataset x. (See Description definition coverage.) min_support minimum support rule dataset x. (See Description definition support.) min_confidence minimum confidence rule dataset x. (See Description definition confidence.) contingency_table logical value indicating whether provide contingency table rule. TRUE, columns pp, pn, np, nn added output table. columns contain number rows satisfying antecedent consequent, antecedent consequent, consequent antecedent, neither antecedent consequent, respectively. measures character vector specifying additional quality measures compute. NULL, additional measures computed. Possible values \"lift\", \"conviction\", \"added_value\". See https://mhahsler.github.io/arules/docs/measures description measures. t_norm t-norm used compute conjunction weights. must one \"goedel\" (minimum t-norm), \"goguen\" (product t-norm), \"lukas\" (Łukasiewicz t-norm). max_results maximum number generated conditions execute callback function . number found conditions exceeds max_results, function stops generating new conditions returns results. avoid long computations search, recommended set max_results reasonable positive value. Setting max_results Inf generate possible conditions. verbose logical value indicating whether print progress messages. threads number threads use parallel computation. error_context named list providing context error messages. mainly useful dig_associations() called another function want error messages refer argument names calling function. list must contain following elements: arg_x - name argument x arg_antecedent - name argument antecedent arg_consequent - name argument consequent arg_disjoint - name argument disjoint arg_excluded - name argument excluded arg_min_length - name argument min_length arg_max_length - name argument max_length arg_min_coverage - name argument min_coverage arg_min_support - name argument min_support arg_min_confidence - name argument min_confidence arg_contingency_table - name argument contingency_table arg_measures - name argument measures arg_t_norm - name argument t_norm arg_max_results - name argument max_results arg_verbose - name argument verbose arg_threads - name argument threads","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_associations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for association rules — dig_associations","text":"S3 object, instance associations nugget classes, tibble found patterns computed quality measures.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/dig_associations.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Search for association rules — dig_associations","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_associations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search for association rules — dig_associations","text":"","code":"d <- partition(mtcars, .breaks = 2) #> Warning: n same as number of different finite values\\neach different finite value is a separate class #> Warning: n same as number of different finite values\\neach different finite value is a separate class dig_associations(d,                  antecedent = !starts_with(\"mpg\"),                  consequent = starts_with(\"mpg\"),                  min_support = 0.3,                  min_confidence = 0.8,                  measures = c(\"lift\", \"conviction\")) #> # A tibble: 524 × 10 #>    antecedent        consequent support confidence coverage conseq_support count #>    <chr>             <chr>        <dbl>      <dbl>    <dbl>          <dbl> <dbl> #>  1 {carb=(-Inf;4.5]… {mpg=(-In…   0.344      0.846    0.406          0.719    11 #>  2 {carb=(-Inf;4.5]… {mpg=(-In…   0.312      0.909    0.344          0.719    10 #>  3 {am=(-Inf;0.5],c… {mpg=(-In…   0.312      0.909    0.344          0.719    10 #>  4 {am=(-Inf;0.5],c… {mpg=(-In…   0.375      0.857    0.438          0.719    12 #>  5 {carb=(-Inf;4.5]… {mpg=(-In…   0.5        0.889    0.562          0.719    16 #>  6 {carb=(-Inf;4.5]… {mpg=(-In…   0.375      1        0.375          0.719    12 #>  7 {am=(-Inf;0.5],c… {mpg=(-In…   0.375      1        0.375          0.719    12 #>  8 {am=(-Inf;0.5],c… {mpg=(-In…   0.375      1        0.375          0.719    12 #>  9 {am=(-Inf;0.5],c… {mpg=(-In…   0.375      1        0.375          0.719    12 #> 10 {am=(-Inf;0.5],c… {mpg=(-In…   0.375      1        0.375          0.719    12 #> # ℹ 514 more rows #> # ℹ 3 more variables: antecedent_length <int>, lift <dbl>, conviction <dbl>"},{"path":"https://beerda.github.io/nuggets/reference/dig_baseline_contrasts.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for conditions that yield in statistically significant one-sample test in selected variables. — dig_baseline_contrasts","title":"Search for conditions that yield in statistically significant one-sample test in selected variables. — dig_baseline_contrasts","text":"Baseline contrast patterns identify conditions specific feature significantly different given value performing one-sample statistical test. Scheme: var != 0 | C Variable var (average) significantly different 0 condition C. Example: (measure_error != 0 | measure_tool_A  measuring measure tool , average measure error significantly different 0. baseline contrast computed using one-sample statistical test, specified method argument. function computes contrast variables specified vars argument. Baseline contrasts computed sub-data corresponding conditions generated condition columns. Function dig_baseline_contrasts() supports crisp conditions , .e., condition columns x must logical.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_baseline_contrasts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for conditions that yield in statistically significant one-sample test in selected variables. — dig_baseline_contrasts","text":"","code":"dig_baseline_contrasts(   x,   condition = where(is.logical),   vars = where(is.numeric),   disjoint = var_names(colnames(x)),   excluded = NULL,   min_length = 0L,   max_length = Inf,   min_support = 0,   max_support = 1,   method = \"t\",   alternative = \"two.sided\",   h0 = 0,   conf_level = 0.95,   max_p_value = 0.05,   wilcox_exact = FALSE,   wilcox_correct = TRUE,   wilcox_tol_root = 1e-04,   wilcox_digits_rank = Inf,   max_results = Inf,   verbose = FALSE,   threads = 1 )"},{"path":"https://beerda.github.io/nuggets/reference/dig_baseline_contrasts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for conditions that yield in statistically significant one-sample test in selected variables. — dig_baseline_contrasts","text":"x matrix data frame data search patterns . condition tidyselect expression (see tidyselect syntax) specifying columns use condition predicates vars tidyselect expression (see tidyselect syntax) specifying columns use computation contrasts disjoint atomic vector size equal number columns x specifies groups predicates: elements disjoint vector equal, corresponding columns x present together single condition. x prepared partition(), using var_names() function x's column names convenient way create disjoint vector. excluded NULL list character vectors, character vector contains names columns must appear together single condition. min_length minimum size (minimum number predicates) condition generated (must greater equal 0). 0, empty condition generated first place. max_length maximum size (maximum number predicates) condition generated. equal Inf, maximum length conditions limited number available predicates. min_support minimum support condition trigger callback function . support condition relative frequency condition dataset x. logical data, equals relative frequency rows condition predicates TRUE . numerical (double) input, support computed mean (rows) multiplications predicate values. max_support maximum support condition trigger callback function . See argument min_support details support condition. method character string indicating contrast compute. One \"t\", parametric, \"wilcox\", non-parametric test equality position. alternative indicates alternative hypothesis must one \"two.sided\", \"greater\" \"less\". \"greater\" corresponds positive association, \"less\" negative association. h0 numeric value specifying null hypothesis test. \"t\" method, value mean. \"wilcox\" method, value median. default value 0. conf_level numeric value specifying level confidence interval. default value 0.95. max_p_value maximum p-value test pattern considered significant. p-value test greater max_p_value, pattern included result. wilcox_exact (used \"wilcox\" method ) logical value indicating whether exact p-value computed. NULL, exact p-value computed sample sizes less 50. See wilcox.test() exact argument information. Contrary behavior wilcox.test(), default value FALSE. wilcox_correct (used \"wilcox\" method ) logical value indicating whether continuity correction applied normal approximation p-value, wilcox_exact FALSE. See wilcox.test() correct argument information. wilcox_tol_root (used \"wilcox\" method ) numeric value specifying tolerance root-finding algorithm used compute exact p-value. See wilcox.test() tol.root argument information. wilcox_digits_rank (used \"wilcox\" method ) numeric value specifying number digits round ranks . See wilcox.test() digits.rank argument information. max_results maximum number generated conditions execute callback function . number found conditions exceeds max_results, function stops generating new conditions returns results. avoid long computations search, recommended set max_results reasonable positive value. Setting max_results Inf generate possible conditions. verbose logical scalar indicating whether print progress messages. threads number threads use parallel computation.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_baseline_contrasts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for conditions that yield in statistically significant one-sample test in selected variables. — dig_baseline_contrasts","text":"S3 object instance baseline_contrasts nugget classes tibble found patterns rows. following columns always present: condition condition pattern character string form {p1 & p2 & ... & pn} p1, p2, ..., pn x's column names. support support condition, .e., relative frequency condition dataset x. var name contrast variable. estimate estimated mean median variable var. statistic statistic selected test. p_value p-value underlying test. n number rows sub-data corresponding condition. conf_int_lo lower bound confidence interval estimate. conf_int_hi upper bound confidence interval estimate. alternative character string indicating alternative hypothesis. value must one \"two.sided\", \"greater\", \"less\". method character string indicating method used test. comment character string additional information test (mainly error messages failure). \"t\" method, following additional columns also present (see also t.test()): df degrees freedom t test. stderr standard error mean.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/dig_baseline_contrasts.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Search for conditions that yield in statistically significant one-sample test in selected variables. — dig_baseline_contrasts","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_complement_contrasts.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for conditions that provide significant differences in selected variables to the rest of the data table — dig_complement_contrasts","title":"Search for conditions that provide significant differences in selected variables to the rest of the data table — dig_complement_contrasts","text":"Complement contrast patterns identify conditions significant difference numerical variable elements satisfy identified condition rest data table. Scheme: (var | C) != (var | C) statistically significant difference variable var group elements satisfy condition C group elements satisfy condition C. Example: (life_expectancy | smoker) < (life_expectancy | non-smoker) life expectancy people smoke cigarettes average significantly lower people smoke. complement contrast computed using two-sample statistical test, specified method argument. function computes complement contrast variables specified vars argument. Complement contrasts computed based sub-data corresponding conditions generated condition columns rest data table. Function #' dig_complement_contrasts() supports crisp conditions , .e., condition columns x must logical.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_complement_contrasts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for conditions that provide significant differences in selected variables to the rest of the data table — dig_complement_contrasts","text":"","code":"dig_complement_contrasts(   x,   condition = where(is.logical),   vars = where(is.numeric),   disjoint = var_names(colnames(x)),   excluded = NULL,   min_length = 0L,   max_length = Inf,   min_support = 0,   max_support = 1 - min_support,   method = \"t\",   alternative = \"two.sided\",   h0 = if (method == \"var\") 1 else 0,   conf_level = 0.95,   max_p_value = 0.05,   t_var_equal = FALSE,   wilcox_exact = FALSE,   wilcox_correct = TRUE,   wilcox_tol_root = 1e-04,   wilcox_digits_rank = Inf,   max_results = Inf,   verbose = FALSE,   threads = 1L )"},{"path":"https://beerda.github.io/nuggets/reference/dig_complement_contrasts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for conditions that provide significant differences in selected variables to the rest of the data table — dig_complement_contrasts","text":"x matrix data frame data search patterns . condition tidyselect expression (see tidyselect syntax) specifying columns use condition predicates vars tidyselect expression (see tidyselect syntax) specifying columns use computation contrasts disjoint atomic vector size equal number columns x specifies groups predicates: elements disjoint vector equal, corresponding columns x present together single condition. x prepared partition(), using var_names() function x's column names convenient way create disjoint vector. excluded NULL list character vectors, character vector contains names columns must appear together single condition. min_length minimum size (minimum number predicates) condition generated (must greater equal 0). 0, empty condition generated first place. max_length maximum size (maximum number predicates) condition generated. equal Inf, maximum length conditions limited number available predicates. min_support minimum support condition trigger callback function . support condition relative frequency condition dataset x. logical data, equals relative frequency rows condition predicates TRUE . numerical (double) input, support computed mean (rows) multiplications predicate values. max_support maximum support condition trigger callback function . See argument min_support details support condition. method character string indicating contrast compute. One \"t\", parametric, \"wilcox\", non-parametric test equality position, \"var\" F-test comparison variances two populations. alternative indicates alternative hypothesis must one \"two.sided\", \"greater\" \"less\". \"greater\" corresponds positive association, \"less\" negative association. h0 numeric value specifying null hypothesis test. \"t\" method, difference means. \"wilcox\" method, difference medians. \"var\" method, hypothesized ratio population variances. default value 1 \"var\" method, 0 otherwise. conf_level numeric value specifying level confidence interval. default value 0.95. max_p_value maximum p-value test pattern considered significant. p-value test greater max_p_value, pattern included result. t_var_equal (used \"t\" method ) logical value indicating whether variances two samples assumed equal. TRUE, pooled variance used estimate variance t-test. FALSE, Welch (Satterthwaite) approximation degrees freedom used. See t.test() var.equal argument information. wilcox_exact (used \"wilcox\" method ) logical value indicating whether exact p-value computed. NULL, exact p-value computed sample sizes less 50. See wilcox.test() exact argument information. Contrary behavior wilcox.test(), default value FALSE. wilcox_correct (used \"wilcox\" method ) logical value indicating whether continuity correction applied normal approximation p-value, wilcox_exact FALSE. See wilcox.test() correct argument information. wilcox_tol_root (used \"wilcox\" method ) numeric value specifying tolerance root-finding algorithm used compute exact p-value. See wilcox.test() tol.root argument information. wilcox_digits_rank (used \"wilcox\" method ) numeric value specifying number digits round ranks . See wilcox.test() digits.rank argument information. max_results maximum number generated conditions execute callback function . number found conditions exceeds max_results, function stops generating new conditions returns results. avoid long computations search, recommended set max_results reasonable positive value. Setting max_results Inf generate possible conditions. verbose logical scalar indicating whether print progress messages. threads number threads use parallel computation.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_complement_contrasts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for conditions that provide significant differences in selected variables to the rest of the data table — dig_complement_contrasts","text":"S3 object instance complement_contrasts nugget classes tibble found patterns rows. following columns always present: condition condition pattern character string form {p1 & p2 & ... & pn} p1, p2, ..., pn x's column names. support support condition, .e., relative frequency condition dataset x. var name contrast variable. estimate estimate value (see underlying test. statistic statistic selected test. p_value p-value underlying test. n_x number rows sub-data corresponding condition. n_y number rows sub-data corresponding negation condition. conf_int_lo lower bound confidence interval estimate. conf_int_hi upper bound confidence interval estimate. alternative character string indicating alternative hypothesis. value must one \"two.sided\", \"greater\", \"less\". method character string indicating method used test. comment character string additional information test (mainly error messages failure). \"t\" method, following additional columns also present (see also t.test()): df degrees freedom t test. stderr standard error mean difference.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/dig_complement_contrasts.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Search for conditions that provide significant differences in selected variables to the rest of the data table — dig_complement_contrasts","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_correlations.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for conditional correlations — dig_correlations","title":"Search for conditional correlations — dig_correlations","text":"Conditional correlations patterns identify strong relationships pairs numeric variables specific conditions. Scheme: xvar ~ yvar | Cxvar yvar highly correlates data satisfy condition C. Example: study_time ~ test_score | hard_exam hard exams, amount study time highly correlated obtained exam's test score. function computes correlations combinations xvars yvars columns x multiple sub-data corresponding conditions generated condition columns.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_correlations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for conditional correlations — dig_correlations","text":"","code":"dig_correlations(   x,   condition = where(is.logical),   xvars = where(is.numeric),   yvars = where(is.numeric),   disjoint = var_names(colnames(x)),   excluded = NULL,   method = \"pearson\",   alternative = \"two.sided\",   exact = NULL,   min_length = 0L,   max_length = Inf,   min_support = 0,   max_support = 1,   max_results = Inf,   verbose = FALSE,   threads = 1 )"},{"path":"https://beerda.github.io/nuggets/reference/dig_correlations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for conditional correlations — dig_correlations","text":"x matrix data frame data search . condition tidyselect expression (see tidyselect syntax) specifying columns use condition predicates xvars tidyselect expression (see tidyselect syntax) specifying columns use computation correlations yvars tidyselect expression (see tidyselect syntax) specifying columns use computation correlations disjoint atomic vector size equal number columns x specifies groups predicates: elements disjoint vector equal, corresponding columns x present together single condition. x prepared partition(), using var_names() function x's column names convenient way create disjoint vector. excluded NULL list character vectors, character vector contains names columns must appear together single condition. method character string indicating correlation coefficient used test. One \"pearson\", \"kendall\", \"spearman\" alternative indicates alternative hypothesis must one \"two.sided\", \"greater\" \"less\". \"greater\" corresponds positive association, \"less\" negative association. exact logical indicating whether exact p-value computed. Used Kendall's tau Spearman's rho. See stats::cor.test() information. min_length minimum size (minimum number predicates) condition generated (must greater equal 0). 0, empty condition generated first place. max_length maximum size (maximum number predicates) condition generated. equal Inf, maximum length conditions limited number available predicates. min_support minimum support condition trigger callback function . support condition relative frequency condition dataset x. logical data, equals relative frequency rows condition predicates TRUE . numerical (double) input, support computed mean (rows) multiplications predicate values. max_support maximum support condition trigger callback function . See argument min_support details support condition. max_results maximum number generated conditions execute callback function . number found conditions exceeds max_results, function stops generating new conditions returns results. avoid long computations search, recommended set max_results reasonable positive value. Setting max_results Inf generate possible conditions. verbose logical scalar indicating whether print progress messages. threads number threads use parallel computation.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_correlations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for conditional correlations — dig_correlations","text":"S3 object instance correlations nugget classes tibble found patterns.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/dig_correlations.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Search for conditional correlations — dig_correlations","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_correlations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search for conditional correlations — dig_correlations","text":"","code":"# convert iris$Species into dummy logical variables d <- partition(iris, Species)  # find conditional correlations between all pairs of numeric variables dig_correlations(d,                  condition = where(is.logical),                  xvars = Sepal.Length:Petal.Width,                  yvars = Sepal.Length:Petal.Width) #> # A tibble: 24 × 10 #>    condition      support xvar  yvar  estimate  p_value method alternative  rows #>    <chr>            <dbl> <chr> <chr>    <dbl>    <dbl> <chr>  <chr>       <int> #>  1 {}               1     Sepa… Sepa…   -0.118 1.52e- 1 Pears… two.sided     150 #>  2 {}               1     Sepa… Peta…    0.872 1.04e-47 Pears… two.sided     150 #>  3 {}               1     Sepa… Peta…    0.818 2.33e-37 Pears… two.sided     150 #>  4 {}               1     Sepa… Peta…   -0.428 4.51e- 8 Pears… two.sided     150 #>  5 {}               1     Sepa… Peta…   -0.366 4.07e- 6 Pears… two.sided     150 #>  6 {}               1     Peta… Peta…    0.963 4.68e-86 Pears… two.sided     150 #>  7 {Species=seto…   0.333 Sepa… Sepa…    0.743 6.71e-10 Pears… two.sided      50 #>  8 {Species=seto…   0.333 Sepa… Peta…    0.267 6.07e- 2 Pears… two.sided      50 #>  9 {Species=seto…   0.333 Sepa… Peta…    0.278 5.05e- 2 Pears… two.sided      50 #> 10 {Species=seto…   0.333 Sepa… Peta…    0.178 2.17e- 1 Pears… two.sided      50 #> # ℹ 14 more rows #> # ℹ 1 more variable: condition_length <int>  # With `condition = NULL`, dig_correlations() computes correlations between # all pairs of numeric variables on the whole dataset only, which is an # alternative way of computing the correlation matrix dig_correlations(iris,                  condition = NULL,                  xvars = Sepal.Length:Petal.Width,                  yvars = Sepal.Length:Petal.Width) #> # A tibble: 6 × 10 #>   condition support xvar        yvar  estimate  p_value method alternative  rows #>   <chr>       <dbl> <chr>       <chr>    <dbl>    <dbl> <chr>  <chr>       <int> #> 1 {}              1 Sepal.Leng… Sepa…   -0.118 1.52e- 1 Pears… two.sided     150 #> 2 {}              1 Sepal.Leng… Peta…    0.872 1.04e-47 Pears… two.sided     150 #> 3 {}              1 Sepal.Leng… Peta…    0.818 2.33e-37 Pears… two.sided     150 #> 4 {}              1 Sepal.Width Peta…   -0.428 4.51e- 8 Pears… two.sided     150 #> 5 {}              1 Sepal.Width Peta…   -0.366 4.07e- 6 Pears… two.sided     150 #> 6 {}              1 Petal.Leng… Peta…    0.963 4.68e-86 Pears… two.sided     150 #> # ℹ 1 more variable: condition_length <int>"},{"path":"https://beerda.github.io/nuggets/reference/dig_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for grid-based rules — dig_grid","title":"Search for grid-based rules — dig_grid","text":"function creates grid column names specified xvars yvars (see var_grid()). , enumerates conditions created data x (calling dig()) condition row grid combinations, user-defined function f executed sub-data created x selecting rows x satisfy generated condition selecting columns grid's row. Function useful searching patterns based relationships pairs columns, dig_correlations().","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for grid-based rules — dig_grid","text":"","code":"dig_grid(   x,   f,   condition = where(is.logical),   xvars = where(is.numeric),   yvars = where(is.numeric),   disjoint = var_names(colnames(x)),   excluded = NULL,   allow = \"all\",   na_rm = FALSE,   type = \"crisp\",   min_length = 0L,   max_length = Inf,   min_support = 0,   max_support = 1,   max_results = Inf,   verbose = FALSE,   threads = 1L,   error_context = list(arg_x = \"x\", arg_f = \"f\", arg_condition = \"condition\", arg_xvars =     \"xvars\", arg_yvars = \"yvars\", arg_disjoint = \"disjoint\", arg_excluded = \"excluded\",     arg_allow = \"allow\", arg_na_rm = \"na_rm\", arg_type = \"type\", arg_min_length =     \"min_length\", arg_max_length = \"max_length\", arg_min_support = \"min_support\",     arg_max_support = \"max_support\", arg_max_results = \"max_results\", arg_verbose =     \"verbose\", arg_threads = \"threads\", call = current_env()) )"},{"path":"https://beerda.github.io/nuggets/reference/dig_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for grid-based rules — dig_grid","text":"x matrix data frame data search . f callback function executed generated condition. arguments callback function differ based value type argument (see ): type = \"crisp\" (, boolean), callback function f must accept single argument pd type data.frame single (yvars == NULL) two (yvars != NULL) columns, accessible pd[[1]] pd[[2]]. Data frame pd subset original data frame x rows satisfy generated condition. Optionally, callback function may accept argument nd subset original data frame x rows satisfy generated condition. type = \"fuzzy\", callback function f must accept argument d type data.frame single (yvars == NULL) two (yvars != NULL) columns, accessible d[[1]] d[[2]], numeric argument weights length number rows d. weights argument contains truth degree generated condition row d. truth degree number interval \\([0, 1]\\) represents degree satisfaction condition original data row. cases, function must return list scalar values, converted single row result final tibble. condition tidyselect expression (see tidyselect syntax) specifying columns use condition predicates. selected columns must logical numeric. numeric, fuzzy conditions considered. xvars tidyselect expression (see tidyselect syntax) specifying columns x, whose names used domain combinations use first place (xvar) yvars NULL tidyselect expression (see tidyselect syntax) specifying columns x, whose names used domain combinations use second place (yvar) disjoint atomic vector size equal number columns x specifies groups predicates: elements disjoint vector equal, corresponding columns x NEITHER present together single condition single combination xvars yvars. x prepared partition(), using var_names() function x's column names convenient way create disjoint vector. excluded NULL list character vectors, character vector contains names columns must appear together single condition. allow character string specifying columns allowed selected xvars yvars arguments. Possible values : \"\" - columns allowed selected \"numeric\" - numeric columns allowed selected na_rm logical value indicating whether remove rows missing values sub-data callback function f called type character string specifying type conditions processed. \"crisp\" type accepts logical columns condition predicates. \"fuzzy\" type accepts logical numeric columns condition predicates numeric data interval \\([0, 1]\\). callback function f differs based value type argument (see description f ). min_length minimum size (minimum number predicates) condition generated (must greater equal 0). 0, empty condition generated first place. max_length maximum size (maximum number predicates) condition generated. equal Inf, maximum length conditions limited number available predicates. min_support minimum support condition trigger callback function . support condition relative frequency condition dataset x. logical data, equals relative frequency rows condition predicates TRUE . numerical (double) input, support computed mean (rows) multiplications predicate values. max_support maximum support condition trigger callback function . See argument min_support details support condition. max_results maximum number generated conditions execute callback function . number found conditions exceeds max_results, function stops generating new conditions returns results. avoid long computations search, recommended set max_results reasonable positive value. Setting max_results Inf generate possible conditions. verbose logical scalar indicating whether print progress messages. threads number threads use parallel computation. error_context list details used error messages. argument useful dig_grid() called another function provide error messages, refer arguments calling function. list must contain following elements: arg_x - name argument x character string arg_condition - name argument condition character string arg_xvars - name argument xvars character string arg_yvars - name argument yvars character string call - environment evaluate error messages.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for grid-based rules — dig_grid","text":"S3 object, instance nugget class, tibble found patterns. row represents single call callback function f.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/dig_grid.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Search for grid-based rules — dig_grid","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search for grid-based rules — dig_grid","text":"","code":"# *** Example of crisp (boolean) patterns: # dichotomize iris$Species crispIris <- partition(iris, Species)  # a simple callback function that computes mean difference of `xvar` and `yvar` f <- function(pd) {     list(m = mean(pd[[1]] - pd[[2]]),          n = nrow(pd))     }  # call f() for each condition created from column `Species` dig_grid(crispIris,          f,          condition = starts_with(\"Species\"),          xvars = starts_with(\"Sepal\"),          yvars = starts_with(\"Petal\"),          type = \"crisp\") #> # A tibble: 16 × 7 #>    condition            support xvar         yvar       m     n condition_length #>    <chr>                  <dbl> <chr>        <chr>  <dbl> <int>            <int> #>  1 {}                     1     Sepal.Length Peta…  2.09    150                0 #>  2 {}                     1     Sepal.Length Peta…  4.64    150                0 #>  3 {}                     1     Sepal.Width  Peta… -0.701   150                0 #>  4 {}                     1     Sepal.Width  Peta…  1.86    150                0 #>  5 {Species=setosa}       0.333 Sepal.Length Peta…  3.54     50                1 #>  6 {Species=setosa}       0.333 Sepal.Length Peta…  4.76     50                1 #>  7 {Species=setosa}       0.333 Sepal.Width  Peta…  1.97     50                1 #>  8 {Species=setosa}       0.333 Sepal.Width  Peta…  3.18     50                1 #>  9 {Species=versicolor}   0.333 Sepal.Length Peta…  1.68     50                1 #> 10 {Species=versicolor}   0.333 Sepal.Length Peta…  4.61     50                1 #> 11 {Species=versicolor}   0.333 Sepal.Width  Peta… -1.49     50                1 #> 12 {Species=versicolor}   0.333 Sepal.Width  Peta…  1.44     50                1 #> 13 {Species=virginica}    0.333 Sepal.Length Peta…  1.04     50                1 #> 14 {Species=virginica}    0.333 Sepal.Length Peta…  4.56     50                1 #> 15 {Species=virginica}    0.333 Sepal.Width  Peta… -2.58     50                1 #> 16 {Species=virginica}    0.333 Sepal.Width  Peta…  0.948    50                1  # *** Example of fuzzy patterns: # create fuzzy sets from Sepal columns fuzzyIris <- partition(iris,                        starts_with(\"Sepal\"),                        .method = \"triangle\",                        .breaks = 3)  # a simple callback function that computes a weighted mean of a difference of # `xvar` and `yvar` f <- function(d, weights) {     list(m = weighted.mean(d[[1]] - d[[2]], w = weights),          w = sum(weights)) }  # call f() for each fuzzy condition created from column fuzzy sets whose # names start with \"Sepal\" dig_grid(fuzzyIris,          f,          condition = starts_with(\"Sepal\"),          xvars = Petal.Length,          yvars = Petal.Width,          type = \"fuzzy\") #> # A tibble: 16 × 7 #>    condition                   support xvar  yvar      m      w condition_length #>    <chr>                         <dbl> <chr> <chr> <dbl>  <dbl>            <int> #>  1 {}                          1       Peta… Peta…  2.56 150                   0 #>  2 {Sepal.Width=(2;3.2;4.4)}   0.694   Peta… Peta…  2.56 104.                  1 #>  3 {Sepal.Length=(4.3;6.1;7.9… 0.408   Peta… Peta…  2.78  61.2                 2 #>  4 {Sepal.Length=(-Inf;4.3;6.… 0.188   Peta… Peta…  1.50  28.2                 2 #>  5 {Sepal.Length=(6.1;7.9;Inf… 0.0988  Peta… Peta…  3.70  14.8                 2 #>  6 {Sepal.Length=(4.3;6.1;7.9… 0.600   Peta… Peta…  2.74  89.9                 1 #>  7 {Sepal.Length=(4.3;6.1;7.9… 0.143   Peta… Peta…  3.06  21.5                 2 #>  8 {Sepal.Length=(4.3;6.1;7.9… 0.0486  Peta… Peta…  1.47   7.30                2 #>  9 {Sepal.Length=(-Inf;4.3;6.… 0.271   Peta… Peta…  1.59  40.7                 1 #> 10 {Sepal.Length=(-Inf;4.3;6.… 0.0472  Peta… Peta…  2.23   7.08                2 #> 11 {Sepal.Length=(-Inf;4.3;6.… 0.0364  Peta… Peta…  1.22   5.45                2 #> 12 {Sepal.Width=(-Inf;2;3.2)}  0.212   Peta… Peta…  2.96  31.8                 1 #> 13 {Sepal.Length=(6.1;7.9;Inf… 0.0218  Peta… Peta…  3.87   3.26                2 #> 14 {Sepal.Length=(6.1;7.9;Inf… 0.129   Peta… Peta…  3.76  19.3                 1 #> 15 {Sepal.Length=(6.1;7.9;Inf… 0.00833 Peta… Peta…  4.22   1.25                2 #> 16 {Sepal.Width=(3.2;4.4;Inf)} 0.0933  Peta… Peta…  1.62  14.0                 1"},{"path":"https://beerda.github.io/nuggets/reference/dig_paired_baseline_contrasts.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for conditions that provide significant differences between paired variables — dig_paired_baseline_contrasts","title":"Search for conditions that provide significant differences between paired variables — dig_paired_baseline_contrasts","text":"Paired baseline contrast patterns identify conditions significant difference statistical feature two paired numeric variables. Scheme: (xvar - yvar) != 0 | C statistically significant difference paired variables xvar yvar condition C. Example: (daily_ice_cream_income - daily_tea_income) > 0 | sunny condition sunny weather, paired test shows daily ice-cream income significantly higher daily tea income. paired baseline contrast  computed using paired version statistical test, specified method argument. function computes paired contrast pairs variables, first variable specified xvars argument second variable specified yvars argument. Paired baseline contrasts computed sub-data corresponding conditions generated condition columns. Function dig_paired_baseline_contrasts() supports crisp conditions , .e., condition columns x must logical.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_paired_baseline_contrasts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for conditions that provide significant differences between paired variables — dig_paired_baseline_contrasts","text":"","code":"dig_paired_baseline_contrasts(   x,   condition = where(is.logical),   xvars = where(is.numeric),   yvars = where(is.numeric),   disjoint = var_names(colnames(x)),   excluded = NULL,   min_length = 0L,   max_length = Inf,   min_support = 0,   max_support = 1,   method = \"t\",   alternative = \"two.sided\",   h0 = 0,   conf_level = 0.95,   max_p_value = 1,   t_var_equal = FALSE,   wilcox_exact = FALSE,   wilcox_correct = TRUE,   wilcox_tol_root = 1e-04,   wilcox_digits_rank = Inf,   max_results = Inf,   verbose = FALSE,   threads = 1 )"},{"path":"https://beerda.github.io/nuggets/reference/dig_paired_baseline_contrasts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for conditions that provide significant differences between paired variables — dig_paired_baseline_contrasts","text":"x matrix data frame data search patterns . condition tidyselect expression (see tidyselect syntax) specifying columns use condition predicates xvars tidyselect expression (see tidyselect syntax) specifying columns use computation contrasts yvars tidyselect expression (see tidyselect syntax) specifying columns use computation contrasts disjoint atomic vector size equal number columns x specifies groups predicates: elements disjoint vector equal, corresponding columns x present together single condition. x prepared partition(), using var_names() function x's column names convenient way create disjoint vector. excluded NULL list character vectors, character vector contains names columns must appear together single condition. min_length minimum size (minimum number predicates) condition generated (must greater equal 0). 0, empty condition generated first place. max_length maximum size (maximum number predicates) condition generated. equal Inf, maximum length conditions limited number available predicates. min_support minimum support condition trigger callback function . support condition relative frequency condition dataset x. logical data, equals relative frequency rows condition predicates TRUE . numerical (double) input, support computed mean (rows) multiplications predicate values. max_support maximum support condition trigger callback function . See argument min_support details support condition. method character string indicating contrast compute. One \"t\", parametric, \"wilcox\", non-parametric test equality position. alternative indicates alternative hypothesis must one \"two.sided\", \"greater\" \"less\". \"greater\" corresponds positive association, \"less\" negative association. h0 numeric value specifying null hypothesis test. \"t\" method, difference means. \"wilcox\" method, difference medians. default value 0. conf_level numeric value specifying level confidence interval. default value 0.95. max_p_value maximum p-value test pattern considered significant. p-value test greater max_p_value, pattern included result. t_var_equal (used \"t\" method ) logical value indicating whether variances two samples assumed equal. TRUE, pooled variance used estimate variance t-test. FALSE, Welch (Satterthwaite) approximation degrees freedom used. See t.test() var.equal argument information. wilcox_exact (used \"wilcox\" method ) logical value indicating whether exact p-value computed. NULL, exact p-value computed sample sizes less 50. See wilcox.test() exact argument information. Contrary behavior wilcox.test(), default value FALSE. wilcox_correct (used \"wilcox\" method ) logical value indicating whether continuity correction applied normal approximation p-value, wilcox_exact FALSE. See wilcox.test() correct argument information. wilcox_tol_root (used \"wilcox\" method ) numeric value specifying tolerance root-finding algorithm used compute exact p-value. See wilcox.test() tol.root argument information. wilcox_digits_rank (used \"wilcox\" method ) numeric value specifying number digits round ranks . See wilcox.test() digits.rank argument information. max_results maximum number generated conditions execute callback function . number found conditions exceeds max_results, function stops generating new conditions returns results. avoid long computations search, recommended set max_results reasonable positive value. Setting max_results Inf generate possible conditions. verbose logical scalar indicating whether print progress messages. threads number threads use parallel computation.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_paired_baseline_contrasts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for conditions that provide significant differences between paired variables — dig_paired_baseline_contrasts","text":"S3 object instance paired_baseline_contrasts nugget classes tibble found patterns rows. following columns always present: condition condition pattern character string form {p1 & p2 & ... & pn} p1, p2, ..., pn x's column names. support support condition, .e., relative frequency condition dataset x. xvar name first variable contrast. yvar name second variable contrast. estimate estimated difference variable var. statistic statistic selected test. p_value p-value underlying test. n number rows sub-data corresponding condition. conf_int_lo lower bound confidence interval estimate. conf_int_hi upper bound confidence interval estimate. alternative character string indicating alternative hypothesis. value must one \"two.sided\", \"greater\", \"less\". method character string indicating method used test. comment character string additional information test (mainly error messages failure). \"t\" method, following additional columns also present (see also t.test()): df degrees freedom t test. stderr standard error mean difference.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/dig_paired_baseline_contrasts.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Search for conditions that provide significant differences between paired variables — dig_paired_baseline_contrasts","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_paired_baseline_contrasts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search for conditions that provide significant differences between paired variables — dig_paired_baseline_contrasts","text":"","code":"# Compute ratio of sepal and petal length and width for iris dataset crispIris <- iris crispIris$Sepal.Ratio <- iris$Sepal.Length / iris$Sepal.Width crispIris$Petal.Ratio <- iris$Petal.Length / iris$Petal.Width  # Create predicates from the Species column crispIris <- partition(crispIris, Species)  # Compute paired contrasts for ratios of sepal and petal length and width dig_paired_baseline_contrasts(crispIris,                               condition = where(is.logical),                               xvars = Sepal.Ratio,                               yvars = Petal.Ratio,                               method = \"t\",                               min_support = 0.1) #> # A tibble: 4 × 16 #>   condition  support xvar  yvar  estimate statistic    df  p_value     n conf_lo #>   <chr>        <dbl> <chr> <chr>    <dbl>     <dbl> <dbl>    <dbl> <int>   <dbl> #> 1 {}           1     Sepa… Peta…   -2.36      -10.5   149 1.31e-19   150  -2.80  #> 2 {Species=…   0.333 Sepa… Peta…   -5.44      -13.5    49 4.41e-18    50  -6.25  #> 3 {Species=…   0.333 Sepa… Peta…   -1.08      -25.6    49 5.13e-30    50  -1.17  #> 4 {Species=…   0.333 Sepa… Peta…   -0.550     -11.1    49 4.85e-15    50  -0.649 #> # ℹ 6 more variables: conf_hi <dbl>, stderr <dbl>, alternative <chr>, #> #   method <chr>, comment <chr>, condition_length <int>"},{"path":"https://beerda.github.io/nuggets/reference/dig_tautologies.html","id":null,"dir":"Reference","previous_headings":"","what":"Find tautologies or ","title":"Find tautologies or ","text":"function finds tautologies dataset, .e., rules form {a1 & a2 & ... & } => {c} a1, a2, ..., antecedents c consequent. intent searching tautologies find rules always true, may used filtering generated conditions. resulting rules may used basis list excluded formulae (see excluded argument dig()).","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_tautologies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find tautologies or ","text":"","code":"dig_tautologies(   x,   antecedent = everything(),   consequent = everything(),   disjoint = var_names(colnames(x)),   max_length = Inf,   min_coverage = 0,   min_support = 0,   min_confidence = 0,   measures = NULL,   t_norm = \"goguen\",   max_results = Inf,   verbose = FALSE,   threads = 1 )"},{"path":"https://beerda.github.io/nuggets/reference/dig_tautologies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find tautologies or ","text":"x matrix data frame data search . matrix must numeric (double) logical. x data frame column must either numeric (double) logical. antecedent tidyselect expression (see tidyselect syntax) specifying columns use antecedent (left) part rules consequent tidyselect expression (see tidyselect syntax) specifying columns use consequent (right) part rules disjoint atomic vector size equal number columns x specifies groups predicates: elements disjoint vector equal, corresponding columns x present together single condition. x prepared partition(), using var_names() function x's column names convenient way create disjoint vector. max_length maximum length, .e., maximum number predicates antecedent, rule generated. equal Inf, maximum length limited number available predicates. min_coverage minimum coverage rule dataset x. (See Description definition coverage.) min_support minimum support rule dataset x. (See Description definition support.) min_confidence minimum confidence rule dataset x. (See Description definition confidence.) measures character vector specifying additional quality measures compute. NULL, additional measures computed. Possible values \"lift\", \"conviction\", \"added_value\". See https://mhahsler.github.io/arules/docs/measures description measures. t_norm t-norm used compute conjunction weights. must one \"goedel\" (minimum t-norm), \"goguen\" (product t-norm), \"lukas\" (Łukasiewicz t-norm). max_results maximum number generated conditions execute callback function . number found conditions exceeds max_results, function stops generating new conditions returns results. avoid long computations search, recommended set max_results reasonable positive value. Setting max_results Inf generate possible conditions. verbose logical value indicating whether print progress messages. threads number threads use parallel computation.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_tautologies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find tautologies or ","text":"S3 object instance associations nugget classes tibble found tautologies format equal output dig_associations().","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_tautologies.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find tautologies or ","text":"search tautologies performed iteratively searching rules increasing length antecedent. Rules found previous iterations used excluded argument next iteration.","code":""},{"path":"https://beerda.github.io/nuggets/reference/dig_tautologies.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Find tautologies or ","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/explore.associations.html","id":null,"dir":"Reference","previous_headings":"","what":"Show interactive application to explore association rules — explore.associations","title":"Show interactive application to explore association rules — explore.associations","text":"Launches interactive Shiny application visual exploration mined association rules. explorer provides tools inspecting rule quality, comparing interestingness measures, interactively filtering subsets rules. original dataset supplied, application also allows contextual exploration rules respect underlying data.","code":""},{"path":"https://beerda.github.io/nuggets/reference/explore.associations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show interactive application to explore association rules — explore.associations","text":"","code":"# S3 method for class 'associations' explore(x, data = NULL, ...)"},{"path":"https://beerda.github.io/nuggets/reference/explore.associations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show interactive application to explore association rules — explore.associations","text":"x object S3 class associations, typically created dig_associations(). data optional data frame containing dataset rules mined. Providing enables additional contextual features explorer, examining supporting records. ... Currently ignored.","code":""},{"path":"https://beerda.github.io/nuggets/reference/explore.associations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show interactive application to explore association rules — explore.associations","text":"object class shiny.appobj representing Shiny application. \"printed\" interactive R session, application launched immediately default web browser.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/explore.associations.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Show interactive application to explore association rules — explore.associations","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/explore.associations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show interactive application to explore association rules — explore.associations","text":"","code":"if (FALSE) { # \\dontrun{ data(\"iris\") # convert all columns into dummy logical variables part <- partition(iris, .breaks = 3)  # find association rules rules <- dig_associations(part)  # launch the interactive explorer explore(rules, data = part) } # }"},{"path":"https://beerda.github.io/nuggets/reference/fire.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain truth-degrees of conditions — fire","title":"Obtain truth-degrees of conditions — fire","text":"Given data frame matrix truth values predicates, compute truth values set conditions expressed elementary conjunctions.","code":""},{"path":"https://beerda.github.io/nuggets/reference/fire.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain truth-degrees of conditions — fire","text":"","code":"fire(x, condition, t_norm = \"goguen\")"},{"path":"https://beerda.github.io/nuggets/reference/fire.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain truth-degrees of conditions — fire","text":"x matrix data frame containing predicate truth values. x matrix, must numeric (double) logical. x data frame, columns must numeric (double) logical. condition character vector conditions, formatted according format_condition(). example, \"{p1,p2,p3}\" represents condition composed three predicates \"p1\", \"p2\", \"p3\". Every predicate mentioned condition must present column x. t_norm string specifying triangular norm (t-norm) used compute conjunctions predicate values. Must one \"goedel\" (minimum t-norm), \"goguen\" (product t-norm), \"lukas\" (Łukasiewicz t-norm).","code":""},{"path":"https://beerda.github.io/nuggets/reference/fire.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain truth-degrees of conditions — fire","text":"numeric matrix entries interval \\([0, 1]\\) giving truth degrees conditions. matrix nrow(x) rows length(condition) columns. element row column j corresponds truth degree j-th condition evaluated -th row x.","code":""},{"path":"https://beerda.github.io/nuggets/reference/fire.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Obtain truth-degrees of conditions — fire","text":"element condition must character string format \"{p1,p2,p3}\", \"p1\", \"p2\", \"p3\" predicate names. data object x must contain columns whose names correspond exactly predicates referenced conditions. condition evaluated every row x conjunction predicates, conjunction operation determined t_norm argument. empty condition (\"{}\") always evaluated 1 (.e., fully true).","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/fire.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Obtain truth-degrees of conditions — fire","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/fire.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Obtain truth-degrees of conditions — fire","text":"","code":"d <- data.frame(   a = c(1, 0.8, 0.5, 0.2, 0),   b = c(0.5, 1, 0.5, 0, 1),   c = c(0.9, 0.9, 0.1, 0.8, 0.7) )  # Evaluate conditions with different t-norms fire(d, c(\"{a,c}\", \"{}\", \"{a,b,c}\"), t_norm = \"goguen\") #>      [,1] [,2]  [,3] #> [1,] 0.90    1 0.450 #> [2,] 0.72    1 0.720 #> [3,] 0.05    1 0.025 #> [4,] 0.16    1 0.000 #> [5,] 0.00    1 0.000 fire(d, c(\"{a,c}\", \"{a,b}\"), t_norm = \"goedel\") #>      [,1] [,2] #> [1,]  0.9  0.5 #> [2,]  0.8  0.8 #> [3,]  0.1  0.5 #> [4,]  0.2  0.0 #> [5,]  0.0  0.0 fire(d, c(\"{b,c}\"), t_norm = \"lukas\") #>      [,1] #> [1,]  0.4 #> [2,]  0.9 #> [3,]  0.0 #> [4,]  0.0 #> [5,]  0.7"},{"path":"https://beerda.github.io/nuggets/reference/format_condition.html","id":null,"dir":"Reference","previous_headings":"","what":"Format a vector of predicates into a condition string — format_condition","title":"Format a vector of predicates into a condition string — format_condition","text":"Convert character vector predicate names standardized string representation condition. Predicates concatenated commas enclosed curly braces. formatting ensures consistency storing comparing conditions functions.","code":""},{"path":"https://beerda.github.io/nuggets/reference/format_condition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format a vector of predicates into a condition string — format_condition","text":"","code":"format_condition(condition)"},{"path":"https://beerda.github.io/nuggets/reference/format_condition.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format a vector of predicates into a condition string — format_condition","text":"condition character vector predicate names formatted. NULL length zero, result \"{}\", representing empty condition always true.","code":""},{"path":"https://beerda.github.io/nuggets/reference/format_condition.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format a vector of predicates into a condition string — format_condition","text":"character scalar containing formatted condition string.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/format_condition.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Format a vector of predicates into a condition string — format_condition","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/format_condition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format a vector of predicates into a condition string — format_condition","text":"","code":"format_condition(NULL) #> [1] \"{}\" format_condition(character(0)) #> [1] \"{}\" format_condition(c(\"a\", \"b\", \"c\")) #> [1] \"{a,b,c}\""},{"path":"https://beerda.github.io/nuggets/reference/geom_diamond.html","id":null,"dir":"Reference","previous_headings":"","what":"Geom for drawing diamond plots of lattice structures — geom_diamond","title":"Geom for drawing diamond plots of lattice structures — geom_diamond","text":"Create custom ggplot2 geom visualizing lattice structures diamond plots. geom particularly useful displaying association rules ancestor–descendant relationships clear, compact graphical form.","code":""},{"path":"https://beerda.github.io/nuggets/reference/geom_diamond.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Geom for drawing diamond plots of lattice structures — geom_diamond","text":"","code":"geom_diamond(   mapping = NULL,   data = NULL,   stat = \"identity\",   position = \"identity\",   na.rm = FALSE,   linetype = \"solid\",   linewidth = NA,   nudge_x = 0,   nudge_y = 0.125,   show.legend = NA,   inherit.aes = TRUE,   ... )"},{"path":"https://beerda.github.io/nuggets/reference/geom_diamond.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Geom for drawing diamond plots of lattice structures — geom_diamond","text":"mapping Aesthetic mappings, usually created ggplot2::aes(). data data frame representing lattice structure plot. stat Statistical transformation apply; defaults \"identity\". position Position adjustment geom; defaults \"identity\". na.rm Logical; TRUE, missing values silently removed. linetype Line type edges; defaults \"solid\". linewidth Width edges connecting parent child nodes. set NA, edge widths determined linewidth aesthetic. aesthetic provided, default width 0.5 used. nudge_x Horizontal nudge applied labels. nudge_y Vertical nudge applied labels. show.legend Logical; whether include legend. Defaults FALSE. inherit.aes Logical; whether inherit aesthetics plot. Defaults TRUE. ... Additional arguments passed ggplot2::layer().","code":""},{"path":"https://beerda.github.io/nuggets/reference/geom_diamond.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Geom for drawing diamond plots of lattice structures — geom_diamond","text":"ggplot2 layer object adds diamond lattice visualization existing plot.","code":""},{"path":"https://beerda.github.io/nuggets/reference/geom_diamond.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Geom for drawing diamond plots of lattice structures — geom_diamond","text":"diamond plot, nodes (diamonds) represent items conditions within lattice, edges denote inclusion (subset) relationships . geom combines node edge rendering flexible control aesthetics labels, color, size. Concept overview lattice represents inclusion relationships conditions. node corresponds condition, line connects condition direct descendants:   layout positions broader (general) conditions descendants. helps visualize hierarchical structures produced association rule mining subset lattices. Supported aesthetics condition – character vector conditions formatted format_condition(). condition defines one node lattice. hierarchy determined subset inclusion: condition \\(X\\) descendant \\(Y\\) \\(Y \\subset X\\). condition must unique. label – optional text label node. omitted, condition string used. colour – border color node. fill – interior color node. size – size nodes. shape – node shape. alpha – transparency nodes. stroke – border line width nodes. linewidth – edge width parent child nodes, computed difference aesthetic .","code":"{a}          <- ancestor (parent)       /   \\   {a,b}   {a,c}     <- direct descendants (children)      \\     /      {a,b,c}        <- leaf condition"},{"path":"https://beerda.github.io/nuggets/reference/geom_diamond.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Geom for drawing diamond plots of lattice structures — geom_diamond","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/geom_diamond.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Geom for drawing diamond plots of lattice structures — geom_diamond","text":"","code":"if (FALSE) { # \\dontrun{ library(ggplot2)  # Prepare data by partitioning numeric columns into fuzzy or crisp sets part <- partition(iris, .breaks = 3)  # Find all antecedents with \"Sepal\" for rules with consequent \"Species=setosa\" rules <- dig_associations(part,                           antecedent = starts_with(\"Sepal\"),                           consequent = `Species=setosa`,                           min_length = 0,                           max_length = Inf,                           min_coverage = 0,                           min_support = 0,                           min_confidence = 0,                           measures = c(\"lift\", \"conviction\"),                           max_results = Inf)  # Add abbreviated labels for readability rules$abbrev <- shorten_condition(rules$antecedent)  # Plot the lattice of rules as a diamond diagram ggplot(rules) +   aes(condition = antecedent,       fill = confidence,       linewidth = confidence,       size = coverage,       label = abbrev) +   geom_diamond() } # }"},{"path":"https://beerda.github.io/nuggets/reference/is_almost_constant.html","id":null,"dir":"Reference","previous_headings":"","what":"Test whether a vector is almost constant — is_almost_constant","title":"Test whether a vector is almost constant — is_almost_constant","text":"Check vector contains (almost) value majority elements. function returns TRUE proportion frequent value x greater equal specified threshold.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_almost_constant.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test whether a vector is almost constant — is_almost_constant","text":"","code":"is_almost_constant(x, threshold = 1, na_rm = FALSE)"},{"path":"https://beerda.github.io/nuggets/reference/is_almost_constant.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test whether a vector is almost constant — is_almost_constant","text":"x vector tested. threshold numeric scalar interval \\([0,1]\\) specifying minimum required proportion frequent value. Defaults 1. na_rm Logical; TRUE, NA values removed computing proportions. FALSE, NA treated ordinary value, large number NAs can cause function return TRUE.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_almost_constant.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test whether a vector is almost constant — is_almost_constant","text":"logical scalar. Returns TRUE following cases: x empty length one. x contains NA values. proportion frequent value x greater equal threshold. Otherwise, returns FALSE.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_almost_constant.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Test whether a vector is almost constant — is_almost_constant","text":"useful detecting low-variability degenerate variables, may uninformative modeling analysis.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/is_almost_constant.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Test whether a vector is almost constant — is_almost_constant","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_almost_constant.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test whether a vector is almost constant — is_almost_constant","text":"","code":"is_almost_constant(1) #> [1] TRUE is_almost_constant(1:10) #> [1] FALSE is_almost_constant(c(NA, NA, NA), na_rm = TRUE) #> [1] TRUE is_almost_constant(c(NA, NA, NA), na_rm = FALSE) #> [1] TRUE is_almost_constant(c(NA, NA, NA, 1, 2), threshold = 0.5, na_rm = FALSE) #> [1] TRUE is_almost_constant(c(NA, NA, NA, 1, 2), threshold = 0.5, na_rm = TRUE) #> [1] TRUE"},{"path":"https://beerda.github.io/nuggets/reference/is_condition.html","id":null,"dir":"Reference","previous_headings":"","what":"Check whether a list of character vectors contains valid conditions — is_condition","title":"Check whether a list of character vectors contains valid conditions — is_condition","text":"valid condition character vector predicate names, predicate corresponds column name given data frame matrix. function verifies element list x contains valid predicates match column names data.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_condition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check whether a list of character vectors contains valid conditions — is_condition","text":"","code":"is_condition(x, data)"},{"path":"https://beerda.github.io/nuggets/reference/is_condition.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check whether a list of character vectors contains valid conditions — is_condition","text":"x list character vectors, representing condition. data matrix data frame whose column names define valid predicates.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_condition.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check whether a list of character vectors contains valid conditions — is_condition","text":"logical vector one element condition x. element TRUE corresponding condition valid, .e. predicates column names data. Otherwise, FALSE.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_condition.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check whether a list of character vectors contains valid conditions — is_condition","text":"Special cases: empty character vector (character(0)) considered valid condition always passes check. NULL element treated empty character vector, .e., also valid condition.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/is_condition.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Check whether a list of character vectors contains valid conditions — is_condition","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_condition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check whether a list of character vectors contains valid conditions — is_condition","text":"","code":"d <- data.frame(foo = 1:5, bar = 1:5, blah = 1:5)  is_condition(list(\"foo\"), d) #> [1] TRUE is_condition(list(c(\"bar\", \"blah\"), NULL, c(\"foo\", \"bzz\")), d) #> [1]  TRUE  TRUE FALSE"},{"path":"https://beerda.github.io/nuggets/reference/is_degree.html","id":null,"dir":"Reference","previous_headings":"","what":"Test whether an object contains numeric values from the interval \\([0,1]\\) — is_degree","title":"Test whether an object contains numeric values from the interval \\([0,1]\\) — is_degree","text":"Check input consists numeric values 0 1, inclusive. often useful validating truth degrees, membership values fuzzy sets, probabilities.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_degree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test whether an object contains numeric values from the interval \\([0,1]\\) — is_degree","text":"","code":"is_degree(x, na_rm = FALSE)"},{"path":"https://beerda.github.io/nuggets/reference/is_degree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test whether an object contains numeric values from the interval \\([0,1]\\) — is_degree","text":"x object tested. Can numeric vector, matrix, array. na_rm Logical; whether ignore NA values. TRUE, NAs treated valid values. FALSE x contains NAs, function immediately returns FALSE.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_degree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test whether an object contains numeric values from the interval \\([0,1]\\) — is_degree","text":"logical scalar. Returns TRUE (non-NA) elements x numeric lie within closed interval \\([0,1]\\). Returns FALSE : x contains NA values na_rm = FALSE element outside interval \\([0,1]\\) x numeric x empty (length(x) == 0)","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/is_degree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Test whether an object contains numeric values from the interval \\([0,1]\\) — is_degree","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_degree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test whether an object contains numeric values from the interval \\([0,1]\\) — is_degree","text":"","code":"is_degree(0.5) #> [1] TRUE is_degree(c(0, 0.2, 1)) #> [1] TRUE is_degree(c(0.5, NA), na_rm = TRUE)   # TRUE #> [1] TRUE is_degree(c(0.5, NA), na_rm = FALSE)  # FALSE #> [1] FALSE is_degree(c(-0.1, 0.5))               # FALSE #> [1] FALSE is_degree(numeric(0))                 # FALSE #> [1] TRUE"},{"path":"https://beerda.github.io/nuggets/reference/is_nugget.html","id":null,"dir":"Reference","previous_headings":"","what":"Test whether an object is a nugget — is_nugget","title":"Test whether an object is a nugget — is_nugget","text":"Check given object nugget, .e. object created nugget(). flavour specified, function returns TRUE object nugget given flavour.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_nugget.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test whether an object is a nugget — is_nugget","text":"","code":"is_nugget(x, flavour = NULL)"},{"path":"https://beerda.github.io/nuggets/reference/is_nugget.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test whether an object is a nugget — is_nugget","text":"x object tested. flavour Optional character string specifying required flavour nugget. NULL (default), function checks whether x nugget flavour.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_nugget.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test whether an object is a nugget — is_nugget","text":"logical scalar: TRUE x nugget (specified flavour, given), otherwise FALSE.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_nugget.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Test whether an object is a nugget — is_nugget","text":"Technically, nuggets implemented S3 objects. object considered nugget inherits S3 class \"nugget\". nugget given flavour inherits specified flavour class \"nugget\" class.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/is_nugget.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Test whether an object is a nugget — is_nugget","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_subset.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine whether one vector is a subset of another — is_subset","title":"Determine whether one vector is a subset of another — is_subset","text":"Check elements x also contained y. equivalent testing whether setdiff(x, y) empty.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_subset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine whether one vector is a subset of another — is_subset","text":"","code":"is_subset(x, y)"},{"path":"https://beerda.github.io/nuggets/reference/is_subset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine whether one vector is a subset of another — is_subset","text":"x first vector. y second vector.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_subset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine whether one vector is a subset of another — is_subset","text":"logical scalar. Returns TRUE x subset y, .e. elements x also elements y. Returns FALSE otherwise.","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_subset.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Determine whether one vector is a subset of another — is_subset","text":"x empty, result always TRUE (empty set subset set). y empty x , result FALSE. Duplicates x ignored; set membership tested. NA values treated ordinary elements. particular, NA x considered subset element NA also present y.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/is_subset.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Determine whether one vector is a subset of another — is_subset","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/is_subset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine whether one vector is a subset of another — is_subset","text":"","code":"is_subset(1:3, 1:5)               # TRUE #> [1] TRUE is_subset(c(2, 5), 1:4)           # FALSE #> [1] FALSE is_subset(numeric(0), 1:5)        # TRUE #> [1] TRUE is_subset(1:3, numeric(0))        # FALSE #> [1] FALSE is_subset(c(1, NA), c(1, 2, NA))  # TRUE #> [1] TRUE is_subset(c(NA), 1:5)             # FALSE #> [1] FALSE"},{"path":"https://beerda.github.io/nuggets/reference/nugget.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a nugget object of a given flavour — nugget","title":"Create a nugget object of a given flavour — nugget","text":"Construct nugget object, S3 object used store represent results (e.g., rules patterns) nuggets framework.","code":""},{"path":"https://beerda.github.io/nuggets/reference/nugget.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a nugget object of a given flavour — nugget","text":"","code":"nugget(x, flavour, call_function, call_data, call_args)"},{"path":"https://beerda.github.io/nuggets/reference/nugget.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a nugget object of a given flavour — nugget","text":"x object rules patterns, typically tibble data frame. NULL, converted empty tibble. flavour character string specifying flavour nugget, NULL flavour assigned. given, returned object inherit \"nugget\" specified flavour class. call_function character scalar giving name function created nugget. Stored attribute provenance. call_data list containing information data passed function created nugget. Stored attribute reproducibility. call_args list arguments passed function created nugget. Stored attribute reproducibility.","code":""},{"path":"https://beerda.github.io/nuggets/reference/nugget.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a nugget object of a given flavour — nugget","text":"tibble object S3 subclass \"nugget\" , specified, given flavour class. object also contains attributes \"call_function\" \"call_args\" describing provenance.","code":""},{"path":"https://beerda.github.io/nuggets/reference/nugget.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a nugget object of a given flavour — nugget","text":"nugget technically tibble (data frame) inherits \"nugget\" class , optionally, flavour-specific S3 class. allows distinguishing different types nuggets (flavours) still supporting generic methods nuggets. nugget stores additional provenance information attributes: \"call_function\" — name function created nugget. \"call_args\" — list arguments passed function. attributes make possible reconstruct track nugget created, supports reproducibility, transparency, debugging. example, one can inspect attr(n, \"call_args\") recover original parameters used mine patterns.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/nugget.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create a nugget object of a given flavour — nugget","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/nugget.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a nugget object of a given flavour — nugget","text":"","code":"df <- data.frame(lhs = c(\"a\", \"b\"), rhs = c(\"c\", \"d\")) n <- nugget(df,             flavour = \"rules\",             call_function = \"example_function\",             call_data = list(ncol = 2,                              nrow = 2,                              colnames = c(\"lhs\", \"rhs\")),             call_args = list(data = \"mydata\"))  inherits(n, \"nugget\")      # TRUE #> [1] TRUE inherits(n, \"rules\")       # TRUE #> [1] TRUE attr(n, \"call_function\")   # \"dig_example_function\" #> [1] \"example_function\" attr(n, \"call_args\")       # list(data = \"mydata\") #> $data #> [1] \"mydata\" #>"},{"path":"https://beerda.github.io/nuggets/reference/nuggets-package.html","id":null,"dir":"Reference","previous_headings":"","what":"nuggets: Extensible Framework for Data Pattern Exploration — nuggets-package","title":"nuggets: Extensible Framework for Data Pattern Exploration — nuggets-package","text":"framework systematic exploration association rules (Agrawal et al., 1994, https://www.vldb.org/conf/1994/P487.PDF), contrast patterns (Chen, 2022, doi:10.48550/arXiv.2209.13556 ), emerging patterns (Dong et al., 1999, doi:10.1145/312129.312191 ), subgroup discovery (Atzmueller, 2015, doi:10.1002/widm.1144 ), conditional correlations (Hájek, 1978, doi:10.1007/978-3-642-66943-9 ). User-defined functions may also supplied guide custom pattern searches. Supports crisp (Boolean) fuzzy data. Generates candidate conditions expressed elementary conjunctions, evaluates dataset, inspects induced sub-data statistical, logical, structural properties associations, correlations, contrasts. Includes methods visualization logical structures supports interactive exploration integrated Shiny applications.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/nuggets-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"nuggets: Extensible Framework for Data Pattern Exploration — nuggets-package","text":"Maintainer: Michal Burda michal.burda@osu.cz (ORCID)","code":""},{"path":"https://beerda.github.io/nuggets/reference/parse_condition.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert condition strings into lists of predicate vectors — parse_condition","title":"Convert condition strings into lists of predicate vectors — parse_condition","text":"Parse character vector conditions list predicate vectors. element list corresponds one condition. condition string predicates separated commas enclosed curly braces, produced format_condition(). function splits string component predicates.","code":""},{"path":"https://beerda.github.io/nuggets/reference/parse_condition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert condition strings into lists of predicate vectors — parse_condition","text":"","code":"parse_condition(..., .sort = FALSE)"},{"path":"https://beerda.github.io/nuggets/reference/parse_condition.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert condition strings into lists of predicate vectors — parse_condition","text":"... One character vectors conditions parsed. .sort Logical flag indicating whether predicates result sorted alphabetically. Defaults FALSE.","code":""},{"path":"https://beerda.github.io/nuggets/reference/parse_condition.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert condition strings into lists of predicate vectors — parse_condition","text":"list character vectors, element corresponds one condition contains parsed predicates.","code":""},{"path":"https://beerda.github.io/nuggets/reference/parse_condition.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert condition strings into lists of predicate vectors — parse_condition","text":"multiple vectors conditions provided via ..., combined element-wise. result single list element formed merging predicates corresponding elements input vectors. input vectors differ length, shorter ones recycled. Empty conditions (\"{}\") parsed empty character vectors (character(0)).","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/parse_condition.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Convert condition strings into lists of predicate vectors — parse_condition","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/parse_condition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert condition strings into lists of predicate vectors — parse_condition","text":"","code":"parse_condition(c(\"{a}\", \"{x=1, z=2, y=3}\", \"{}\")) #> [[1]] #> [1] \"a\" #>  #> [[2]] #> [1] \"x=1\" \"z=2\" \"y=3\" #>  #> [[3]] #> character(0) #>   # Merge conditions from multiple vectors element-wise parse_condition(c(\"{b}\", \"{x=1, z=2, y=3}\", \"{q}\", \"{}\"),                 c(\"{a}\", \"{v=10, w=11}\",    \"{}\",  \"{r,s,t}\")) #> [[1]] #> [1] \"b\" \"a\" #>  #> [[2]] #> [1] \"x=1\"  \"z=2\"  \"y=3\"  \"v=10\" \"w=11\" #>  #> [[3]] #> [1] \"q\" #>  #> [[4]] #> [1] \"r\" \"s\" \"t\" #>   # Sorting predicates within each condition parse_condition(\"{z,y,x}\", .sort = TRUE) #> [[1]] #> [1] \"x\" \"y\" \"z\" #>"},{"path":"https://beerda.github.io/nuggets/reference/partition.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert columns of a data frame to Boolean or fuzzy sets (triangular, trapezoidal, or raised-cosine) — partition","title":"Convert columns of a data frame to Boolean or fuzzy sets (triangular, trapezoidal, or raised-cosine) — partition","text":"Transform selected columns data frame either dummy logical variables membership degrees fuzzy sets, leaving remaining columns unchanged. transformed column typically produces multiple new columns output.","code":""},{"path":"https://beerda.github.io/nuggets/reference/partition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert columns of a data frame to Boolean or fuzzy sets (triangular, trapezoidal, or raised-cosine) — partition","text":"","code":"partition(   .data,   .what = everything(),   ...,   .breaks = NULL,   .labels = NULL,   .na = TRUE,   .keep = FALSE,   .method = \"crisp\",   .style = \"equal\",   .style_params = list(),   .right = TRUE,   .span = 1,   .inc = 1 )"},{"path":"https://beerda.github.io/nuggets/reference/partition.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert columns of a data frame to Boolean or fuzzy sets (triangular, trapezoidal, or raised-cosine) — partition","text":".data data frame processed. .tidyselect expression (see tidyselect syntax) selecting columns transform. ... Additional tidyselect expressions selecting columns. .breaks Ignored .method = \"dummy\". methods, either integer (number intervals/sets) numeric vector breakpoints. .labels Optional character vector labels used new column names. NULL, labels generated automatically. .na TRUE, adds extra logical column source column containing NA values (e.g., x=NA). .keep TRUE, keep original columns output. .method Transformation method numeric columns: \"dummy\", \"crisp\", \"triangle\", \"raisedcos\". .style Controls breakpoints determined .breaks integer. Values correspond methods classInt::classIntervals(), e.g., \"equal\", \"quantile\", \"kmeans\", \"sd\", \"hclust\", \"bclust\", \"fisher\", \"jenks\", \"dpih\", \"headtails\", \"maximum\", \"box\". Defaults \"equal\". Used .method = \"crisp\" .breaks single integer. .style_params named list parameters passed interval computation method specified .style. Used .method = \"crisp\" .breaks integer. .right \"crisp\", whether intervals right-closed left-open (TRUE), left-closed right-open (FALSE). .span Number consecutive breaks forming set. \"crisp\", controls interval width. \"triangle\"/\"raisedcos\", .span = 1 produces triangular sets, .span = 2 trapezoidal sets. .inc Step size shifting breaks generating successive sets. .inc = 1, possible sets created; larger values skip sets.","code":""},{"path":"https://beerda.github.io/nuggets/reference/partition.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert columns of a data frame to Boolean or fuzzy sets (triangular, trapezoidal, or raised-cosine) — partition","text":"tibble .data transformed Boolean fuzzy predicates.","code":""},{"path":"https://beerda.github.io/nuggets/reference/partition.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert columns of a data frame to Boolean or fuzzy sets (triangular, trapezoidal, or raised-cosine) — partition","text":"transformations often used preprocessing step calling dig() one derivatives, dig_correlations(), dig_paired_baseline_contrasts(), dig_associations(). transformation depends column type: logical column x expanded two logical columns: x=TRUE x=FALSE; factor column x levels l1, l2, l3 becomes three logical columns: x=l1, x=l2, x=l3; numeric column x transformed according .method: .method = \"dummy\": column treated factor one level per unique value, expanded dummy columns; .method = \"crisp\": column discretized intervals (defined .breaks, .style, .style_params) expanded dummy columns representing intervals; .method = \"triangle\" .method = \"raisedcos\": column converted one fuzzy sets, represented membership degrees \\([0,1]\\) (triangular raised-cosine shaped). Details numeric transformations controlled .breaks, .labels, .style, .style_params, .right, .span, .inc. Crisp partitioning efficient works well attributes distinct categories clear boundaries. Fuzzy partitioning recommended modeling gradual changes uncertainty, allowing smooth category transitions higher computational cost.","code":""},{"path":"https://beerda.github.io/nuggets/reference/partition.html","id":"crisp-transformation-of-numeric-data","dir":"Reference","previous_headings":"","what":"Crisp transformation of numeric data","title":"Convert columns of a data frame to Boolean or fuzzy sets (triangular, trapezoidal, or raised-cosine) — partition","text":".method = \"crisp\", numeric columns discretized set dummy logical variables, representing one interval values. .breaks integer, specifies number intervals column divided. intervals determined using .style .style_params arguments, allowing equal-width also data-driven breakpoints (e.g., quantile k-means based). first last intervals automatically extend infinity. .breaks numeric vector, specifies interval boundaries directly. Infinite values allowed. .style argument defines breakpoints computed .breaks integer. Supported methods (classInt::classIntervals()) include: \"equal\" – equal-width intervals across column range (default); \"quantile\" – equal-frequency intervals (see quantile() additional parameters may passed .style_params; note probs parameter set automatically included .style_params); \"kmeans\" – intervals found 1D k-means clustering (see kmeans() additional parameters); \"sd\" – intervals based standard deviations mean; \"hclust\" – hierarchical clustering intervals (see hclust() additional parameters); \"bclust\" – model-based clustering intervals (see e1071::bclust() additional parameters); \"fisher\" / \"jenks\" – Fisher–Jenks optimal partitioning; \"dpih\" – kernel-based density partitioning (see KernSmooth::dpih() additional parameters); \"headtails\" – head/tails natural breaks; \"maximum\" – maximization-based partitioning; \"box\" – breaks boxplot hinges. Additional parameters methods can passed .style_params, named list arguments accepted respective algorithm classInt::classIntervals(). example, .style = \"kmeans\", one can specify .style_params = list(algorithm = \"Lloyd\") request Lloyd's algorithm k-means clustering. .span = 1 .inc = 1, generated intervals consecutive non-overlapping. example, .breaks = c(1, 3, 5, 7, 9, 11) .right = TRUE, intervals \\((1;3]\\), \\((3;5]\\), \\((5;7]\\), \\((7;9]\\), \\((9;11]\\). .right = FALSE, intervals left-closed: \\([1;3)\\), \\([3;5)\\), etc. Larger .span values produce overlapping intervals. example, .span = 2, .inc = 1, .right = TRUE, intervals \\((1;5]\\), \\((3;7]\\), \\((5;9]\\), \\((7;11]\\). .inc argument controls far window shifts along .breaks. .span = 1, .inc = 2 → \\((1;3]\\), \\((5;7]\\), \\((9;11]\\). .span = 2, .inc = 3 → \\((1;5]\\), \\((9;11]\\).","code":""},{"path":"https://beerda.github.io/nuggets/reference/partition.html","id":"fuzzy-transformation-of-numeric-data","dir":"Reference","previous_headings":"","what":"Fuzzy transformation of numeric data","title":"Convert columns of a data frame to Boolean or fuzzy sets (triangular, trapezoidal, or raised-cosine) — partition","text":".method = \"triangle\" .method = \"raisedcos\", numeric columns converted fuzzy membership degrees \\([0,1]\\). .breaks integer, specifies number fuzzy sets. .breaks numeric vector, directly defines fuzzy set boundaries. Infinite values produce open-ended sets. .span = 1, fuzzy set defined three consecutive breaks: membership 0 outside outer breaks, rises 1 middle break, decreases back 0 — yielding triangular raised-cosine sets. .span > 1, fuzzy sets use four consecutive breaks: membership increases first two, remains 1 middle two, decreases last two — creating trapezoidal sets. Border shapes linear .method = \"triangle\" cosine .method = \"raisedcos\". .inc argument defines step break windows: .span = 1, .inc = 1 → \\((1;3;5)\\), \\((3;5;7)\\), \\((5;7;9)\\), \\((7;9;11)\\). .span = 2, .inc = 1 → \\((1;3;5;7)\\), \\((3;5;7;9)\\), \\((5;7;9;11)\\). .span = 1, .inc = 3 → \\((1;3;5)\\), \\((7;9;11)\\).","code":""},{"path":"https://beerda.github.io/nuggets/reference/partition.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Convert columns of a data frame to Boolean or fuzzy sets (triangular, trapezoidal, or raised-cosine) — partition","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/partition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert columns of a data frame to Boolean or fuzzy sets (triangular, trapezoidal, or raised-cosine) — partition","text":"","code":"# Crisp transformation using equal-width bins partition(CO2, conc, .method = \"crisp\", .breaks = 4) #> # A tibble: 84 × 8 #>    Plant Type   Treatment  uptake `conc=(-Inf;321]` `conc=(321;548]` #>    <ord> <fct>  <fct>       <dbl> <lgl>             <lgl>            #>  1 Qn1   Quebec nonchilled   16   TRUE              FALSE            #>  2 Qn1   Quebec nonchilled   30.4 TRUE              FALSE            #>  3 Qn1   Quebec nonchilled   34.8 TRUE              FALSE            #>  4 Qn1   Quebec nonchilled   37.2 FALSE             TRUE             #>  5 Qn1   Quebec nonchilled   35.3 FALSE             TRUE             #>  6 Qn1   Quebec nonchilled   39.2 FALSE             FALSE            #>  7 Qn1   Quebec nonchilled   39.7 FALSE             FALSE            #>  8 Qn2   Quebec nonchilled   13.6 TRUE              FALSE            #>  9 Qn2   Quebec nonchilled   27.3 TRUE              FALSE            #> 10 Qn2   Quebec nonchilled   37.1 TRUE              FALSE            #> # ℹ 74 more rows #> # ℹ 2 more variables: `conc=(548;774]` <lgl>, `conc=(774;Inf]` <lgl>  # Crisp transformation using quantile-based bins partition(CO2, conc, .method = \"crisp\", .breaks = 4, .style = \"quantile\") #> # A tibble: 84 × 8 #>    Plant Type   Treatment  uptake `conc=(-Inf;175]` `conc=(175;350]` #>    <ord> <fct>  <fct>       <dbl> <lgl>             <lgl>            #>  1 Qn1   Quebec nonchilled   16   TRUE              FALSE            #>  2 Qn1   Quebec nonchilled   30.4 TRUE              FALSE            #>  3 Qn1   Quebec nonchilled   34.8 FALSE             TRUE             #>  4 Qn1   Quebec nonchilled   37.2 FALSE             TRUE             #>  5 Qn1   Quebec nonchilled   35.3 FALSE             FALSE            #>  6 Qn1   Quebec nonchilled   39.2 FALSE             FALSE            #>  7 Qn1   Quebec nonchilled   39.7 FALSE             FALSE            #>  8 Qn2   Quebec nonchilled   13.6 TRUE              FALSE            #>  9 Qn2   Quebec nonchilled   27.3 TRUE              FALSE            #> 10 Qn2   Quebec nonchilled   37.1 FALSE             TRUE             #> # ℹ 74 more rows #> # ℹ 2 more variables: `conc=(350;675]` <lgl>, `conc=(675;Inf]` <lgl>  # Crisp transformation using k-means clustering for breakpoints partition(CO2, conc, .method = \"crisp\", .breaks = 4, .style = \"kmeans\") #> # A tibble: 84 × 8 #>    Plant Type   Treatment  uptake `conc=(-Inf;135]` `conc=(135;300]` #>    <ord> <fct>  <fct>       <dbl> <lgl>             <lgl>            #>  1 Qn1   Quebec nonchilled   16   TRUE              FALSE            #>  2 Qn1   Quebec nonchilled   30.4 FALSE             TRUE             #>  3 Qn1   Quebec nonchilled   34.8 FALSE             TRUE             #>  4 Qn1   Quebec nonchilled   37.2 FALSE             FALSE            #>  5 Qn1   Quebec nonchilled   35.3 FALSE             FALSE            #>  6 Qn1   Quebec nonchilled   39.2 FALSE             FALSE            #>  7 Qn1   Quebec nonchilled   39.7 FALSE             FALSE            #>  8 Qn2   Quebec nonchilled   13.6 TRUE              FALSE            #>  9 Qn2   Quebec nonchilled   27.3 FALSE             TRUE             #> 10 Qn2   Quebec nonchilled   37.1 FALSE             TRUE             #> # ℹ 74 more rows #> # ℹ 2 more variables: `conc=(300;588]` <lgl>, `conc=(588;Inf]` <lgl>  # Crisp transformation using Lloyd algorithm for k-means clustering for breakpoints partition(CO2, conc, .method = \"crisp\", .breaks = 4, .style = \"kmeans\",           .style_params = list(algorithm = \"Lloyd\")) #> # A tibble: 84 × 8 #>    Plant Type   Treatment  uptake `conc=(-Inf;135]` `conc=(135;300]` #>    <ord> <fct>  <fct>       <dbl> <lgl>             <lgl>            #>  1 Qn1   Quebec nonchilled   16   TRUE              FALSE            #>  2 Qn1   Quebec nonchilled   30.4 FALSE             TRUE             #>  3 Qn1   Quebec nonchilled   34.8 FALSE             TRUE             #>  4 Qn1   Quebec nonchilled   37.2 FALSE             FALSE            #>  5 Qn1   Quebec nonchilled   35.3 FALSE             FALSE            #>  6 Qn1   Quebec nonchilled   39.2 FALSE             FALSE            #>  7 Qn1   Quebec nonchilled   39.7 FALSE             FALSE            #>  8 Qn2   Quebec nonchilled   13.6 TRUE              FALSE            #>  9 Qn2   Quebec nonchilled   27.3 FALSE             TRUE             #> 10 Qn2   Quebec nonchilled   37.1 FALSE             TRUE             #> # ℹ 74 more rows #> # ℹ 2 more variables: `conc=(300;588]` <lgl>, `conc=(588;Inf]` <lgl>  # Fuzzy triangular transformation (default) partition(CO2, conc:uptake, .method = \"triangle\", .breaks = 3) #> # A tibble: 84 × 9 #>    Plant Type   Treatment  `conc=(-Inf;95;548)` `conc=(95;548;1000)` #>    <ord> <fct>  <fct>                     <dbl>                <dbl> #>  1 Qn1   Quebec nonchilled                1                    0     #>  2 Qn1   Quebec nonchilled                0.823                0.177 #>  3 Qn1   Quebec nonchilled                0.658                0.342 #>  4 Qn1   Quebec nonchilled                0.437                0.563 #>  5 Qn1   Quebec nonchilled                0.106                0.894 #>  6 Qn1   Quebec nonchilled                0                    0.719 #>  7 Qn1   Quebec nonchilled                0                    0     #>  8 Qn2   Quebec nonchilled                1                    0     #>  9 Qn2   Quebec nonchilled                0.823                0.177 #> 10 Qn2   Quebec nonchilled                0.658                0.342 #> # ℹ 74 more rows #> # ℹ 4 more variables: `conc=(548;1000;Inf)` <dbl>, #> #   `uptake=(-Inf;7.7;26.6)` <dbl>, `uptake=(7.7;26.6;45.5)` <dbl>, #> #   `uptake=(26.6;45.5;Inf)` <dbl>  # Raised-cosine fuzzy sets partition(CO2, conc:uptake, .method = \"raisedcos\", .breaks = 3) #> # A tibble: 84 × 9 #>    Plant Type   Treatment  `conc=(-Inf;95;548)` `conc=(95;548;1000)` #>    <ord> <fct>  <fct>                     <dbl>                <dbl> #>  1 Qn1   Quebec nonchilled               1                    0      #>  2 Qn1   Quebec nonchilled               0.925                0.0750 #>  3 Qn1   Quebec nonchilled               0.738                0.262  #>  4 Qn1   Quebec nonchilled               0.402                0.598  #>  5 Qn1   Quebec nonchilled               0.0274               0.973  #>  6 Qn1   Quebec nonchilled               0                    0.818  #>  7 Qn1   Quebec nonchilled               0                    0      #>  8 Qn2   Quebec nonchilled               1                    0      #>  9 Qn2   Quebec nonchilled               0.925                0.0750 #> 10 Qn2   Quebec nonchilled               0.738                0.262  #> # ℹ 74 more rows #> # ℹ 4 more variables: `conc=(548;1000;Inf)` <dbl>, #> #   `uptake=(-Inf;7.7;26.6)` <dbl>, `uptake=(7.7;26.6;45.5)` <dbl>, #> #   `uptake=(26.6;45.5;Inf)` <dbl>  # Overlapping trapezoidal fuzzy sets (Ruspini condition) partition(CO2, conc:uptake, .method = \"triangle\", .breaks = 3,           .span = 2, .inc = 2) #> # A tibble: 84 × 9 #>    Plant Type   Treatment  `conc=(-Inf;95;276;457)` `conc=(276;457;638;819)` #>    <ord> <fct>  <fct>                         <dbl>                    <dbl> #>  1 Qn1   Quebec nonchilled                    1                        0     #>  2 Qn1   Quebec nonchilled                    1                        0     #>  3 Qn1   Quebec nonchilled                    1                        0     #>  4 Qn1   Quebec nonchilled                    0.591                    0.409 #>  5 Qn1   Quebec nonchilled                    0                        1     #>  6 Qn1   Quebec nonchilled                    0                        0.796 #>  7 Qn1   Quebec nonchilled                    0                        0     #>  8 Qn2   Quebec nonchilled                    1                        0     #>  9 Qn2   Quebec nonchilled                    1                        0     #> 10 Qn2   Quebec nonchilled                    1                        0     #> # ℹ 74 more rows #> # ℹ 4 more variables: `conc=(638;819;1000;Inf)` <dbl>, #> #   `uptake=(-Inf;7.7;15.3;22.8)` <dbl>, `uptake=(15.3;22.8;30.4;37.9)` <dbl>, #> #   `uptake=(30.4;37.9;45.5;Inf)` <dbl>  # Different settings per column CO2 |>   partition(Plant:Treatment) |>   partition(conc,             .method = \"raisedcos\",             .breaks = c(-Inf, 95, 175, 350, 675, 1000, Inf)) |>   partition(uptake,             .method = \"triangle\",             .breaks = c(-Inf, 7.7, 28.3, 45.5, Inf),             .labels = c(\"low\", \"medium\", \"high\")) #> # A tibble: 84 × 24 #>    `Plant=Qn1` `Plant=Qn2` `Plant=Qn3` `Plant=Qc1` `Plant=Qc3` `Plant=Qc2` #>    <lgl>       <lgl>       <lgl>       <lgl>       <lgl>       <lgl>       #>  1 TRUE        FALSE       FALSE       FALSE       FALSE       FALSE       #>  2 TRUE        FALSE       FALSE       FALSE       FALSE       FALSE       #>  3 TRUE        FALSE       FALSE       FALSE       FALSE       FALSE       #>  4 TRUE        FALSE       FALSE       FALSE       FALSE       FALSE       #>  5 TRUE        FALSE       FALSE       FALSE       FALSE       FALSE       #>  6 TRUE        FALSE       FALSE       FALSE       FALSE       FALSE       #>  7 TRUE        FALSE       FALSE       FALSE       FALSE       FALSE       #>  8 FALSE       TRUE        FALSE       FALSE       FALSE       FALSE       #>  9 FALSE       TRUE        FALSE       FALSE       FALSE       FALSE       #> 10 FALSE       TRUE        FALSE       FALSE       FALSE       FALSE       #> # ℹ 74 more rows #> # ℹ 18 more variables: `Plant=Mn3` <lgl>, `Plant=Mn2` <lgl>, `Plant=Mn1` <lgl>, #> #   `Plant=Mc2` <lgl>, `Plant=Mc3` <lgl>, `Plant=Mc1` <lgl>, #> #   `Type=Quebec` <lgl>, `Type=Mississippi` <lgl>, #> #   `Treatment=nonchilled` <lgl>, `Treatment=chilled` <lgl>, #> #   `conc=(-Inf;95;175)` <dbl>, `conc=(95;175;350)` <dbl>, #> #   `conc=(175;350;675)` <dbl>, `conc=(350;675;1000)` <dbl>, …"},{"path":"https://beerda.github.io/nuggets/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics explore","code":""},{"path":"https://beerda.github.io/nuggets/reference/remove_almost_constant.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove almost constant columns from a data frame — remove_almost_constant","title":"Remove almost constant columns from a data frame — remove_almost_constant","text":"Test columns specified .remove almost constant. column considered almost constant proportion frequent value greater equal threshold specified .threshold. See is_almost_constant() details.","code":""},{"path":"https://beerda.github.io/nuggets/reference/remove_almost_constant.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove almost constant columns from a data frame — remove_almost_constant","text":"","code":"remove_almost_constant(   .data,   .what = everything(),   ...,   .threshold = 1,   .na_rm = FALSE,   .verbose = FALSE )"},{"path":"https://beerda.github.io/nuggets/reference/remove_almost_constant.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove almost constant columns from a data frame — remove_almost_constant","text":".data data frame. .tidyselect expression (see tidyselect syntax) specifying columns process. ... Additional tidyselect expressions selecting columns. .threshold Numeric scalar interval \\([0,1]\\) giving minimum required proportion frequent value column considered almost constant. .na_rm Logical; TRUE, NA values removed computing proportions. FALSE, NA treated regular value. See is_almost_constant() details. .verbose Logical; TRUE, print message listing removed columns.","code":""},{"path":"https://beerda.github.io/nuggets/reference/remove_almost_constant.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove almost constant columns from a data frame — remove_almost_constant","text":"data frame selected columns removed meet definition almost constant.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/remove_almost_constant.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Remove almost constant columns from a data frame — remove_almost_constant","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/remove_almost_constant.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove almost constant columns from a data frame — remove_almost_constant","text":"","code":"d <- data.frame(a1 = 1:10,                 a2 = c(1:9, NA),                 b1 = \"b\",                 b2 = NA,                 c1 = rep(c(TRUE, FALSE), 5),                 c2 = rep(c(TRUE, NA), 5),                 d  = c(rep(TRUE, 4), rep(FALSE, 4), NA, NA))  # Remove columns that are constant (threshold = 1) remove_almost_constant(d, .threshold = 1.0, .na_rm = FALSE) #> # A tibble: 10 × 5 #>       a1    a2 c1    c2    d     #>    <int> <int> <lgl> <lgl> <lgl> #>  1     1     1 TRUE  TRUE  TRUE  #>  2     2     2 FALSE NA    TRUE  #>  3     3     3 TRUE  TRUE  TRUE  #>  4     4     4 FALSE NA    TRUE  #>  5     5     5 TRUE  TRUE  FALSE #>  6     6     6 FALSE NA    FALSE #>  7     7     7 TRUE  TRUE  FALSE #>  8     8     8 FALSE NA    FALSE #>  9     9     9 TRUE  TRUE  NA    #> 10    10    NA FALSE NA    NA    remove_almost_constant(d, .threshold = 1.0, .na_rm = TRUE) #> # A tibble: 10 × 4 #>       a1    a2 c1    d     #>    <int> <int> <lgl> <lgl> #>  1     1     1 TRUE  TRUE  #>  2     2     2 FALSE TRUE  #>  3     3     3 TRUE  TRUE  #>  4     4     4 FALSE TRUE  #>  5     5     5 TRUE  FALSE #>  6     6     6 FALSE FALSE #>  7     7     7 TRUE  FALSE #>  8     8     8 FALSE FALSE #>  9     9     9 TRUE  NA    #> 10    10    NA FALSE NA     # Remove columns where the majority value occurs in >= 50% of rows remove_almost_constant(d, .threshold = 0.5, .na_rm = FALSE) #> # A tibble: 10 × 3 #>       a1    a2 d     #>    <int> <int> <lgl> #>  1     1     1 TRUE  #>  2     2     2 TRUE  #>  3     3     3 TRUE  #>  4     4     4 TRUE  #>  5     5     5 FALSE #>  6     6     6 FALSE #>  7     7     7 FALSE #>  8     8     8 FALSE #>  9     9     9 NA    #> 10    10    NA NA    remove_almost_constant(d, .threshold = 0.5, .na_rm = TRUE) #> # A tibble: 10 × 2 #>       a1    a2 #>    <int> <int> #>  1     1     1 #>  2     2     2 #>  3     3     3 #>  4     4     4 #>  5     5     5 #>  6     6     6 #>  7     7     7 #>  8     8     8 #>  9     9     9 #> 10    10    NA  # Restrict check to a subset of columns remove_almost_constant(d, a1:b2, .threshold = 0.5, .na_rm = TRUE) #> # A tibble: 10 × 5 #>       a1    a2 c1    c2    d     #>    <int> <int> <lgl> <lgl> <lgl> #>  1     1     1 TRUE  TRUE  TRUE  #>  2     2     2 FALSE NA    TRUE  #>  3     3     3 TRUE  TRUE  TRUE  #>  4     4     4 FALSE NA    TRUE  #>  5     5     5 TRUE  TRUE  FALSE #>  6     6     6 FALSE NA    FALSE #>  7     7     7 TRUE  TRUE  FALSE #>  8     8     8 FALSE NA    FALSE #>  9     9     9 TRUE  TRUE  NA    #> 10    10    NA FALSE NA    NA"},{"path":"https://beerda.github.io/nuggets/reference/remove_ill_conditions.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove invalid conditions from a list — remove_ill_conditions","title":"Remove invalid conditions from a list — remove_ill_conditions","text":"given list character vectors, remove elements valid conditions.","code":""},{"path":"https://beerda.github.io/nuggets/reference/remove_ill_conditions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove invalid conditions from a list — remove_ill_conditions","text":"","code":"remove_ill_conditions(x, data)"},{"path":"https://beerda.github.io/nuggets/reference/remove_ill_conditions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove invalid conditions from a list — remove_ill_conditions","text":"x list character vectors, representing condition. data matrix data frame whose column names define valid predicates.","code":""},{"path":"https://beerda.github.io/nuggets/reference/remove_ill_conditions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove invalid conditions from a list — remove_ill_conditions","text":"list containing elements x valid conditions.","code":""},{"path":"https://beerda.github.io/nuggets/reference/remove_ill_conditions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remove invalid conditions from a list — remove_ill_conditions","text":"valid condition character vector predicates, predicate corresponds column name supplied data frame matrix. Empty character vectors NULL elements also considered valid conditions. function acts simple filter around is_condition(). checks element x column names data removes contain invalid predicates. result preserves valid conditions discards invalid ones.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/remove_ill_conditions.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Remove invalid conditions from a list — remove_ill_conditions","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/remove_ill_conditions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove invalid conditions from a list — remove_ill_conditions","text":"","code":"d <- data.frame(foo = 1:5, bar = 1:5, blah = 1:5)  conds <- list(c(\"foo\", \"bar\"), \"blah\", \"invalid\", character(0), NULL) remove_ill_conditions(conds, d) #> [[1]] #> [1] \"foo\" \"bar\" #>  #> [[2]] #> [1] \"blah\" #>  #> [[3]] #> character(0) #>  #> [[4]] #> NULL #>  # keeps \"foo\",\"bar\"; \"blah\"; empty; NULL"},{"path":"https://beerda.github.io/nuggets/reference/shorten_condition.html","id":null,"dir":"Reference","previous_headings":"","what":"Shorten predicates within conditions — shorten_condition","title":"Shorten predicates within conditions — shorten_condition","text":"function takes character vector conditions shortens predicates within condition according specified method.","code":""},{"path":"https://beerda.github.io/nuggets/reference/shorten_condition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shorten predicates within conditions — shorten_condition","text":"","code":"shorten_condition(x, method = \"letters\")"},{"path":"https://beerda.github.io/nuggets/reference/shorten_condition.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shorten predicates within conditions — shorten_condition","text":"x character vector conditions, formatted string (e.g., \"{=1,b=100,c=3}\"). method character scalar specifying shortening method. Must one \"letters\", \"abbrev4\", \"abbrev8\", \"none\". Defaults \"letters\".","code":""},{"path":"https://beerda.github.io/nuggets/reference/shorten_condition.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shorten predicates within conditions — shorten_condition","text":"character vector conditions predicates shortened according specified method.","code":""},{"path":"https://beerda.github.io/nuggets/reference/shorten_condition.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Shorten predicates within conditions — shorten_condition","text":"element x must condition formatted string, e.g. \"{=1,b=100,c=3}\" (see format_condition()). function shortens predicates condition based selected method: \"letters\": predicates replaced single letters English alphabet, starting first distinct predicate; \"abbrev4\": predicates abbreviated 4 characters using arules::abbreviate(); \"abbrev8\": predicates abbreviated 8 characters using arules::abbreviate(); \"none\": shortening applied; predicates remain unchanged. Predicate shortening useful visualization reporting, especially original predicate names long complex. Note shortening applied consistently across conditions x.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/shorten_condition.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Shorten predicates within conditions — shorten_condition","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/shorten_condition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Shorten predicates within conditions — shorten_condition","text":"","code":"shorten_condition(c(\"{a=1,b=100,c=3}\", \"{a=2}\", \"{b=100,c=3}\"),                   method = \"letters\") #> [1] \"{A,B,C}\" \"{D}\"     \"{B,C}\"    shorten_condition(c(\"{helloWorld=1}\", \"{helloWorld=2}\", \"{c=3,helloWorld=1}\"),                   method = \"abbrev4\") #> [1] \"{hllW=1}\"     \"{hllW=2}\"     \"{c=3,hllW=1}\"  shorten_condition(c(\"{helloWorld=1}\", \"{helloWorld=2}\", \"{c=3,helloWorld=1}\"),                   method = \"abbrev8\") #> [1] \"{hellWrld=1}\"     \"{hellWrld=2}\"     \"{c=3,hellWrld=1}\"  shorten_condition(c(\"{helloWorld=1}\", \"{helloWorld=2}\"),                   method = \"none\") #> [1] \"{helloWorld=1}\" \"{helloWorld=2}\""},{"path":"https://beerda.github.io/nuggets/reference/values.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract values from predicate names — values","title":"Extract values from predicate names — values","text":"function extracts value part character vector predicate names. element x expected follow pattern <varname>=<value>, <varname> variable name <value> associated value.","code":""},{"path":"https://beerda.github.io/nuggets/reference/values.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract values from predicate names — values","text":"","code":"values(x)"},{"path":"https://beerda.github.io/nuggets/reference/values.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract values from predicate names — values","text":"x character vector predicate names.","code":""},{"path":"https://beerda.github.io/nuggets/reference/values.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract values from predicate names — values","text":"character vector containing <value> parts predicate names x. Elements without equal sign return empty string. x NULL, function returns NULL. x empty vector (character(0)), function returns empty vector (character(0)).","code":""},{"path":"https://beerda.github.io/nuggets/reference/values.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract values from predicate names — values","text":"element contain equal sign (=), function returns empty string element. function counterpart var_names(), extracts variable part predicates. Together, var_names() values() provide convenient way split predicate strings variable value components.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/values.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract values from predicate names — values","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/values.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract values from predicate names — values","text":"","code":"values(c(\"a=1\", \"a=2\", \"b=x\", \"b=y\")) #> [1] \"1\" \"2\" \"x\" \"y\" # returns c(\"1\", \"2\", \"x\", \"y\")  values(c(\"a\", \"b=3\")) #> [1] \"\"  \"3\" # returns c(\"\", \"3\")"},{"path":"https://beerda.github.io/nuggets/reference/var_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a tibble of combinations of selected column names — var_grid","title":"Create a tibble of combinations of selected column names — var_grid","text":"xvars yvars arguments tidyselect expressions (see tidyselect syntax) specify columns x whose names used form combinations. yvars NULL, function creates tibble one column, var, enumerating column names selected xvars expression. yvars NULL, function creates tibble two columns, xvar yvar, whose rows enumerate combinations column names specified xvars yvars. allowed specify column xvars yvars. cases, self-combinations (column paired ) removed result. words, function creates grid possible pairs \\((xx, yy)\\) \\(xx \\xvars\\), \\(yy \\yvars\\), \\(xx \\neq yy\\).","code":""},{"path":"https://beerda.github.io/nuggets/reference/var_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a tibble of combinations of selected column names — var_grid","text":"","code":"var_grid(   x,   xvars = everything(),   yvars = everything(),   allow = \"all\",   disjoint = var_names(colnames(x)),   xvar_name = if (quo_is_null(enquo(yvars))) \"var\" else \"xvar\",   yvar_name = \"yvar\",   error_context = list(arg_x = \"x\", arg_xvars = \"xvars\", arg_yvars = \"yvars\", arg_allow =     \"allow\", arg_disjoint = \"disjoint\", arg_xvar_name = \"xvar_name\", arg_yvar_name =     \"yvar_name\", call = current_env()) )"},{"path":"https://beerda.github.io/nuggets/reference/var_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a tibble of combinations of selected column names — var_grid","text":"x data frame matrix. xvars tidyselect expression specifying columns x whose names used first position (xvar) combinations. yvars NULL tidyselect expression specifying columns x whose names used second position (yvar) combinations. allow character string specifying columns may selected xvars yvars. Possible values : \"\" – columns may selected; \"numeric\" – numeric columns may selected. disjoint atomic vector length equal number columns x specifies disjoint groups predicates. Columns belonging group (.e. value disjoint) appear together single combination xvars yvars. Ignored yvars NULL. xvar_name character string specifying name first column (xvar) output tibble. yvar_name character string specifying name second column (yvar) output tibble. column omitted yvars NULL. error_context list providing details error messages. useful var_grid() called another function, allowing error messages reference caller’s argument names. list must contain: arg_x – name argument x; arg_xvars – name argument xvars; arg_yvars – name argument yvars; arg_allow – name argument allow; arg_xvar_name – name xvar column output; arg_yvar_name – name yvar column output; call – calling environment evaluating error messages.","code":""},{"path":"https://beerda.github.io/nuggets/reference/var_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a tibble of combinations of selected column names — var_grid","text":"yvars NULL, tibble single column (var). yvars NULL, tibble two columns (xvar, yvar) enumerating valid combinations column names selected xvars yvars. order variables result follows order selected xvars yvars.","code":""},{"path":"https://beerda.github.io/nuggets/reference/var_grid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a tibble of combinations of selected column names — var_grid","text":"var_grid() typically used function requires systematic list variables variable pairs analyze. example, can used generate pairs variables correlation, association, contrast analysis. flexibility xvars yvars makes possible restrict grid specific subsets variables ensuring invalid redundant combinations (e.g., self-pairs disjoint groups) excluded automatically. allow argument can used restrict selection columns numeric columns . useful resulting variable combinations used analyses require numeric data, correlation contrast tests. disjoint argument allows specifying groups columns appear together single combination. useful certain columns represent mutually exclusive categories measurements analyzed together. example, disjoint groups columns measurement type, function ensure combination includes two columns type.","code":""},{"path":"https://beerda.github.io/nuggets/reference/var_grid.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create a tibble of combinations of selected column names — var_grid","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/var_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a tibble of combinations of selected column names — var_grid","text":"","code":"# Grid of all pairwise column combinations in CO2 var_grid(CO2) #> # A tibble: 10 × 2 #>    xvar      yvar      #>    <chr>     <chr>     #>  1 Plant     Type      #>  2 Plant     Treatment #>  3 Plant     conc      #>  4 Plant     uptake    #>  5 Type      Treatment #>  6 Type      conc      #>  7 Type      uptake    #>  8 Treatment conc      #>  9 Treatment uptake    #> 10 conc      uptake     # Grid of combinations where the first column is Plant, Type, or Treatment, # and the second column is conc or uptake var_grid(CO2, xvars = Plant:Treatment, yvars = conc:uptake) #> # A tibble: 6 × 2 #>   xvar      yvar   #>   <chr>     <chr>  #> 1 Plant     conc   #> 2 Plant     uptake #> 3 Type      conc   #> 4 Type      uptake #> 5 Treatment conc   #> 6 Treatment uptake  # Prevent variables from the same disjoint group from being paired together d <- data.frame(a = 1:5, b = 6:10, c = 11:15, d = 16:20) # Group (a, b) together and (c, d) together var_grid(d, xvars = everything(), yvars = everything(),          disjoint = c(1, 1, 2, 2)) #> # A tibble: 4 × 2 #>   xvar  yvar  #>   <chr> <chr> #> 1 a     c     #> 2 a     d     #> 3 b     c     #> 4 b     d"},{"path":"https://beerda.github.io/nuggets/reference/var_names.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract variable names from predicate names — var_names","title":"Extract variable names from predicate names — var_names","text":"function extracts variable part character vector predicate names. element x expected follow pattern <varname>=<value>, <varname> variable name <value> associated value.","code":""},{"path":"https://beerda.github.io/nuggets/reference/var_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract variable names from predicate names — var_names","text":"","code":"var_names(x)"},{"path":"https://beerda.github.io/nuggets/reference/var_names.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract variable names from predicate names — var_names","text":"x character vector predicate names.","code":""},{"path":"https://beerda.github.io/nuggets/reference/var_names.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract variable names from predicate names — var_names","text":"character vector containing <varname> parts predicate names x. element contain =, entire string returned . x NULL, function returns NULL. x length zero (character(0)), function returns character(0).","code":""},{"path":"https://beerda.github.io/nuggets/reference/var_names.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract variable names from predicate names — var_names","text":"element contain equal sign (=), entire string returned unchanged. function counterpart values(), extracts value part predicates. Together, var_names() values() provide convenient way split predicate strings variable value components.","code":""},{"path":[]},{"path":"https://beerda.github.io/nuggets/reference/var_names.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract variable names from predicate names — var_names","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/reference/var_names.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract variable names from predicate names — var_names","text":"","code":"var_names(c(\"a=1\", \"a=2\", \"b=x\", \"b=y\")) #> [1] \"a\" \"a\" \"b\" \"b\" # returns c(\"a\", \"a\", \"b\", \"b\")  var_names(c(\"a\", \"b=3\")) #> [1] \"a\" \"b\" # returns c(\"a\", \"b\")  var_names(character(0)) #> character(0) # returns character(0)  var_names(NULL) #> NULL # returns character(0)"},{"path":"https://beerda.github.io/nuggets/reference/which_antichain.html","id":null,"dir":"Reference","previous_headings":"","what":"Return indices of first elements of the list, which are incomparable with preceding elements. — which_antichain","title":"Return indices of first elements of the list, which are incomparable with preceding elements. — which_antichain","text":"function returns indices elements given list x, incomparable (.e., neither subset superset) preceding element. first element always selected. next element selected incomparable previously selected elements.","code":""},{"path":"https://beerda.github.io/nuggets/reference/which_antichain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return indices of first elements of the list, which are incomparable with preceding elements. — which_antichain","text":"","code":"which_antichain(x, distance = 0)"},{"path":"https://beerda.github.io/nuggets/reference/which_antichain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return indices of first elements of the list, which are incomparable with preceding elements. — which_antichain","text":"x list integerish vectors distance non-negative integer, specifies allowed discrepancy compared sets","code":""},{"path":"https://beerda.github.io/nuggets/reference/which_antichain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return indices of first elements of the list, which are incomparable with preceding elements. — which_antichain","text":"integer vector indices selected (incomparable) elements.","code":""},{"path":"https://beerda.github.io/nuggets/reference/which_antichain.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Return indices of first elements of the list, which are incomparable with preceding elements. — which_antichain","text":"Michal Burda","code":""},{"path":"https://beerda.github.io/nuggets/news/index.html","id":"nuggets-202-development-version","dir":"Changelog","previous_headings":"","what":"nuggets 2.0.2 (development version)","title":"nuggets 2.0.2 (development version)","text":"fixed rchk protection stack imbalance CombinatorialProgress constructor","code":""},{"path":"https://beerda.github.io/nuggets/news/index.html","id":"nuggets-201","dir":"Changelog","previous_headings":"","what":"nuggets 2.0.1","title":"nuggets 2.0.1","text":"released: 2025-10-13 fixed problem C++20 testthat downgrading system requirements C++17 added association_matrix()","code":""},{"path":"https://beerda.github.io/nuggets/news/index.html","id":"nuggets-200","dir":"Changelog","previous_headings":"","what":"nuggets 2.0.0","title":"nuggets 2.0.0","text":"released: 2025-10-13 completely rewritten core algorithm dig() dig_*() functions now return nugget S3 objects added is_almost_constant(), remove_almost_constant(), parse_condition(), fire(), is_condition(), remove_ill_conditions(), shorten_condition() added dig_tautologies() added geom_diamond() added .span .inc arguments partition() added various styles (quantile, k-means, hclust, bclust, …) crisp partitioning partition() added explore() function interactive exploration patterns added exclude argument dig() dig_*() functions added support disjoint parameter var_grid() dig_grid() added progress bar dig() function added “dummy” method partition()","code":""},{"path":"https://beerda.github.io/nuggets/news/index.html","id":"nuggets-140","dir":"Changelog","previous_headings":"","what":"nuggets 1.4.0","title":"nuggets 1.4.0","text":"released: 2025-01-08 added var_names(), dig_baseline_contrasts(), dig_complement_contrasts() dig_contrasts() renamed dig_paired_baseline_contrasts() dig_implications() renamed dig_associations() dichotomize() deprecated (use partition() instead) added max_support argument dig() added max_results argument dig() optimized performance dig() added min_conditional_focus_support argument dig() fixed handling NULL returned callback function dig_grid() argument d callback function dig_grid() renamed pd added handling nd argument callback function dig_grid() added max_p_value argument dig_paired_baseline_contrasts() improved error messages added nuggets vignette started using lifecycle pkgdown","code":""},{"path":"https://beerda.github.io/nuggets/news/index.html","id":"nuggets-130","dir":"Changelog","previous_headings":"","what":"nuggets 1.3.0","title":"nuggets 1.3.0","text":"released: 2024-11-13 added is_degree(), dig_contrasts(), partition() implemented fuzzy variant dig_grid() fixed crash mixing logical (crisp) numeric (fuzzy) inputs dig()","code":""},{"path":"https://beerda.github.io/nuggets/news/index.html","id":"nuggets-120","dir":"Changelog","previous_headings":"","what":"nuggets 1.2.0","title":"nuggets 1.2.0","text":"released: 2024-10-11 added var_grid(), dig_grid() added measures argument dig_implications() fixed contingency table arguments computation (pp, pn, np, nn) - previously, computed relative frequencies, now computed counts fixed new-delete-type-mismatch ASAN error caused wrong implementation AlignedAllocator fixed memory leaks","code":""},{"path":"https://beerda.github.io/nuggets/news/index.html","id":"nuggets-110","dir":"Changelog","previous_headings":"","what":"nuggets 1.1.0","title":"nuggets 1.1.0","text":"released: 2024-10-08 added .argument dichotomize() fixed handling xvars, yvars tidy-selectors dig_correlations() added filtering foci support added handling callback function arguments related contingency tables (pp, pn, np, nn arguments)","code":""},{"path":"https://beerda.github.io/nuggets/news/index.html","id":"nuggets-102","dir":"Changelog","previous_headings":"","what":"nuggets 1.0.2","title":"nuggets 1.0.2","text":"released: 2024-01-09 fixed handling arguments min_coverage min_support dig_implications() attempt fix LTO error related run_testthat_tests() - fixed using RC version Rcpp (1.0.11.6)","code":""},{"path":"https://beerda.github.io/nuggets/news/index.html","id":"nuggets-101","dir":"Changelog","previous_headings":"","what":"nuggets 1.0.1","title":"nuggets 1.0.1","text":"released: 2023-11-29 attempt fix tests failing R-devel","code":""},{"path":"https://beerda.github.io/nuggets/news/index.html","id":"nuggets-100","dir":"Changelog","previous_headings":"","what":"nuggets 1.0.0","title":"nuggets 1.0.0","text":"released: 2023-11-28 first version package implemented: dichotomize(), dig(), dig_implications(), dig_correlations(), which_antichain(), format_condition(), is_subset()","code":""}]
